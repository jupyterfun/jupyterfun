<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>数理统计、机器学习入门 on jupyter.fun</title>
    <link>/statistics/</link>
    <description>Recent content in 数理统计、机器学习入门 on jupyter.fun</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>zh-ch</language>
    <lastBuildDate>Sun, 07 Aug 2022 16:07:59 +0800</lastBuildDate><atom:link href="/statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>描述统计</title>
      <link>/statistics/statistics1/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics1/</guid>
      <description>数理统计以概率论为基础, 研究大量随机现象的统计规律性. 分为 描述统计 和 推断统计 , 在数据分析领域具有非常重要的地位
描述统计, 就是从总体数据中提取变量的主要信息(总和, 均值, 最大, 最多等), 从而从总体层面上, 对数据进行统计性描述. 通常配合绘制相关统计图进行辅助
统计学的变量类型 统计学中的变量指研究对象的特征(属性), 每个变量都有变量值和类型, 类型可分为:
类别变量 : 对研究对象定性, 分类
类别变量又可分为:
有序类别变量: 描述对象等级或顺序等, 例如, 优良中差 无序类别变量: 仅做分类, 例如 A, B 血型, 男女 数值变量 : 对研究对象定量描述
数值变量又可分为:
离散变量: 取值只能用自然数或整数个单位计算, 例如统计人数 连续变量: 在一定区间内可以任意取值, 例如计算身高 数值变量对加, 减, 求平均等操作有意义, 而类别变量无意义
统计量 描述统计所提取的统计信息, 称为统计量, 主要包括:
类别分析: 频数, 频率 集中趋势分析: 均值, 中位数, 众数, 分位数 离散程度分析: 极差, 方差, 标准差 描述分布形状: 偏度, 峰度 准备数据:</description>
    </item>
    <item>
      <title>推断统计</title>
      <link>/statistics/statistics2/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics2/</guid>
      <description>推断统计, 通过样本推断总体的统计方法, 包括对总体的未知参数进行估计; 对关于参数的假设进行检查; 对总体进行预测预报等. 推断统计的基本问题可以分为两大类：一类是 参数估计 问题; 另一类是 假设检验 问题
1, 总体, 个体与样本 总体, 要研究对象的所有数据, 获取通常比较困难. 总体中的某个数据, 就是个体. 从总体中抽取部分个体, 就构成了样本, 样本中的个体数, 称为样本容量.
2, 参数估计 参数估计, 用样本指标(统计量)估计总体指标(参数). 参数估计有 点估计 和 区间估计 两种
2.01, 点估计 点估计是依据样本统计量估计总体中的未知参数. 通常它们是总体的某个特征值，如数学期望, 方差和相关系数等. 点估计问题就是要构造一个只依赖于样本的量，作为总体未知参数的估计值.
2.02, 区间估计 区间估计是根据样本的统计量, 计算出一个可能的区间(置信区间) 和 概率(置信度), 表示总体的未知参数有多少概率位于该区间.
注意:
点估计使用一个值来作为总体参数值, 能给出具体值, 但易受随机抽样影响, 准确性不够
区间估计使用一个置信区间和置信度, 表示总体参数值有多少可能(置信度)会在该范围(置信区间)内, 能给出合理的范围和信心指数, 不能给出具体值
2.03, 中心极限定理 要确定置信区间与置信度, 我们先要知道总体与样本之间, 在分布上有着怎样的联系. 中心极限定理(独立同分布的中心极限定理)给出了它们之间的联系:
如果总体均值为 $\mu$, 方差为 $\sigma^{2}$, 我们进行随机抽样, 样本容量为 n, 当 n 增大时，则样本均值 $\bar{X}$ 逐渐趋近服从均值为 $\mu$, 方差为 $\sigma^{2} / n$ 的正态分布：</description>
    </item>
    <item>
      <title>线性回归</title>
      <link>/statistics/statistics3/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics3/</guid>
      <description>1, 模型 模型是指对于某个(类)实际问题的求解或客观事物运行规律进行抽象后的一种形式化表达方式, 可以理解为一个函数(一种映射规则)
任何模型都是由三个部分组成: 目标, 变量和关系. 建模时明确了模型的目标，才能进一步确定影响目标(因变量)的各关键变量(自变量)，进而确定变量之间的关系(函数关系)
通过大量数据检验(训练)模型, 将模型(函数)的各个参数求解, 当参数确定之后, 便可利用模型对未知数据进行求值, 预测
用于训练模型的样本数据中的每个属性称为特征, 用 x 表示, 样本中的每条数据经过模型计算得到的输出值称为标签(监督学习), 用 y 表示, 从而得到 y = f(x) 的函数关系
2, 回归分析 在统计学中, 回归分析指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法
回归分析按照涉及的变量的多少，分为一元回归分析和多元回归分析；按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析
回归分析解释自变量 x 发生改变, 因变量 y 会如何改变
拟合 , 插值 和 逼近 是数值分析的三大基础工具. 线性回归和非线性回归, 也叫线性拟合和非线性拟合, 拟合就是从整体上靠近已知点列，构造一种算法(模型或函数), 使得算法能够更加符合真实数据
3, 简单线性回归 线性回归分析的自变量和因变量之间是线性关系, 只有一个自变量时称为 简单线性回归 , 多个自变量时称为 多元线性回归
简单线性回归方程:
$$\hat{y}=w * x+b$$ $\hat{y}$ 为因变量, x 为自变量, w 为比例关系, b 为截距, w 和 b 就是模型的参数.</description>
    </item>
    <item>
      <title>逻辑回归</title>
      <link>/statistics/statistics4/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics4/</guid>
      <description>逻辑回归和线性回归有类似之处, 都是利用线性加权计算的模型, 但逻辑回归是分类算法, 例如对是否患癌症进行预测, 因变量就是 是 和 否 , 两个类别, 自变量可以是年龄, 性别, 饮食, 作息, 病菌感染等, 自变量既可以是数值变量, 也可以是类别变量
1, 逻辑回归二分类推导 和线性回归类似, 设自变量为 x, 每个自变量的权重为 w, 令:
$$\begin{array}{l} z=w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{n} x_{n}+b \ =\sum_{j=1}^{n} w_{j} x_{j}+b \ =\sum_{j=0}^{n} w_{j} x_{j} \ =\vec{w}^{T} \cdot \vec{x} \end{array}$$z 是一个连续值, 取值范围(-∞, +∞), 为了实现分类, 一般设置阈值 z = 0, 当 z &amp;gt; 0 时, 将样本判定为一个类别(正例), 该类别设为 1, 当 z ≤ 0 时, 判定为另一个类别(负例), 该类别设为 0, 再设因变量为 y, 从而逻辑回归方程可表示为:</description>
    </item>
    <item>
      <title>分类模型评估</title>
      <link>/statistics/statistics5/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics5/</guid>
      <description>在完成模型训练之后，需要对模型的效果进行评估，根据评估结果继续调整模型的参数, 特征或者算法，以达到满意的结果
1, 混淆矩阵 将 真正例(TP), 假正例(FP), 真负例(TN), 假负例(FN) 统计于一个方阵中, 观察比较, 评价模型好坏, 矩阵如下:
混淆矩阵统计数量, 评价不直观也有限, 基于混淆矩阵又延伸出 正确率, 精准率, 召回率, F1(调和平均值), ROC曲线和AUC等
2, 评估指标分析 正确率:
$$\text { 正确率 }=\frac{T P+T N}{T P+T N+F P+F N}$$正确率, 表示总体(包括正负)预测正确的比率, 在模型对正例和负例的预测准确度差异较大时, 难以评价模型的好坏, 例如正例较多, 负例较少, 正例全部预测对了, 负例只预测对几个, 正确率却可能较高
精准率:
$$\text { 精准率 }=\frac{T P}{T P+F P}$$精准率, 表示所有预测为正例的结果中 预测正确的正例 的占比, 精准率越高, 说明正例预测正确概率越高, 因此精准率更关注”一击必中”, 比如通过预测找出上涨的概率很高的一支股票
召回率:
$$\text { 召回率 }=\frac{T P}{T P+F N}$$召回率, 表示所有真实的正例中, 预测正确的正例 的占比, 召回率越高, 说明正例被”召回”的越多, 因此召回率更关注”宁错一千, 不放一个”, 例如通过预测尽可能将新冠肺炎患者全部隔离观察</description>
    </item>
    <item>
      <title>KNN 算法</title>
      <link>/statistics/statistics6/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics6/</guid>
      <description>1, 关于 KNN KNN (K-Nearest Neighbor), 即 K 近邻算法, K 个最近的邻居. 当需要预测一个未知样本的时候, 就由与该样本最近的 K 个邻居来决定
KNN 既可以用于分类, 也可用于回归. 用来分类时, 使用 K 个邻居中, 类别数量最多(或加权最多)者, 作为预测结果; 当用来回归分析时, 使用 K 个邻居的均值(或加权均值), 作为预测结果
KNN 算法的原理是: 样本映射到多维空间时, 相似度较高的样本, 距离也会较接近, “近朱者赤近墨者黑”
2, K 值 KNN 算法的 K 值是一个模型训练前就要人为指定的参数 超参数 , 不同于模型内部通过训练数据计算得到的参数. KNN 的超参数, 需要通常通过 交叉验证 的方式来选择最合适的参数组合
K 值的选择非常重要, K 值较小时, 模型预测依赖附近的邻居, 敏感性高, 稳定性低, 容易导致过拟合; 反之, K 值较大, 敏感性低, 稳定性高, 容易欠拟合
K 值在数据量小时, 可以通过遍历所有样本(穷举)的方式找出最近的 K 个邻居, 当数据量庞大时, 穷举耗费大量时间, 此时可以采用 KD树 来找 K 个邻居</description>
    </item>
    <item>
      <title>朴素贝叶斯</title>
      <link>/statistics/statistics7/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics7/</guid>
      <description>1, 概率基础 样本空间 :
在 随机试验 E 中, 实验的所有可能结果组成的集合, 称为 样本空间 S, 样本空间的每个元素, 即 E 的每个结果, 称 样本点
随机事件 :
进行随机试验时, 满足某种条件的样本点组成的集合, S 的子集, 称作 随机事件 , 只有一个样本点时, 称作 基本事件
概率 :
对于随机事件 A, 概率为:
$P(A)=\frac{A \text { 中基本事件数 }}{S \text { 中基本事件数 }}$ 条件概率 :
定义事件 A 发生的前提下, 事件 B 发生的概率 P(B | A) 为条件概率:
$$P(B \mid A)=\frac{P(A B)}{P(A)}$$由条件概率的定义可得, 事件 A 和 B 同时发生的概率 P(AB) 满足如下 乘法定理 :
$$P(A B)=P(B \mid A) P(A)$$独立性:</description>
    </item>
    <item>
      <title>决策树</title>
      <link>/statistics/statistics8/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics8/</guid>
      <description>1, 概念理解 决策树: 通过数据特征的差别, 用已知数据训练将不同数据划分到不同分支(子树)中, 层层划分, 最终得到一个树型结构, 用来对未知数据进行预测, 实现分类或回归
例如, 有如下数据集, 预测第 11 条数据能否偿还债务:
序号 有无房产 婚姻状况 年收入 能否偿还债务 1 是 单身 125 能 2 否 已婚 100 能 3 否 单身 100 能 4 是 已婚 110 能 5 是 离婚 60 能 6 否 离婚 95 不能 7 否 单身 85 不能 8 否 已婚 75 能 9 否 单身 90 不能 10 是 离婚 220 能 11 否 已婚 94 ?</description>
    </item>
    <item>
      <title>K-Means 算法</title>
      <link>/statistics/statistics9/</link>
      <pubDate>Sun, 07 Aug 2022 16:07:59 +0800</pubDate>
      <guid>/statistics/statistics9/</guid>
      <description>1, 聚类 前面接触的算法, 都是 监督学习 , 即训练数据中自变量(特征)和因变量(结果)都是已知的, 用含有结果的训练集建立模型, 然后对未知结果的数据进行预测
聚类属于 无监督学习 , 训练数据中没有”已知结果的监督”. 聚类的目的, 就是通过已知样本数据的特征, 将数据划分为若干个类别, 每个类别成一个类簇, 使得同一个簇内的数据相似度越大, “物以类聚”, 不同簇之间的数据相似度越小, 聚类效果越好
聚类的样本相似度根据距离来度量
2, K-Means 即 K 均值算法, 是常见的聚类算法, 该算法将数据集分为 K 个簇, 每个簇使用簇内所有样本的均值来表示, 该均值称为”质心”
K-Means 算法的目标, 就是选择适合的质心, 使得每个簇内, 样本点距质心的距离尽可能的小, 从而保证簇内样本有较高相似度
算法实现步骤:
a, 从样本中选择 K 个点作为初始质心
b, 计算每个样本点到各个质心的距离, 将样本点划分到距离最近的质心所对应的簇中
c, 计算每个簇内所有样本的均值, 使用该均值作为新的质心
d, 重复 b 和 c, 重复一定次数质心一般会趋于稳定, 如果达到以下条件, 重复结束:
– 质心位置变化小于指定的阈值
– 达到最迭代环次数
对于算法的实现步骤, 我们有几个重要的疑问:
– 1.怎么评价质心是否达到了最佳位置?
– 2.初始质心随机选, 还是选在哪里?
– 3.</description>
    </item>
  </channel>
</rss>