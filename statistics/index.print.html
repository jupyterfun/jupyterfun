<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.101.0">
    <meta name="generator" content="Relearn 5.2.1+tip">
    <meta name="description" content="">
    <meta name="author" content="吴明文">
    <title>数理统计、机器学习入门 - jupyter.fun</title>
    <link href="/statistics/" rel="canonical" type="text/html" title="jupyter.fun">
    <link href="/statistics/index.xml" rel="alternate" type="application/rss+xml" title="jupyter.fun"><link rel="icon" href="/favicon.png" type="image/png" />
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="/css/fontawesome-all.min.css?1661054444" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fontawesome-all.min.css?1661054444" rel="stylesheet"></noscript>
    <link href="/css/featherlight.min.css?1661054444" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/featherlight.min.css?1661054444" rel="stylesheet"></noscript>
    <link href="/css/auto-complete.css?1661054444" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/auto-complete.css?1661054444" rel="stylesheet"></noscript>
    <link href="/css/perfect-scrollbar.min.css?1661054444" rel="stylesheet">
    <link href="/css/nucleus.css?1661054444" rel="stylesheet">
    <link href="/css/fonts.css?1661054444" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="/css/fonts.css?1661054444" rel="stylesheet"></noscript>
    <link href="/css/theme.css?1661054444" rel="stylesheet">
    <link href="/css/theme-blue.css?1661054444" rel="stylesheet" id="variant-style">
    <link href="/css/ie.css?1661054444" rel="stylesheet">
    <link href="/css/variant.css?1661054444" rel="stylesheet">
    <link href="/css/print.css?1661054444" rel="stylesheet" media="print">
    <link href="/css/format-print.css?1661054444" rel="stylesheet">
    <script src="/js/variant.js?1661054444"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      var index_url="/index.json";
      var root_url="/";
      var baseUri=root_url.replace(/\/$/, '');
      // translations
      window.T_Copy_to_clipboard = 'Copy to clipboard';
      window.T_Copied_to_clipboard = 'Copied to clipboard!';
      window.T_Copy_link_to_clipboard = 'Copy link to clipboard';
      window.T_Link_copied_to_clipboard = 'Copied link to clipboard!';
      // some further base stuff
      var baseUriFull='/';
      window.variants && variants.init( [ 'blue' ] );
    </script>
    <script src="/js/jquery.min.js?1661054444" defer></script>
  </head>
  <body class="mobile-support print" data-url="/statistics/">
    <div id="body" class="default-animation">
      <div id="sidebar-overlay"></div>
      <div id="toc-overlay"></div>
      <nav id="topbar" class="highlightable">
        <div>
          <div id="top-print-link">
            <a class="print-link" title='打印预览 (CTRL+ALT+p)' href="/statistics/index.print.html">
              <i class="fas fa-print fa-fw"></i>
            </a>
          </div>
          <div id="breadcrumbs">
            <span id="sidebar-toggle-span">
              <a href="#" id="sidebar-toggle" title='Menu (CTRL+ALT+m)'><i class="fas fa-bars fa-fw"></i></a>
            </span>
            <ol class="links" itemscope itemtype="http://schema.org/BreadcrumbList">
              <meta itemprop="itemListOrder" content="Descending" />
              <li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><meta itemprop="position" content="2" /><a itemprop="item" href="/"><span itemprop="name">主页</span></a> > </li>
              <li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><meta itemprop="position" content="1" /><a itemprop="item" href="/statistics/" aria-disabled="true"><span itemprop="name">数理统计、机器学习入门</span></a></li>
            </ol>
          </div>
        </div>
      </nav>
      <main id="body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <div id="head-tags">
          </div>
          <article class="default">
<h1>数理统计、机器学习入门</h1>

<p>目录和附件</p>

<ul class="children children-li children-sort-weight">
<li><a href="/statistics/statistics1/" >描述统计</a></li><ul></ul>
<li><a href="/statistics/statistics2/" >推断统计</a></li><ul></ul>
<li><a href="/statistics/statistics3/" >线性回归</a></li><ul></ul>
<li><a href="/statistics/statistics4/" >逻辑回归</a></li><ul></ul>
<li><a href="/statistics/statistics5/" >分类模型评估</a></li><ul></ul>
<li><a href="/statistics/statistics6/" >KNN 算法</a></li><ul></ul>
<li><a href="/statistics/statistics7/" >朴素贝叶斯</a></li><ul></ul>
<li><a href="/statistics/statistics8/" >决策树</a></li><ul></ul>
<li><a href="/statistics/statistics9/" >K-Means 算法</a></li><ul></ul>
</ul>

<div class="box attachments cstyle blue">
  <div class="box-label"><i class="fa-fw fas fa-paperclip"></i> jupyter附件</div>
  <ul class="box-content attachments-files">
    <li><a href="/statistics/_index.files/statistics1.ipynb">statistics1.ipynb</a> (50 KB)</li>
    <li><a href="/statistics/_index.files/statistics2.ipynb">statistics2.ipynb</a> (47 KB)</li>
    <li><a href="/statistics/_index.files/statistics3.ipynb">statistics3.ipynb</a> (36 KB)</li>
    <li><a href="/statistics/_index.files/statistics4.ipynb">statistics4.ipynb</a> (70 KB)</li>
    <li><a href="/statistics/_index.files/statistics5.ipynb">statistics5.ipynb</a> (40 KB)</li>
    <li><a href="/statistics/_index.files/statistics6.ipynb">statistics6.ipynb</a> (11 KB)</li>
    <li><a href="/statistics/_index.files/statistics7.ipynb">statistics7.ipynb</a> (22 KB)</li>
    <li><a href="/statistics/_index.files/statistics8.ipynb">statistics8.ipynb</a> (94 KB)</li>
    <li><a href="/statistics/_index.files/statistics9.ipynb">statistics9.ipynb</a> (439 KB)</li>
  </ul>
</div>


            <footer class="footline">
            </footer>
          </article>

          <section>
          <article class="default">
<h1>描述统计</h1>

<p>数理统计以概率论为基础, 研究大量随机现象的统计规律性. 分为 <strong>描述统计</strong> 和 <strong>推断统计</strong> , 在数据分析领域具有非常重要的地位</p>
<p>描述统计, 就是从总体数据中提取变量的主要信息(总和, 均值, 最大, 最多等), 从而从总体层面上, 对数据进行统计性描述.
通常配合绘制相关统计图进行辅助</p>
<h2 id="统计学的变量类型">统计学的变量类型</h2>
<p>统计学中的变量指研究对象的特征(属性), 每个变量都有变量值和类型, 类型可分为:</p>
<p><strong>类别变量</strong> : 对研究对象定性, 分类</p>
<p>类别变量又可分为:</p>
<ul>
<li>有序类别变量: 描述对象等级或顺序等, 例如, 优良中差</li>
<li>无序类别变量: 仅做分类, 例如 A, B 血型, 男女</li>
</ul>
<p><strong>数值变量</strong> : 对研究对象定量描述</p>
<p>数值变量又可分为:</p>
<ul>
<li>离散变量: 取值只能用自然数或整数个单位计算, 例如统计人数</li>
<li>连续变量: 在一定区间内可以任意取值, 例如计算身高</li>
</ul>
<p>数值变量对加, 减, 求平均等操作有意义, 而类别变量无意义</p>
<h2 id="统计量">统计量</h2>
<p>描述统计所提取的统计信息, 称为统计量, 主要包括:</p>
<ul>
<li>类别分析: 频数, 频率</li>
<li>集中趋势分析: 均值, 中位数, 众数, 分位数</li>
<li>离散程度分析: 极差, 方差, 标准差</li>
<li>描述分布形状: 偏度, 峰度</li>
</ul>
<p>准备数据:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.family&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SimHei&#39;</span>  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;axes.unicode_minus&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 正态分布  </span>
</span></span><span style="display:flex;"><span>data1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>around(np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">600</span>))<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 左偏  </span>
</span></span><span style="display:flex;"><span>t1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">21</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)  
</span></span><span style="display:flex;"><span>t2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">21</span>, <span style="color:#ae81ff">31</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)  
</span></span><span style="display:flex;"><span>left_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>concatenate([t1, t2])<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 右偏  </span>
</span></span><span style="display:flex;"><span>t3 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">11</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>)  
</span></span><span style="display:flex;"><span>t4 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">21</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>)  
</span></span><span style="display:flex;"><span>right_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>concatenate([t3, t4])<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 类别  </span>
</span></span><span style="display:flex;"><span>type_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">600</span>)<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>concatenate([data1, left_data, right_data, type_data], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(data,  
</span></span><span style="display:flex;"><span>                   columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;data1&#39;</span>, <span style="color:#e6db74">&#39;left_data&#39;</span>, <span style="color:#e6db74">&#39;right_data&#39;</span>, <span style="color:#e6db74">&#39;type_data&#39;</span>])  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 随机取 10 条数据  </span>
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>sample(<span style="color:#ae81ff">10</span>)  
</span></span></code></pre></div><pre><code>  data1	left_data	right_data	type_data
202	13.0	27.0	8.0	0.0
595	12.0	23.0	15.0	0.0
523	11.0	21.0	20.0	1.0
259	12.0	29.0	8.0	0.0
498	12.0	24.0	3.0	0.0
110	8.0	27.0	1.0	0.0
65	7.0	12.0	5.0	0.0
231	13.0	25.0	2.0	0.0
321	8.0	30.0	3.0	0.0
544	5.0	29.0	19.0	1.0
</code></pre>
<h3 id="a-频数">a, 频数</h3>
<p>数据中某个类别出现的次数称为该类别的频数</p>
<p>例如, 计算上述两个类别(<code>0.0</code> 和 <code>1.0</code>)出现的频数:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>frequency <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;type_data&#39;</span>]<span style="color:#f92672">.</span>value_counts()  
</span></span><span style="display:flex;"><span>frequency 
</span></span></code></pre></div><pre><code>0.0    309
1.0    291
Name: type_data, dtype: int64
</code></pre>
<h3 id="b-频率">b, 频率</h3>
<p>数据中某个类别出现次数与总次数的比值称为该类别的频率</p>
<p>例如, 计算上述两个类别(<code>0.0</code> 和 <code>1.0</code>)出现的频率:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>percentage <span style="color:#f92672">=</span> frequency <span style="color:#f92672">*</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">/</span> len(data)  
</span></span><span style="display:flex;"><span>percentage  
</span></span></code></pre></div><pre><code>0.0    51.5
1.0    48.5
Name: type_data, dtype: float64
</code></pre>
<h3 id="c-均值">c, 均值</h3>
<p>平均值, 一组数据的总和除以数据的个数</p>
<h3 id="d-中位数">d, 中位数</h3>
<p>将一组数据按顺序排列, 位于最中间位置的值, 即是中位数, 如果数据个数为偶数, 取中间两个的平均值</p>
<h3 id="e-众数">e, 众数</h3>
<p>一组数据中出现次数最多的值</p>
<p>通常三者的关系如下图所示:</p>
<p><img src="fenbu.png" alt=""></p>
<p><strong>注意点</strong> :<br>
数值变量通常使用均值和中值表示集中趋势, 类别变量则通常使用众数<br>
正态分布下, 数据量足够多, 三者相同<br>
均值使用所有数据计算, 容易受极端值影响, 中位数和众数则不会<br>
众数在一组数据中可能不唯一</p>
<p>例, 计算字段 <code>data1</code> 的均值, 中位数和众数:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>mean <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;data1&#39;</span>]<span style="color:#f92672">.</span>mean()  
</span></span><span style="display:flex;"><span>median <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;data1&#39;</span>]<span style="color:#f92672">.</span>median()  
</span></span><span style="display:flex;"><span>mode <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;data1&#39;</span>]<span style="color:#f92672">.</span>mode()  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;均值:</span><span style="color:#e6db74">{</span>mean<span style="color:#e6db74">}</span><span style="color:#e6db74"> 中位数:</span><span style="color:#e6db74">{</span>median<span style="color:#e6db74">}</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">众数:</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">{</span>mode<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)  
</span></span></code></pre></div><pre><code>均值:10.121666666666666 中位数:10.0
众数:
0    9.0
dtype: float64
</code></pre>
<h3 id="f-分位数">f, 分位数</h3>
<p>通过 n - 1 个分位, 将升序排列的数据分为 n 个区间, 使得每个区间数值个数相等(或近似相等), 则每个分位对应的数, 就是该 n 分位的分位数.
常用的有四分位数和百分位数</p>
<p>以四分位数为例:<br>
第一个分位称为 1/4 分位(下四分位), 第二个称为 2/4 分位(中四分位), 第三个称为 3/4 分位(上四分位), 其中中四分位数, 其实就是中位数</p>
<p>求四分位的值:</p>
<ul>
<li>
<p>首先计算各个分位的位置<br>
index1 = (n - 1) * 0.25<br>
index2 = (n - 1) * 0.5<br>
index3 = (n - 1) * 0.75<br>
(index 从 0 开始, n 为元素的个数)</p>
</li>
<li>
<p>根据位置计算各个分位的值<br>
index 为整数, 值就是相应的 index 对应的元素<br>
index 不为整数, 四分位位置介于 ceil(index) 和 floor(index) 之间, 加权计算分位值</p>
</li>
</ul>
<p>例, 求 x 的四分位数:<br>
<strong>index 为整数</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">9</span>)  
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> len(x)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>index1 <span style="color:#f92672">=</span> (n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.25</span>  
</span></span><span style="display:flex;"><span>index2 <span style="color:#f92672">=</span> (n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.5</span>    
</span></span><span style="display:flex;"><span>index3 <span style="color:#f92672">=</span> (n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.75</span>    
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([index1, index2, index3])<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>int32)  
</span></span><span style="display:flex;"><span>x[index]  
</span></span></code></pre></div><pre><code>array([2, 4, 6])
</code></pre>
<p><strong>index 不是整数</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>)  
</span></span><span style="display:flex;"><span>n <span style="color:#f92672">=</span> len(x)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>index1 <span style="color:#f92672">=</span> (n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.25</span>  
</span></span><span style="display:flex;"><span>index2 <span style="color:#f92672">=</span> (n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.5</span>    
</span></span><span style="display:flex;"><span>index3 <span style="color:#f92672">=</span> (n <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> <span style="color:#ae81ff">0.75</span>    
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([index1, index2, index3])  
</span></span><span style="display:flex;"><span>left <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>floor(index)<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>int32)  
</span></span><span style="display:flex;"><span>right <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ceil(index)<span style="color:#f92672">.</span>astype(np<span style="color:#f92672">.</span>int32)  
</span></span><span style="display:flex;"><span>weight, _ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>modf(index) <span style="color:#75715e"># 获取 index 整数和小数部分  </span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>result <span style="color:#f92672">=</span> x[left] <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> weight) <span style="color:#f92672">+</span> x[right] <span style="color:#f92672">*</span> weight  
</span></span><span style="display:flex;"><span>result  
</span></span></code></pre></div><pre><code>array([2.25, 4.5 , 6.75])
</code></pre>
<p>Numpy 中计算分位数可直接用方法 <code>np.quantile</code> 和 <code>np.percentile</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>np<span style="color:#f92672">.</span>quantile(x, q<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.25</span>, <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.75</span>]), np<span style="color:#f92672">.</span>percentile(x, q<span style="color:#f92672">=</span>[<span style="color:#ae81ff">25</span>, <span style="color:#ae81ff">50</span>, <span style="color:#ae81ff">75</span>])  
</span></span></code></pre></div><pre><code>(array([2.25, 4.5 , 6.75]), array([2.25, 4.5 , 6.75]))
</code></pre>
<p>Pandas 中计算分位数可利用 <code>describe</code> (默认 4 分位)</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>s <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>Series(x)  
</span></span><span style="display:flex;"><span>s<span style="color:#f92672">.</span>describe()  
</span></span></code></pre></div><pre><code>count    10.00000
mean      4.50000
std       3.02765
min       0.00000
25%       2.25000
50%       4.50000
75%       6.75000
max       9.00000
dtype: float64
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>s<span style="color:#f92672">.</span>describe()<span style="color:#f92672">.</span>iloc[<span style="color:#ae81ff">4</span>:<span style="color:#ae81ff">7</span>]  
</span></span></code></pre></div><pre><code>25%    2.25
50%    4.50
75%    6.75
dtype: float64
</code></pre>
<p>可自定义分位:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>s<span style="color:#f92672">.</span>describe(percentiles<span style="color:#f92672">=</span>[<span style="color:#ae81ff">0.15</span>, <span style="color:#ae81ff">0.4</span>, <span style="color:#ae81ff">0.8</span>])  
</span></span></code></pre></div><pre><code>count    10.00000
mean      4.50000
std       3.02765
min       0.00000
15%       1.35000
40%       3.60000
50%       4.50000
80%       7.20000
max       9.00000
dtype: float64
</code></pre>
<h3 id="g-极差">g, 极差</h3>
<p>一组数据中, 最大值与最小值之差</p>
<h3 id="h-方差">h, 方差</h3>
<p>方差体现一组数据中, 每个元素与均值的偏离程度</p>

<span class="math align-center">$$\sigma^{2}=\frac{1}{n-1} \sum_{i=1}^{n}\left(x_{i}-\bar{x}\right)^{2}$$</span><p>
<span class="math align-center">$x_{i}:$</span> 数组中的每个元素<br>

<span class="math align-center">$n:$</span> 数组元素的个数<br>

<span class="math align-center">$\bar{x}:$</span> 数组中所有元素的均值</p>
<h3 id="i-标准差">i, 标准差</h3>
<p>标准差为方差的开方. 方差和标准差可以体现数据的分散性, 越大越分散, 越小越集中. 也可体现数据波动性(稳定性), 越大波动越大, 反之亦然</p>
<p>当数据足够多时, 可用 n 代替 n - 1</p>
<p>例, 计算 <code>left_data</code> 字段的极差, 方差, 标准差:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sub <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>ptp(data[<span style="color:#e6db74">&#39;left_data&#39;</span>])  
</span></span><span style="display:flex;"><span>var <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;left_data&#39;</span>]<span style="color:#f92672">.</span>var()  
</span></span><span style="display:flex;"><span>std <span style="color:#f92672">=</span> data[<span style="color:#e6db74">&#39;left_data&#39;</span>]<span style="color:#f92672">.</span>std()  
</span></span><span style="display:flex;"><span>sub, var, std  
</span></span></code></pre></div><pre><code>(29.0, 44.631048970506306, 6.680647346665315)
</code></pre>
<p>绘图对比 <code>data1</code> 和 <code>left_data</code> 的分散程度</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">11</span>, <span style="color:#ae81ff">1</span>))  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">1.5</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(data[<span style="color:#e6db74">&#39;data1&#39;</span>], np<span style="color:#f92672">.</span>zeros(len(data)), ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;data1&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(data[<span style="color:#e6db74">&#39;left_data&#39;</span>], np<span style="color:#f92672">.</span>ones(len(data)), ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;left_data&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(data[<span style="color:#e6db74">&#39;data1&#39;</span>]<span style="color:#f92672">.</span>mean(), ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;data1均值&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(data[<span style="color:#e6db74">&#39;left_data&#39;</span>]<span style="color:#f92672">.</span>mean(), ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;left_data均值&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_22_0.png" alt="png"></p>
<h3 id="j-偏度">j, 偏度</h3>
<p>统计数据分布偏斜方向和程度的度量, 统计数据分布非对称程度的数字特征, 偏度为 0 , 对称分布, 小于 0, 左偏分别, 大于 0, 右偏分布</p>
<h3 id="k-峰度">k, 峰度</h3>
<p>表征概率密度分布曲线在平均值处峰值高低的特征数. 直观看来, 峰度反映了峰部的尖度, 峰度高意味着标准差增大是由低频度的大于或小于平均值的极端差值引起的.
在相同的标准差下，峰度越大，分布就有更多的极端值，那么其余值必然要更加集中在众数周围，其分布必然就更加陡峭</p>
<p>样本的峰度是和正态分布相比较而言的统计量, 符合正态分布的峰度为 0</p>
<p>例, 计算 <code>data</code> 中前三个字段的偏度, 峰度与标准差, 并绘图比较:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;偏度:&#39;</span>, data[<span style="color:#e6db74">&#39;data1&#39;</span>]<span style="color:#f92672">.</span>skew(), data[<span style="color:#e6db74">&#39;left_data&#39;</span>]<span style="color:#f92672">.</span>skew(), data[<span style="color:#e6db74">&#39;right_data&#39;</span>]<span style="color:#f92672">.</span>skew())  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;峰度:&#39;</span>, data[<span style="color:#e6db74">&#39;data1&#39;</span>]<span style="color:#f92672">.</span>kurt(), data[<span style="color:#e6db74">&#39;left_data&#39;</span>]<span style="color:#f92672">.</span>kurt(), data[<span style="color:#e6db74">&#39;right_data&#39;</span>]<span style="color:#f92672">.</span>kurt())  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;标准差:&#39;</span>, data[<span style="color:#e6db74">&#39;data1&#39;</span>]<span style="color:#f92672">.</span>std(), data[<span style="color:#e6db74">&#39;left_data&#39;</span>]<span style="color:#f92672">.</span>std(), data[<span style="color:#e6db74">&#39;right_data&#39;</span>]<span style="color:#f92672">.</span>std())  
</span></span></code></pre></div><pre><code>偏度: 0.0013827051273872734 -1.704193031847586 0.9122511031664028
峰度: 0.01807838530280126 2.5013831586663304 0.29539776195275813
标准差: 2.891504548352662 6.680647346665315 4.672046842962734
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(data[<span style="color:#e6db74">&#39;data1&#39;</span>], shade<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;正态&#39;</span>)  
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(data[<span style="color:#e6db74">&#39;left_data&#39;</span>], shade<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;左偏&#39;</span>)  
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>kdeplot(data[<span style="color:#e6db74">&#39;right_data&#39;</span>], shade<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;右偏&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_25_0.png" alt="png"></p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>推断统计</h1>

<p>推断统计, 通过样本推断总体的统计方法, 包括对总体的未知参数进行估计; 对关于参数的假设进行检查; 对总体进行预测预报等.
推断统计的基本问题可以分为两大类：一类是 <strong>参数估计</strong> 问题; 另一类是 <strong>假设检验</strong> 问题</p>
<h2 id="1-总体-个体与样本">1, 总体, 个体与样本</h2>
<p>总体, 要研究对象的所有数据, 获取通常比较困难. 总体中的某个数据, 就是个体. 从总体中抽取部分个体, 就构成了样本, 样本中的个体数, 称为样本容量.</p>
<h2 id="2-参数估计">2, 参数估计</h2>
<p>参数估计, 用样本指标(统计量)估计总体指标(参数). 参数估计有 <strong>点估计</strong> 和 <strong>区间估计</strong> 两种</p>
<h3 id="201-点估计">2.01, 点估计</h3>
<p>点估计是依据样本统计量估计总体中的未知参数. 通常它们是总体的某个特征值，如数学期望, 方差和相关系数等.
点估计问题就是要构造一个只依赖于样本的量，作为总体未知参数的估计值.</p>
<h3 id="202-区间估计">2.02, 区间估计</h3>
<p>区间估计是根据样本的统计量, 计算出一个可能的区间(置信区间) 和 概率(置信度), 表示总体的未知参数有多少概率位于该区间.</p>
<p><strong>注意:</strong><br>
点估计使用一个值来作为总体参数值, 能给出具体值, 但易受随机抽样影响, 准确性不够<br>
区间估计使用一个置信区间和置信度, 表示总体参数值有多少可能(置信度)会在该范围(置信区间)内, 能给出合理的范围和信心指数, 不能给出具体值</p>
<h3 id="203-中心极限定理">2.03, 中心极限定理</h3>
<p>要确定置信区间与置信度, 我们先要知道总体与样本之间, 在分布上有着怎样的联系. 中心极限定理(独立同分布的中心极限定理)给出了它们之间的联系:</p>
<p>如果总体均值为 
<span class="math align-center">$\mu$</span>, 方差为 
<span class="math align-center">$\sigma^{2}$</span>, 我们进行随机抽样, 样本容量为 n, 当 n 增大时，则样本均值 
<span class="math align-center">$\bar{X}$</span>
逐渐趋近服从均值为 
<span class="math align-center">$\mu$</span>, 方差为 
<span class="math align-center">$\sigma^{2} / n$</span> 的正态分布：</p>

<span class="math align-center">$$\bar{X} \sim N\left(\mu, \sigma^{2} / n\right)$$</span><p><strong>说明:</strong><br>
进行多次抽样，每次抽样会得到一个均值, 这些均值会围绕在总体均值左右，呈正态分布<br>
当样本容量 n 足够大时, 抽样样本均值的均值 ≈ 样本均值 
<span class="math align-center">$\bar{X}$</span> ≈ 总体均值 
<span class="math align-center">$\mu$</span>, 样本均值分布的标准差等于 
<span class="math align-center">$\sigma / \sqrt{n}$</span><br>
样本均值分布的标准差, 称为标准误差, 简称标准误</p>
<p>模拟证明:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> seaborn <span style="color:#66d9ef">as</span> sns  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.family&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SimHei&#39;</span>  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;axes.unicode_minus&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义非正态分布总体(也可以是正态分布)  </span>
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">20</span>, <span style="color:#ae81ff">5</span>, size<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>)  
</span></span><span style="display:flex;"><span>data<span style="color:#f92672">.</span>sort()  
</span></span><span style="display:flex;"><span>all_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(data[<span style="color:#ae81ff">0</span>:<span style="color:#ae81ff">8000</span>], size<span style="color:#f92672">=</span><span style="color:#ae81ff">10000</span>)  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># sns.displot(all_)  </span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 将总体的均值和标准差设为已知条件  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;总体均值:&#39;</span>, all_<span style="color:#f92672">.</span>mean(), <span style="color:#e6db74">&#39;总体标准差:&#39;</span>, all_<span style="color:#f92672">.</span>std())  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建存放每次抽样的平均值的数组(初始值为 0)  </span>
</span></span><span style="display:flex;"><span>mean_arr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>zeros(<span style="color:#ae81ff">1000</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 循环抽取 1000 个样本, 每次抽 100 个  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(mean_arr)):  
</span></span><span style="display:flex;"><span>    mean_arr[i] <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(all_, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, replace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)<span style="color:#f92672">.</span>mean()  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 验证结果  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;样本均值:&#39;</span>, mean_arr[<span style="color:#ae81ff">1</span>], <span style="color:#e6db74">&#39;样本均值的均值:&#39;</span>, mean_arr<span style="color:#f92672">.</span>mean(),   
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#39;标准误差:&#39;</span>, mean_arr<span style="color:#f92672">.</span>std(), <span style="color:#e6db74">&#39;偏度:&#39;</span>, pd<span style="color:#f92672">.</span>Series(mean_arr)<span style="color:#f92672">.</span>skew(), sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>sns<span style="color:#f92672">.</span>displot(mean_arr, kde<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show() 
</span></span></code></pre></div><pre><code>总体均值: 18.270423532980452 总体标准差: 3.8201265113791596
样本均值:
18.194948520041606
样本均值的均值:
18.26385715935595
标准误差:
0.373202226318143
偏度:
0.00746666188264042
</code></pre>
<p><img src="output_1_1.png" alt="png"></p>
<h3 id="204-正态分布的特性">2.04, 正态分布的特性</h3>
<p>正态分布: 
<span class="math align-center">$X \sim N\left(\mu, \sigma^{2}\right)$</span></p>
<p><img src="zttx.png" alt="png"></p>
<p>以均值为中心:<br>
在 1 倍标准差内包含约 68.2% 的样本数据<br>
在 2 倍标准差内包含约 95.4% 的样本数据<br>
在 3 倍标准差内包含约 99.7% 的样本数据</p>
<p>证明:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 定义标准差  </span>
</span></span><span style="display:flex;"><span>scale <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义数据  </span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(<span style="color:#ae81ff">0</span>, scale, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> times <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>):  
</span></span><span style="display:flex;"><span>    y <span style="color:#f92672">=</span> x[(x <span style="color:#f92672">&gt;</span> <span style="color:#f92672">-</span>times <span style="color:#f92672">*</span> scale) <span style="color:#f92672">&amp;</span> (x <span style="color:#f92672">&lt;</span> times <span style="color:#f92672">*</span> scale)]  
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>times<span style="color:#e6db74">}</span><span style="color:#e6db74">倍的标准差:&#39;</span>)  
</span></span><span style="display:flex;"><span>    print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;</span><span style="color:#e6db74">{</span>len(y) <span style="color:#f92672">*</span> <span style="color:#ae81ff">100</span> <span style="color:#f92672">/</span> len(x)<span style="color:#e6db74">}</span><span style="color:#e6db74">%&#39;</span>)  
</span></span></code></pre></div><pre><code>1倍的标准差:
68.206%
2倍的标准差:
95.354%
3倍的标准差:
99.711%
</code></pre>
<h3 id="205-重要结论">2.05, 重要结论</h3>
<p>根据中心极限定理和正态分布的特性, 如果总体标准差为
<span class="math align-center">$\sigma$</span>, 对总体进行一次抽样, 如果样本足够大, 则样品均值 
<span class="math align-center">$\bar{X}$</span>
服从正态分布, 该均值约有 95.4% 的概率会在 2 倍的标准误差 (
<span class="math align-center">$\mu - 2\sigma / \sqrt{n}, \mu + 2\sigma / \sqrt{n}$</span>) 范围内, 并且该样本均值约等于总体均值 
<span class="math align-center">$\mu$</span>. 从而, 可以利用这一结论, 对总体均值进行区间估计.</p>
<p><strong>结论验证:</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 随机生成总体均值, 其值未知  </span>
</span></span><span style="display:flex;"><span>mean <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10000</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 总体的标准差已知为 50  </span>
</span></span><span style="display:flex;"><span>std <span style="color:#f92672">=</span> <span style="color:#ae81ff">50</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义总体数据  </span>
</span></span><span style="display:flex;"><span>all_ <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>normal(mean, std, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100000</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 从总体抽取 100 个元素构成样本  </span>
</span></span><span style="display:flex;"><span>sample <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>choice(all_, size<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, replace<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算样本均值  </span>
</span></span><span style="display:flex;"><span>sample_mean <span style="color:#f92672">=</span> sample<span style="color:#f92672">.</span>mean()  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;样本均值:&#39;</span>, sample_mean)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算样本的标准误差  </span>
</span></span><span style="display:flex;"><span>se <span style="color:#f92672">=</span> std <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(n)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算置信区间 95%置信度  </span>
</span></span><span style="display:flex;"><span>min_ <span style="color:#f92672">=</span> sample_mean <span style="color:#f92672">-</span> <span style="color:#ae81ff">1.96</span> <span style="color:#f92672">*</span> se  
</span></span><span style="display:flex;"><span>max_ <span style="color:#f92672">=</span> sample_mean <span style="color:#f92672">+</span> <span style="color:#ae81ff">1.96</span> <span style="color:#f92672">*</span> se  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;置信区间(95%置信度):&#39;</span>, (min_, max_))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 区间估计  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;总体均值有 95% 的概率在</span><span style="color:#e6db74">{</span>(min_, max_)<span style="color:#e6db74">}</span><span style="color:#e6db74">区间内&#39;</span>)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;总体均值:&#39;</span>, mean)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘图辅助  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(mean, <span style="color:#ae81ff">0</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;*&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span>, ms<span style="color:#f92672">=</span><span style="color:#ae81ff">12</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;总体均值&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(sample_mean, <span style="color:#ae81ff">0</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;样本均值&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>hlines(<span style="color:#ae81ff">0</span>, xmin<span style="color:#f92672">=</span>min_, xmax<span style="color:#f92672">=</span>max_, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;b&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;置信区间&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(min_, <span style="color:#ae81ff">0.4</span>, <span style="color:#ae81ff">0.6</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;左边界&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>axvline(max_, <span style="color:#ae81ff">0.4</span>, <span style="color:#ae81ff">0.6</span>, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;--&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;右边界&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><pre><code>样本均值: 9695.658932218576
置信区间(95%置信度): (9685.858932218576, 9705.458932218575)
总体均值有 95% 的概率在(9685.858932218576, 9705.458932218575)区间内
总体均值: 9696
</code></pre>
<p><img src="output_5_1.png" alt="png"></p>
<h2 id="3-假设检验">3, 假设检验</h2>
<p>假设检验(显著性检验), 先对总体做出假设, 然后通过判断样本与总体之间是否存在显著性差异, 来验证总体的假设</p>
<p>假设检验使用了一种类似于 “反证法” 的推理方法，它的特点是：</p>
<ul>
<li>
<p>先对总体做出两个完全相反的假设, 原假设(设为真) 和 备择假设, 计算后导致不合理现象产生，则拒绝原假设, 接受备择假设, 反之接受原假设, 放弃备择假设</p>
</li>
<li>
<p>这种 “反证法” 不同于一般的反证法. 所谓不合理现象产生，并非指形式逻辑上的绝对矛盾，而是基于小概率原理：概率很小的事件在一次试验中几乎是不可能发生的，若发生了，就是不合理的.</p>
</li>
<li>
<p>怎样才算 “小概率”, 通常可将概率不超过 0.05 的事件称为 “小概率事件” ，也可视具体情形而取 0.1 或 0.01 等. 在假设检验中常记这个概率为 α，称为显著性水平</p>
</li>
</ul>
<p>假设检验可分为正态分布检验, 正态总体均值检验, 非参数检验三类, 本文只介绍 <strong>正态总体均值检验</strong> , 包括 Z检验 和 t检验 两种情况</p>
<h3 id="301-关键概念">3.01, 关键概念:</h3>
<p>对总体参数做出两个完全对立的假设, 分别为:<br>
<strong>原假设(零假设) 
<span class="math align-center">$H_{0}$</span><br>
备择假设(对立假设) 
<span class="math align-center">$H_{1}$</span></strong></p>
<p><strong>双边假设检验</strong> :<br>

<span class="math align-center">$H_{0}: \mu=\mu_{0}, H_{1}: \mu \neq \mu_{0}$</span></p>
<p><strong>单边假设检验</strong> :<br>

<span class="math align-center">$H_{0}: \mu \geq \mu_{0}, H_{1}: \mu<\mu_{0}$</span> (<strong>左边检验</strong>)<br>

<span class="math align-center">$H_{0}: \mu \leq \mu_{0}, H_{1}: \mu>\mu_{0}$</span> ( <strong>右边检验</strong> )<br>

<span class="math align-center">$\mu$</span> 为总体均值, 
<span class="math align-center">$\mu_{0}$</span> 为假设均值</p>
<p><strong>显著性水平</strong> : 根据需要设定的小概率事件的概率 α (1 - α 为置信度)</p>
<p><strong>检验统计量</strong> (Z 和 t): 用来判断样本均值与总体均值是否存在显著性差异</p>
<p><strong>P值:</strong> 通过检验统计量计算而得的概率值, 表示原假设可被拒绝的最小值(或可支持原假设的概率):<br>
P ≤ α, 原假设可被拒绝的最小值比显著性水平还低, 原假设可被拒绝, 则拒绝原假设<br>
P &gt; α, 原假设可被拒绝的最小值大于显著性水平, 原假设不可被拒绝, 支持原假设</p>
<h3 id="302-假设检验的步骤">3.02, 假设检验的步骤</h3>
<p>设置原假设与备择假设<br>
设置显著性水平 α<br>
根据问题选择假设检验的方式<br>
计算统计量(Z 或 t)<br>
计算 P值(Z 或 t 围成的分布面积)<br>
根据 P值 与 α值, 决定接受原假设还是备择假设</p>
<p>例, 某车间用一台包装机包装葡萄糖. 袋装糖的净重是一个随机变量，它服从正态分布. 当机器正常时，其均值为 0.5kg，标准差为 0.015kg.
某日开工后为检验包装机是否正常，随机地抽取它所包装的糖 9 袋，称得净重为(kg):<br>
0.497, 0.506, 0.518, 0.524, 0.498, 0.511, 0.520, 0.515, 0.512<br>
判断下面说法是否正确:<br>
(1) 机器正常</p>
<p>例, 某车间用包装机包装葡萄糖. 袋装糖的净重是一个随机变量，它服从正态分布. 随机地抽取糖 9 袋，称得净重为(kg):<br>
0.497, 0.506, 0.518, 0.524, 0.498, 0.511, 0.520, 0.515, 0.512<br>
判断下面说法是否正确:<br>
(2) 该车间袋装糖净重均值为 0.5kg<br>
(3) 该车间袋装糖净重均值不少于 0.5kg<br>
(4) 该车间袋装糖净重均值不多于 0.5kg</p>
<h3 id="303-z检验">3.03, Z检验</h3>
<p>Z检验适用于: 总体正态分布且方差已知, 样本容量较大(一般 ≥ 30)</p>
<p>Z统计量计算公式:</p>

<span class="math align-center">$$Z=\frac{\bar{x}-\mu_{0}}{S_{\bar{x}}}=\frac{\bar{x}-\mu_{0}}{\sigma /
\sqrt{n}}$$</span><p>
<span class="math align-center">$\bar{x}$</span>: 样本均值<br>

<span class="math align-center">$\mu_{0}$</span>: 假设的总体均值<br>

<span class="math align-center">$S_{\bar{x}}$</span>: 样本的标准误差<br>

<span class="math align-center">$\sigma$</span>: 总体的标准差<br>

<span class="math align-center">$n$</span>: 样本容量</p>
<p>检验说法(1): 机器正常</p>
<p>双边检验:<br>
原假设机器正常: 
<span class="math align-center">$H_{0}: \mu=\mu_{0}=0.5kg$</span><br>
备择假设机器不正常: 
<span class="math align-center">$H_{1}: \mu \neq \mu_{0} \neq 0.5kg$</span><br>
设置显著性水平: α = 0.05</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy <span style="color:#f92672">import</span> stats  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 样本已知  </span>
</span></span><span style="display:flex;"><span>a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.497</span>, <span style="color:#ae81ff">0.506</span>, <span style="color:#ae81ff">0.518</span>, <span style="color:#ae81ff">0.524</span>, <span style="color:#ae81ff">0.498</span>, <span style="color:#ae81ff">0.511</span>, <span style="color:#ae81ff">0.520</span>, <span style="color:#ae81ff">0.515</span>, <span style="color:#ae81ff">0.512</span>])  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 总体均值和标准差已知  </span>
</span></span><span style="display:flex;"><span>mean, std <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>, <span style="color:#ae81ff">0.015</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算样本均值  </span>
</span></span><span style="display:flex;"><span>sample_mean <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>mean()  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算样本标准误差  </span>
</span></span><span style="display:flex;"><span>se <span style="color:#f92672">=</span> std <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(len(a))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算 Z统计量  </span>
</span></span><span style="display:flex;"><span>Z <span style="color:#f92672">=</span> (sample_mean <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> se  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Z统计量:&#39;</span>, Z)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算 P值, 双边检验: Z值与其右边曲线围成的面积的 2 倍  </span>
</span></span><span style="display:flex;"><span>P <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> stats<span style="color:#f92672">.</span>norm<span style="color:#f92672">.</span>sf(abs(Z))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;P值:&#39;</span> , P)  
</span></span></code></pre></div><pre><code>Z统计量: 2.244444444444471
P值: 0.02480381963225589
</code></pre>
<p><img src="mianji.png" alt=""></p>
<p>由结果可知, Z值 超过了 1.96, 由 Z值 与其右边曲线围成的面积的 2 倍, 必然小于 α(1.96 与其右边曲线围成的面积的 2 倍), 计算结果 P &lt; α, 因此拒绝原假设, 接受备择假设, 机器不正常</p>
<h3 id="304-t检验">3.04, t检验</h3>
<p>t检验适用于: 总体正态分布, 方差未知, 样本数量较少(一般 &lt; 30), 但是随着样本容量的增加, 分布逐渐趋于正态分布</p>
<p><img src="tzyd.png" alt=""></p>
<p>t统计量计算公式:</p>

<span class="math align-center">$$t=\frac{\bar{x}-\mu_{0}}{S_{\bar{x}}}=\frac{\bar{x}-\mu_{0}}{S / \sqrt{n}}$$</span><p>
<span class="math align-center">$\bar{x}$</span>: 样本均值<br>

<span class="math align-center">$\mu_{0}$</span>: 假设的总体均值<br>

<span class="math align-center">$S_{\bar{x}}$</span>: 样本的标准误差<br>

<span class="math align-center">$S$</span>: 样本的标准差<br>

<span class="math align-center">$n$</span>: 样本容量</p>
<p><strong>双边检验</strong> :<br>
检验说法(2): 该车间袋装糖净重均值为 0.5kg</p>
<p>原假设, 该车间袋装糖净重均值为 0.5kg: 
<span class="math align-center">$H_{0}: \mu=\mu_{0}=0.5kg$</span><br>
备择假设, 该车间袋装糖净重均值不为 0.5kg: 
<span class="math align-center">$H_{1}: \mu \neq \mu_{0} \neq 0.5kg$</span><br>
设置显著性水平: α = 0.05</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 样本已知  </span>
</span></span><span style="display:flex;"><span>a <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.497</span>, <span style="color:#ae81ff">0.506</span>, <span style="color:#ae81ff">0.518</span>, <span style="color:#ae81ff">0.524</span>, <span style="color:#ae81ff">0.498</span>, <span style="color:#ae81ff">0.511</span>, <span style="color:#ae81ff">0.520</span>, <span style="color:#ae81ff">0.515</span>, <span style="color:#ae81ff">0.512</span>])  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 假设的总体均值已知  </span>
</span></span><span style="display:flex;"><span>mean <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.5</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算样本均值  </span>
</span></span><span style="display:flex;"><span>sample_mean <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>mean()  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算样本标准差  </span>
</span></span><span style="display:flex;"><span>std <span style="color:#f92672">=</span> a<span style="color:#f92672">.</span>std()  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算 t统计量  </span>
</span></span><span style="display:flex;"><span>t <span style="color:#f92672">=</span> (sample_mean <span style="color:#f92672">-</span> mean) <span style="color:#f92672">/</span> ( std <span style="color:#f92672">/</span> np<span style="color:#f92672">.</span>sqrt(len(a)))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;t统计量:&#39;</span>, t)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算 P值, df 是自由度: 样本变量可自由取值的个数  </span>
</span></span><span style="display:flex;"><span>P <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span> <span style="color:#f92672">*</span> stats<span style="color:#f92672">.</span>t<span style="color:#f92672">.</span>sf(abs(t), df<span style="color:#f92672">=</span>len(a) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;P值:&#39;</span>, P)  
</span></span></code></pre></div><pre><code>t统计量: 3.802382179137283
P值: 0.005218925008708613
</code></pre>
<p>P &lt; α, 拒绝原假设, 接受备择假设: 该车间袋装糖净重均值不为 0.5kg</p>
<p>还可以通过 scipy 提供的方法 <code>ttest_1samp</code> 来进行 t检验计算:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> scipy <span style="color:#f92672">import</span> stats
</span></span><span style="display:flex;"><span>stats<span style="color:#f92672">.</span>ttest_1samp(a, <span style="color:#ae81ff">0.5</span>)  
</span></span></code></pre></div><pre><code>Ttest_1sampResult(statistic=3.584920298041139, pvalue=0.007137006417828698)
</code></pre>
<p><strong>左边检验</strong> :<br>
检验说法(3): 该车间袋装糖净重均值不少于 0.5kg</p>
<p>原假设, 该车间袋装糖净重均值不少于 0.5kg: 
<span class="math align-center">$H_{0}: \mu \geq \mu_{0}$</span><br>
备择假设, 该车间袋装糖净重均值少于 0.5kg: 
<span class="math align-center">$H_{1}: \mu<\mu_{0}$</span><br>
设置显著性水平: α = 0.05</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># t统计量上述已经计算, 只需计算 P值: t统计量与其左边曲线围成的面积  </span>
</span></span><span style="display:flex;"><span>P <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>t<span style="color:#f92672">.</span>cdf(t, df<span style="color:#f92672">=</span>len(a) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;P值:&#39;</span>, P)  
</span></span></code></pre></div><pre><code>P值: 0.9973905374956458
</code></pre>
<p>P &gt; α, 接受原假设, 该车间袋装糖净重均值不少于 0.5kg</p>
<p><strong>右边检验</strong> :<br>
检验说法(4): 该车间袋装糖净重均值不多于 0.5kg</p>
<p>原假设, 该车间袋装糖净重均值不多于 0.5kg: 
<span class="math align-center">$H_{0}: \mu \leq \mu_{0}$</span><br>
备择假设, 该车间袋装糖净重均值多于 0.5kg: 
<span class="math align-center">$H_{1}: \mu>\mu_{0}$</span><br>
设置显著性水平: α = 0.05</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 计算 P值: t统计量与其右边曲线围成的面积  </span>
</span></span><span style="display:flex;"><span>P <span style="color:#f92672">=</span> stats<span style="color:#f92672">.</span>t<span style="color:#f92672">.</span>sf(t, df<span style="color:#f92672">=</span>len(a) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;P值:&#39;</span>, P)  
</span></span></code></pre></div><pre><code>P值: 0.0026094625043543065
</code></pre>
<p>P &lt; α, 拒绝原假设, 接受备择假设, 该车间袋装糖净重均值多于 0.5kg</p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>线性回归</h1>

<h2 id="1-模型">1, 模型</h2>
<p>模型是指对于某个(类)实际问题的求解或客观事物运行规律进行抽象后的一种形式化表达方式, 可以理解为一个函数(一种映射规则)</p>
<p>任何模型都是由三个部分组成: 目标, 变量和关系.
建模时明确了模型的目标，才能进一步确定影响目标(因变量)的各关键变量(自变量)，进而确定变量之间的关系(函数关系)</p>
<p>通过大量数据检验(训练)模型, 将模型(函数)的各个参数求解, 当参数确定之后, 便可利用模型对未知数据进行求值, 预测</p>
<p>用于训练模型的样本数据中的每个属性称为特征, 用 x 表示, 样本中的每条数据经过模型计算得到的输出值称为标签(监督学习), 用 y 表示, 从而得到 y
= f(x) 的函数关系</p>
<h2 id="2-回归分析">2, 回归分析</h2>
<p>在统计学中, 回归分析指的是确定两种或两种以上变量间相互依赖的定量关系的一种统计分析方法</p>
<p>回归分析按照涉及的变量的多少，分为一元回归分析和多元回归分析；按照因变量的多少，可分为简单回归分析和多重回归分析；按照自变量和因变量之间的关系类型，可分为线性回归分析和非线性回归分析</p>
<p>回归分析解释自变量 x 发生改变, 因变量 y 会如何改变</p>
<p><strong>拟合</strong> , 插值 和 逼近 是数值分析的三大基础工具. 线性回归和非线性回归, 也叫线性拟合和非线性拟合,
拟合就是从整体上靠近已知点列，构造一种算法(模型或函数), 使得算法能够更加符合真实数据</p>
<h2 id="3-简单线性回归">3, 简单线性回归</h2>
<p>线性回归分析的自变量和因变量之间是线性关系, 只有一个自变量时称为 <strong>简单线性回归</strong> , 多个自变量时称为 <strong>多元线性回归</strong></p>
<p>简单线性回归方程:</p>

<span class="math align-center">$$\hat{y}=w * x+b$$</span><p>
<span class="math align-center">$\hat{y}$</span> 为因变量, x 为自变量, w 为比例关系, b 为截距, w 和 b 就是模型的参数. 例如房屋价格与房屋面积的正比例关系</p>
<h2 id="4-多元线性回归">4, 多元线性回归</h2>
<p>现实生活中自变量通常不止一个, 例如影响房屋价格的, 除了房屋面积, 还有交通, 地段, 新旧, 楼层等等因素.
不同的因素对房屋的价格影响力度(权重)不同, 因此使用多个因素来分析房屋的价格(各个因素与房屋价格近似线性关系), 可以得出多元线性回归方程:</p>

<span class="math align-center">$\hat{y}=w_{1} * x_{1}+w_{2} * x_{2}+w_{3} * x_{3}+\cdots+w_{n} * x_{n}+b$</span>
<p>
<span class="math align-center">$x$</span>: 影响因素, 特征<br>

<span class="math align-center">$w$</span>: 每个 x 的影响力度<br>

<span class="math align-center">$n$</span>: 特征个数<br>

<span class="math align-center">$\hat{y}$</span>: 房屋的预测价格</p>
<p>令:</p>

<span class="math align-center">$x_{0}=1, w_{0}=b$</span>
<p>设 
<span class="math align-center">$\vec{w}$</span> 和 
<span class="math align-center">$\vec{x}$</span> 为两个向量如下:</p>

<span class="math align-center">$$\vec{w}=\left(w_{0}, w_{1}, w_{2}, w_{3}, \ldots, w_{n}\right)^{T}$$  

$$\vec{x}=\left(x_{0}, x_{1}, x_{2}, x_{3}, \ldots, x_{n}\right)^{T}$$</span><p>则方程可表示为:</p>

<span class="math align-center">$$\begin{aligned}  
\hat{y} &=w_{0} * x_{0}+w_{1} * x_{1}+w_{2} * x_{2}+w_{3} * x_{3}+\ldots
\ldots+w_{n} * x_{n} \  
=\sum_{j=0}^{n} w_{j} * x_{j} \  
=\vec{w}^{T} \cdot \vec{x}  
\end{aligned}$$</span><p>接下来只需要计算出参数 
<span class="math align-center">$\vec{w}^{T}$</span>, 便可以建立模型</p>
<h2 id="5-损失函数">5, 损失函数</h2>
<p>损失函数, 用来衡量模型预测值与真实值之间的差异的函数, 也称目标函数或代价函数. 损失函数的值越小, 表示预测值与真实值之间的差异越小.</p>
<p>因此, 求解上述模型的参数 
<span class="math align-center">$\vec{w}^{T}$</span>, 就是要建立一个关于模型参数的损失函数(以模型参数 
<span class="math align-center">$\vec{w}^{T}$</span> 为自变量的函数),
然而 
<span class="math align-center">$\vec{w}^{T}$</span> 的取值组合是无限的, 目标就是通过机器学习, 求出一组最佳组合, 使得损失函数的值最小</p>
<p>在线性回归中, 使用平方损失函数(最小二乘法), 用 J(w) 表示:</p>

<span class="math align-center">$$\begin{array}{l}  
J(w)=\frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\hat{y}^{(i)}\right)^{2} \  
=\frac{1}{2} \sum_{i=1}^{m}\left(y^{(i)}-\vec{w}^{T} \vec{x}^{(i)}\right)^{2}  
\end{array}$$</span><p>m: 样本(训练集)数据的条数<br>

<span class="math align-center">$y^{(i)}$</span>: 样本第 i 条数据的真实值<br>

<span class="math align-center">$\hat{y}^{(i)}$</span>: 样本第 i 条数据的预测值<br>

<span class="math align-center">$\vec{x}^{(i)}$</span>: 样本第 i 条数据的特征</p>
<p>m, 
<span class="math align-center">$y^{(i)}$</span> 和 
<span class="math align-center">$\vec{x}^{(i)}$</span> 已知, 要使 J(w) 最小, 对 
<span class="math align-center">$\vec{w}^{T}$</span> 求导并令导数等于 0 ,
便可求得 
<span class="math align-center">$\vec{w}^{T}$</span>, 然后将样本(训练集)输入通过机器学习计算出具体的 
<span class="math align-center">$\vec{w}^{T}$</span></p>
<h2 id="6-回归模型评估">6, 回归模型评估</h2>
<p>建立模型之后, 模型的效果如何, 需要进行评估, 对于回归模型, 可用如下指标来衡量:</p>
<p><strong>MSE</strong> :<br>
平均平方误差, 所有样本数据误差的平方和取均值:</p>

<span class="math align-center">$$M S E=\frac{1}{m} \sum_{i=1}^{m}\left(y^{(i)}-\hat{y}^{(i)}\right)^{2}$$</span><p><strong>RMSE</strong> :<br>
平均平方误差的平方根:</p>

<span class="math align-center">$$R M S E=\sqrt{M S E}=\sqrt{\frac{1}{m}
\sum_{i=1}^{m}\left(y^{(i)}-\hat{y}^{(i)}\right)^{2}}$$</span><p><strong>MAE</strong> :<br>
平均绝对值误差, 所有样本数据误差的绝对值的和取均值:</p>

<span class="math align-center">$$M A E=\frac{1}{m} \sum_{i=1}^{m}\left|y^{(i)}-\hat{y}^{(i)}\right|$$</span><p>上述指标越小越好, 小到什么程度, 不同的对象建立的模型不一样</p>
<p><strong>R²</strong> :<br>
决定系数，反应因变量的全部变异能通过回归关系被自变量解释的比例. 如 R²=0.8，则表示回归关系可以解释因变量 80% 的变异.
换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少 80%</p>
<p>在训练集中 R² 取值范围为 [0, 1], 在测试集(未知数据)中, R² 的取值范围为 [-∞, 1], R² 的值越大, 模型拟合越好</p>
<p>R² 的计算公式:</p>

<span class="math align-center">$$R^{2}=1-\frac{R S S}{T S
S}=1-\frac{\sum_{i=1}^{m}\left(y^{(i)}-\hat{y}^{(i)}\right)^{2}}{\sum_{i=1}^{m}\left(y^{(i)}-\bar{y}\right)^{2}}$$</span><p>
<span class="math align-center">$\bar{y}$</span>: 样本(测试集)的平均值</p>
<p>不管何种对象建立的模型, R² 都是越大模拟越好</p>
<p><strong>例一, 简单线性回归模型: 求鸢尾花花瓣长度和宽度的关系</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 导入用于线性回归的类  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 切分训练集与测试集的模块  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 鸢尾花数据集  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_iris  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置输出数据的精度为 2 (默认是8)  </span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>set_printoptions(precision<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 获取花瓣长度 x, 宽度 y  </span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> load_iris()  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data[:, <span style="color:#ae81ff">2</span>]<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>), iris<span style="color:#f92672">.</span>data[:, <span style="color:#ae81ff">3</span>]  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 将数据拆分为训练集和测试集, 指定测试集占比 test_size  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 指定随机种子 random_state(可以任意值但必须确定), 锁定拆分行为  </span>
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(  
</span></span><span style="display:flex;"><span>    x, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用训练集训练模型  </span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LinearRegression()  
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 求得模型参数  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;权重 w:&#39;</span>, lr<span style="color:#f92672">.</span>coef_, <span style="color:#e6db74">&#39;截距 b:&#39;</span>, lr<span style="color:#f92672">.</span>intercept_)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 调用模型进行预测  </span>
</span></span><span style="display:flex;"><span>y_hat <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict(x_test)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 结果可视化  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.family&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SimHei&#39;</span>  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;axes.unicode_minus&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.size&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">10</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x_train, y_train, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;orange&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;训练集&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x_test, y_test, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;D&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;测试集&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(x, lr<span style="color:#f92672">.</span>predict(x), <span style="color:#e6db74">&#39;r-&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;花瓣长度&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;花瓣宽度&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><pre><code>权重 w: [0.42] 截距 b: -0.370615595909495
</code></pre>
<p><img src="output_1_1.png" alt="png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 模型评估  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> mean_squared_error, mean_absolute_error, r2_score  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;MSE:&#39;</span>, mean_squared_error(y_test, y_hat))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;RMSE:&#39;</span>, np<span style="color:#f92672">.</span>sqrt(mean_squared_error(y_test, y_hat)))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;MAE:&#39;</span>, mean_absolute_error(y_test, y_hat))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;训练集R²:&#39;</span>, r2_score(y_train, lr<span style="color:#f92672">.</span>predict(x_train))) <span style="color:#75715e"># 可换成 lr.score(x_train, y_train)  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;测试集R²:&#39;</span>, r2_score(y_test, y_hat)) <span style="color:#75715e"># 可换成 lr.score(x_test, y_test)  </span>
</span></span></code></pre></div><pre><code>MSE: 0.047866747643216113
RMSE: 0.21878470614559903
MAE: 0.1543808898175286
训练集R²: 0.9317841638431329
测试集R²: 0.9119955391492289
</code></pre>
<p><strong>列二, 多元线性回归模型: 波士顿房价预测</strong></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_boston  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>boston <span style="color:#f92672">=</span> load_boston()  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> boston<span style="color:#f92672">.</span>data, boston<span style="color:#f92672">.</span>target  
</span></span><span style="display:flex;"><span>df <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(np<span style="color:#f92672">.</span>concatenate([x, y<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),   
</span></span><span style="display:flex;"><span>                 columns<span style="color:#f92672">=</span>boston<span style="color:#f92672">.</span>feature_names<span style="color:#f92672">.</span>tolist() <span style="color:#f92672">+</span> [<span style="color:#e6db74">&#39;MEDV&#39;</span>])  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 部分数据  </span>
</span></span><span style="display:flex;"><span>df<span style="color:#f92672">.</span>head(<span style="color:#ae81ff">3</span>)  
</span></span></code></pre></div><pre tabindex="0"><code>  CRIM	ZN	INDUS	CHAS	NOX	RM	AGE	DIS	RAD	TAX	PTRATIO	B	LSTAT	MEDV
0	0.00632	18.0	2.31	0.0	0.538	6.575	65.2	4.0900	1.0	296.0	15.3	396.90	4.98	24.0
1	0.02731	0.0	7.07	0.0	0.469	6.421	78.9	4.9671	2.0	242.0	17.8	396.90	9.14	21.6
2	0.02729	0.0	7.07	0.0	0.469	7.185	61.1	4.9671	2.0	242.0	17.8	392.83	4.03	34.7
</code></pre><div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(  
</span></span><span style="display:flex;"><span>    x, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>)  
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LinearRegression()  
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;权重:&#39;</span>, lr<span style="color:#f92672">.</span>coef_)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;截距:&#39;</span>, lr<span style="color:#f92672">.</span>intercept_)  
</span></span><span style="display:flex;"><span>y_hat <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict(x_test)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;训练集R²:&#39;</span>, lr<span style="color:#f92672">.</span>score(x_train, y_train))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;测试集R²:&#39;</span>, lr<span style="color:#f92672">.</span>score(x_test, y_test))   
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 假如获取了一间房屋的数据, 预测其房价  </span>
</span></span><span style="display:flex;"><span>room_data <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.00732</span>, <span style="color:#ae81ff">17.0</span>, <span style="color:#ae81ff">1.31</span>, <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">0.638</span>, <span style="color:#ae81ff">7.575</span>, <span style="color:#ae81ff">62.2</span>, <span style="color:#ae81ff">5.0900</span>,  
</span></span><span style="display:flex;"><span>                      <span style="color:#ae81ff">1.0</span>, <span style="color:#ae81ff">296.0</span>, <span style="color:#ae81ff">15.3</span>, <span style="color:#ae81ff">396.90</span>, <span style="color:#ae81ff">4.98</span>])<span style="color:#f92672">.</span>reshape(<span style="color:#ae81ff">1</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>y_price <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict(room_data)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;房屋价格:&#39;</span>, y_price)  
</span></span></code></pre></div><pre><code>权重: [-1.53e-01  4.79e-02 -8.60e-03  2.58e+00 -1.46e+01  3.96e+00 -7.92e-03
 -1.46e+00  3.45e-01 -1.25e-02 -9.19e-01  1.32e-02 -5.17e-01]
截距: 32.214120389743606
训练集R²: 0.7468034208269784
测试集R²: 0.7059096071098042
房屋价格: [33.62]
</code></pre>
<p>多元线性回归在空间中, 可表示为一个超平面去拟合空间中的数据点</p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>逻辑回归</h1>

<p>逻辑回归和线性回归有类似之处, 都是利用线性加权计算的模型, 但逻辑回归是分类算法, 例如对是否患癌症进行预测, 因变量就是 <strong>是</strong> 和 <strong>否</strong> ,
两个类别, 自变量可以是年龄, 性别, 饮食, 作息, 病菌感染等, 自变量既可以是数值变量, 也可以是类别变量</p>
<h2 id="1-逻辑回归二分类推导">1, 逻辑回归二分类推导</h2>
<p>和线性回归类似, 设自变量为 x, 每个自变量的权重为 w, 令:</p>

<span class="math align-center">$$\begin{array}{l}  
z=w_{1} x_{1}+w_{2} x_{2}+\cdots+w_{n} x_{n}+b \  
=\sum_{j=1}^{n} w_{j} x_{j}+b \  
=\sum_{j=0}^{n} w_{j} x_{j} \  
=\vec{w}^{T} \cdot \vec{x}  
\end{array}$$</span><p>z 是一个连续值, 取值范围(-∞, +∞), 为了实现分类, 一般设置阈值 z = 0, 当 z &gt; 0 时, 将样本判定为一个类别(正例), 该类别设为
1, 当 z ≤ 0 时, 判定为另一个类别(负例), 该类别设为 0, 再设因变量为 y, 从而逻辑回归方程可表示为:</p>

<span class="math align-center">$y=1, z>0$</span>

<span class="math align-center">$y=0, z \leq 0$</span>
<p>上述方程虽然实现了分类, 但提供的信息有限, 因此引入 <strong>sigmoid函数</strong> (也叫 Logistic函数), 将 z 映射到 (0, 1)
区间，可以实现二分类的同时, 还能体现将样本分为某个类的可能性, 这个可能性设为 p:</p>

<span class="math align-center">$$p=\operatorname{sigmoid}(z)=\frac{1}{1+e^{-z}}$$</span><p>sigmoid 函数图像如下:</p>
<p><img src="sigmoid.png" alt=""></p>
<p>于是, 逻辑回归方程又可表示为:</p>

<span class="math align-center">$y=1, p>0.5$</span>

<span class="math align-center">$y=0, 1-p \geq 0.5$</span>
<p>从而可见, 通过比较 p 和 1-p 哪个更大(z 的阈值不取 0 时做出调整即可), 预测结果就是对应的一类</p>
<h2 id="2-逻辑回归的损失函数">2, 逻辑回归的损失函数</h2>
<p>通过上述推导过程可知, 要得到逻辑回归模型, 最终就是要求得参数 
<span class="math align-center">$\vec{w}^{T}$</span>, 于是将 p 和 1-p 统一, 构造一个损失函数来求

<span class="math align-center">$\vec{w}^{T}$</span>:</p>

<span class="math align-center">$$p(y=1 | x ; w)=s(z)$$</span>
<span class="math align-center">$$p(y=0 | x ; w)=1-s(z)$$</span><p>合并:</p>

<span class="math align-center">$$p(y | x ; w)=s(z)^{y}(1-s(z))^{1-y}$$</span><p>上式表示一个样本的概率, 我们要求解能够使所有样本联合概率密度最大的 
<span class="math align-center">$\vec{w}^{T}$</span> 值, 根据极大似然估计,
所有样本的联合概率密度函数(似然函数)为:</p>

<span class="math align-center">$$\begin{array}{l}  
L(w)=\prod_{i=1}^{m} p\left(y^{(i)} | x^{(i)} ; w\right) \  
=\prod_{i=1}^{m}
s\left(z^{(i)}\right)^{y^{(i)}}\left(1-s\left(z^{(i)}\right)\right)^{1-y^{(i)}}  
\end{array}$$</span><p>取对数, 让累积乘积变累积求和:</p>

<span class="math align-center">$$\begin{array}{l}  
\ln L(w)=\ln \left(\prod_{i=1}^{m}
s\left(z^{(i)}\right)^{y^{(i)}}\left(1-s\left(z^{(i)}\right)^{1-y^{(i)}}\right)\right)
\  
=\sum_{i=1}^{m}\left(y^{(i)} \ln s\left(z^{(i)}\right)+\left(1-y^{(i)}\right)
\ln \left(1-s\left(z^{(i)}\right)\right)\right)  
\end{array}$$</span><p>要求上式最大值, 取反变成求最小值, 就作为逻辑回归的损失函数(交叉熵损失函数):</p>

<span class="math align-center">$$J(w)=-\sum_{i=1}^{m}\left(y^{(i)} \ln
s\left(z^{(i)}\right)+\left(1-y^{(i)}\right) \ln
\left(1-s\left(z^{(i)}\right)\right)\right)$$</span><p>利用梯度下降法最终求得 
<span class="math align-center">$\vec{w}^{T}$</span> (省略)</p>
<p>例, 对鸢尾花实现二分类并分析:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_iris  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> warnings  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>warnings<span style="color:#f92672">.</span>filterwarnings(<span style="color:#e6db74">&#39;ignore&#39;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> load_iris()  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data, iris<span style="color:#f92672">.</span>target  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 鸢尾花数据集有 3 个类别, 4 个特性, 取两个类别, 两个特性  </span>
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x[y<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>:]  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> y[y<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>]  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 拆分训练集与测试集  </span>
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(x, y,  
</span></span><span style="display:flex;"><span>        test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 训练分类模型  </span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LogisticRegression()  
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试  </span>
</span></span><span style="display:flex;"><span>y_hat <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict(x_test)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;权重:&#39;</span>, lr<span style="color:#f92672">.</span>coef_)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;偏置:&#39;</span>, lr<span style="color:#f92672">.</span>intercept_)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;真实值:&#39;</span>, y_test)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;预测值:&#39;</span>, y_hat)  
</span></span></code></pre></div><pre><code>权重: [[2.54536368 2.15257324]]
偏置: [-16.08741502]
真实值: [2 1 2 1 1 1 1 1 1 1 2 2 2 1 1 1 1 1 2 2 1 1 2 1 2]
预测值: [2 1 1 1 1 1 1 2 1 1 2 2 2 1 1 1 1 1 2 2 1 1 2 1 2]
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 样本的真实类别可视化  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.family&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SimHei&#39;</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 取出两种鸢尾花的特征  </span>
</span></span><span style="display:flex;"><span>c1 <span style="color:#f92672">=</span> x[y<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>]  
</span></span><span style="display:flex;"><span>c2 <span style="color:#f92672">=</span> x[y<span style="color:#f92672">==</span><span style="color:#ae81ff">2</span>]  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制样本分布  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x<span style="color:#f92672">=</span>c1[:, <span style="color:#ae81ff">0</span>], y<span style="color:#f92672">=</span>c1[:, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;类别1&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x<span style="color:#f92672">=</span>c2[:, <span style="color:#ae81ff">0</span>], y<span style="color:#f92672">=</span>c2[:, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;类别2&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;花瓣长度&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;花瓣宽度&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;鸢尾花样本分布&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_3_0.png" alt="png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 将预测类别和真实类别可视化对比  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">2.2</span>))  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(y_test, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, ms<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;真实类别&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(y_hat, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;x&#39;</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;&#39;</span>, ms<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;预测类别&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;样本序号&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;类别&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;预测结果&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_5_0.png" alt="png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 因预测样本所属类别时, 通过比较概率得到结果,   </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 我们可将结果对应的概率可视化  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 获取预测的概率值  </span>
</span></span><span style="display:flex;"><span>probability <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict_proba(x_test)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;概率:&#39;</span>, probability[:<span style="color:#ae81ff">5</span>], sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>index <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(len(x_test))  
</span></span><span style="display:flex;"><span>pro_0 <span style="color:#f92672">=</span> probability[:, <span style="color:#ae81ff">0</span>]  
</span></span><span style="display:flex;"><span>pro_1 <span style="color:#f92672">=</span> probability[:, <span style="color:#ae81ff">1</span>]  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置预测结果标签, 对和错  </span>
</span></span><span style="display:flex;"><span>tick_label <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(y_test<span style="color:#f92672">==</span>y_hat, <span style="color:#e6db74">&#39;对&#39;</span>, <span style="color:#e6db74">&#39;错&#39;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制堆叠图  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">2</span>))  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(index, height<span style="color:#f92672">=</span>pro_0, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;类别1的概率&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>bar(index, height<span style="color:#f92672">=</span>pro_1, color<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, bottom<span style="color:#f92672">=</span>pro_0,  
</span></span><span style="display:flex;"><span>        label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;类别2的概率&#39;</span>, tick_label<span style="color:#f92672">=</span>tick_label)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;预测结果&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;各类别的概率&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;分类概率&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show() 
</span></span></code></pre></div><pre><code>概率:
[[0.46933862 0.53066138]
 [0.98282882 0.01717118]
 [0.72589695 0.27410305]
 [0.91245661 0.08754339]
 [0.80288412 0.19711588]]
</code></pre>
<p><img src="output_7_1.png" alt="png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 绘制决策边界  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 决策边界: 不同类别的分界线  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> matplotlib.colors <span style="color:#f92672">import</span> ListedColormap  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义绘制函数  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_decision_boundary</span>(model, x, y):  
</span></span><span style="display:flex;"><span>    color <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;r&#39;</span>, <span style="color:#e6db74">&#39;g&#39;</span>, <span style="color:#e6db74">&#39;b&#39;</span>]  
</span></span><span style="display:flex;"><span>    marker <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#39;o&#39;</span>, <span style="color:#e6db74">&#39;v&#39;</span>, <span style="color:#e6db74">&#39;x&#39;</span>]  
</span></span><span style="display:flex;"><span>    class_label <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>unique(y)  
</span></span><span style="display:flex;"><span>    cmap <span style="color:#f92672">=</span> ListedColormap(color[:len(class_label)])  
</span></span><span style="display:flex;"><span>    x1_min, x2_min <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>min(x, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>    x1_max, x2_max <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>max(x, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>    x1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(x1_min <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>, x1_max <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0.02</span>)  
</span></span><span style="display:flex;"><span>    x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>arange(x2_min <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>, x2_max <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0.02</span>)  
</span></span><span style="display:flex;"><span>    x1, x2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>meshgrid(x1, x2)  
</span></span><span style="display:flex;"><span>    z <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(np<span style="color:#f92672">.</span>array([x1<span style="color:#f92672">.</span>ravel(), x2<span style="color:#f92672">.</span>ravel()])<span style="color:#f92672">.</span>T)<span style="color:#f92672">.</span>reshape(x1<span style="color:#f92672">.</span>shape)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>contourf(x1, x2, z, cmap<span style="color:#f92672">=</span>cmap, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> i, class_ <span style="color:#f92672">in</span> enumerate(class_label):  
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>scatter(x<span style="color:#f92672">=</span>x[y<span style="color:#f92672">==</span>class_, <span style="color:#ae81ff">0</span>], y<span style="color:#f92672">=</span>x[y<span style="color:#f92672">==</span>class_, <span style="color:#ae81ff">1</span>],  
</span></span><span style="display:flex;"><span>                c<span style="color:#f92672">=</span>cmap<span style="color:#f92672">.</span>colors[i], label<span style="color:#f92672">=</span>class_, marker<span style="color:#f92672">=</span>marker[i])  
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>show()  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制模型在训练集上的决策边界  </span>
</span></span><span style="display:flex;"><span>plot_decision_boundary(lr, x_train, y_train)  
</span></span></code></pre></div><p><img src="output_8_0.png" alt="png"></p>
<p><strong>拓展</strong> :<br>
逻辑回归实现多分类</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> load_iris()  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data, iris<span style="color:#f92672">.</span>target  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x[:, <span style="color:#ae81ff">2</span>:]  
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(x, y,   
</span></span><span style="display:flex;"><span>        test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LogisticRegression()  
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试分类  </span>
</span></span><span style="display:flex;"><span>y_hat <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict(x_test)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 可视化结果  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;axes.unicode_minus&#39;</span>]<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>  
</span></span><span style="display:flex;"><span>plot_decision_boundary(lr, x_test, y_test)  
</span></span></code></pre></div><p><img src="output_10_0.png" alt="png"></p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>分类模型评估</h1>

<p>在完成模型训练之后，需要对模型的效果进行评估，根据评估结果继续调整模型的参数, 特征或者算法，以达到满意的结果</p>
<h2 id="1-混淆矩阵">1, 混淆矩阵</h2>
<p>将 真正例(TP), 假正例(FP), 真负例(TN), 假负例(FN) 统计于一个方阵中, 观察比较, 评价模型好坏, 矩阵如下:</p>
<p><img src="hxjz.png" alt=""></p>
<p>混淆矩阵统计数量, 评价不直观也有限, 基于混淆矩阵又延伸出 正确率, 精准率, 召回率, F1(调和平均值), ROC曲线和AUC等</p>
<h2 id="2-评估指标分析">2, 评估指标分析</h2>
<p><strong>正确率:</strong></p>

<span class="math align-center">$$\text { 正确率 }=\frac{T P+T N}{T P+T N+F P+F N}$$</span><p>正确率, 表示总体(包括正负)预测正确的比率, 在模型对正例和负例的预测准确度差异较大时, 难以评价模型的好坏, 例如正例较多, 负例较少,
正例全部预测对了, 负例只预测对几个, 正确率却可能较高</p>
<p><strong>精准率:</strong></p>

<span class="math align-center">$$\text { 精准率 }=\frac{T P}{T P+F P}$$</span><p>精准率, 表示所有预测为正例的结果中 预测正确的正例 的占比, 精准率越高, 说明正例预测正确概率越高, 因此精准率更关注”一击必中”,
比如通过预测找出上涨的概率很高的一支股票</p>
<p><strong>召回率:</strong></p>

<span class="math align-center">$$\text { 召回率 }=\frac{T P}{T P+F N}$$</span><p>召回率, 表示所有真实的正例中, 预测正确的正例 的占比, 召回率越高, 说明正例被”召回”的越多, 因此召回率更关注”宁错一千, 不放一个”,
例如通过预测尽可能将新冠肺炎患者全部隔离观察</p>
<p><strong>调和平均值 F1</strong> :</p>

<span class="math align-center">$$F 1=\frac{2 * \text {精准率} * \text {召回率}}{\text {精准率}+\text {召回率}}$$</span><p>F1 将综合了精准率和召回率, F1越高, 说明模型预测效果越好, F1 能够直接评估模型的好坏</p>
<p><strong>ROC曲线:</strong></p>
<p>ROC (Receiver Operating Characteristic) 曲线, 用图像来描述分类模型的性能好坏. 图像纵轴为 真 正例率(TPR),
横轴为 假 正例率(FPR):</p>

<span class="math align-center">$$\begin{array}{l}  
T P R=\text { 召回率 }=\frac{T P}{T P+F N} \  
F P R=\frac{F P}{F P+T N}  
\end{array}$$</span><p>上述两式通过取分类模型的不同阈值, 从而计算出不同的值, 绘制出曲线, 曲线必过 (0,0) 和 (1, 1) 两个点, TPR 增长得越快,
曲线越往上凸, 模型的分类性能就越好. 如果 ROC 曲线为对角线, 可将模型理解为随机猜测; 如果 ROC 曲线在 0 点 真 正例率就达到了 1,
此时模型最完美</p>
<p><strong>AUC:</strong></p>
<p>AUC (Area Under the Curve), 是 ROC 曲线下面的面积, 因为有时通过 ROC 曲线看不出哪个分类模型性能好, 而 AUC
比较数值就不存在这样的问题</p>
<p>以鸢尾花数据集做如下练习:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_iris  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression   
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> confusion_matrix  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> warnings  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;font.family&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;SimHei&#34;</span>  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;axes.unicode_minus&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>   
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;font.size&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>   
</span></span><span style="display:flex;"><span>warnings<span style="color:#f92672">.</span>filterwarnings(<span style="color:#e6db74">&#34;ignore&#34;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> load_iris()  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data, iris<span style="color:#f92672">.</span>target  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x[y<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>:]  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> y[y<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>]  
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(x, y,  
</span></span><span style="display:flex;"><span>                        test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LogisticRegression()  
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>y_hat <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict(x_test)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 传入真实值与预测值, 创建混淆矩阵  </span>
</span></span><span style="display:flex;"><span>matrix <span style="color:#f92672">=</span> confusion_matrix(y_true<span style="color:#f92672">=</span>y_test, y_pred<span style="color:#f92672">=</span>y_hat)  
</span></span><span style="display:flex;"><span>print(matrix)  
</span></span><span style="display:flex;"><span>y_hat[y_hat<span style="color:#f92672">==</span><span style="color:#ae81ff">1</span>]<span style="color:#f92672">.</span>sum()  
</span></span></code></pre></div><pre><code>[[15  1]
 [ 1  8]]





16
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 将混淆矩阵可视化  </span>
</span></span><span style="display:flex;"><span>mat <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>matshow(matrix, cmap<span style="color:#f92672">=</span>plt<span style="color:#f92672">.</span>cm<span style="color:#f92672">.</span>Blues, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.5</span>)  
</span></span><span style="display:flex;"><span>label <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;负例&#34;</span>, <span style="color:#e6db74">&#34;正例&#34;</span>]  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 获取当前的绘图对象  </span>
</span></span><span style="display:flex;"><span>ax <span style="color:#f92672">=</span> plt<span style="color:#f92672">.</span>gca()  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置属性, 设类别 1 为负例  </span>
</span></span><span style="display:flex;"><span>ax<span style="color:#f92672">.</span>set(  
</span></span><span style="display:flex;"><span>    xticks<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>arange(matrix<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]),   
</span></span><span style="display:flex;"><span>    yticks<span style="color:#f92672">=</span>np<span style="color:#f92672">.</span>arange(matrix<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]),  
</span></span><span style="display:flex;"><span>    xticklabels<span style="color:#f92672">=</span>label,   
</span></span><span style="display:flex;"><span>    yticklabels<span style="color:#f92672">=</span>label,   
</span></span><span style="display:flex;"><span>    title<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;混淆矩阵可视化</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#34;</span>,   
</span></span><span style="display:flex;"><span>    ylabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;真实值&#34;</span>,   
</span></span><span style="display:flex;"><span>    xlabel<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;预测值&#34;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置统计值的位置  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(matrix<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>]):  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> j <span style="color:#f92672">in</span> range(matrix<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">1</span>]):  
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>text(x<span style="color:#f92672">=</span>j, y<span style="color:#f92672">=</span>i, s<span style="color:#f92672">=</span>matrix[i, j], va<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center&#34;</span>, ha<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;center&#34;</span>)   
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_2_0.png" alt="png"></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 计算各个评估指标  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> accuracy_score, precision_score, recall_score, f1_score  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;正确率：&#34;</span>, accuracy_score(y_test, y_hat))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 默认以 1 为正例, 我们将 2 设为正例  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;精准率：&#34;</span>, precision_score(y_test, y_hat, pos_label<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;召回率：&#34;</span>, recall_score(y_test, y_hat, pos_label<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;F1：&#34;</span>, f1_score(y_test, y_hat, pos_label<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 也可以用逻辑回归模型对象的score方法计算正确率   </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;score方法计算正确率：&#34;</span>, lr<span style="color:#f92672">.</span>score(x_test, y_test))  
</span></span></code></pre></div><pre><code>正确率： 0.92
精准率： 0.8888888888888888
召回率： 0.8888888888888888
F1： 0.8888888888888888
score方法计算正确率： 0.92
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 还可以用 classification_report 方法直接计算各个指标  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> classification_report  
</span></span><span style="display:flex;"><span>print(classification_report(y_true<span style="color:#f92672">=</span>y_test, y_pred<span style="color:#f92672">=</span>y_hat))  
</span></span></code></pre></div><pre><code>              precision    recall  f1-score   support

           1       0.94      0.94      0.94        16
           2       0.89      0.89      0.89         9

    accuracy                           0.92        25
   macro avg       0.91      0.91      0.91        25
weighted avg       0.92      0.92      0.92        25
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 绘制 ROC曲线 和计算 AUC  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> roc_curve, auc, roc_auc_score  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> load_iris()  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data, iris<span style="color:#f92672">.</span>target  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x[y<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>:]  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> y[y<span style="color:#f92672">!=</span><span style="color:#ae81ff">0</span>]  
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(x, y,  
</span></span><span style="display:flex;"><span>                            test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置模型参数(有默认值可以不设), 并进行训练  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 不同的参数训练结果不一样, 需要注意参数之间关系  </span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LogisticRegression(multi_class<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ovr&#34;</span>, solver<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;liblinear&#34;</span>)  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># lr = LogisticRegression(multi_class=&#34;multinomial&#34;)  </span>
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 获取样本的概率  </span>
</span></span><span style="display:flex;"><span>probo <span style="color:#f92672">=</span> lr<span style="color:#f92672">.</span>predict_proba(x_test)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;类别 2 的概率:&#39;</span>, probo[:, <span style="color:#ae81ff">1</span>][:<span style="color:#ae81ff">5</span>])  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 将概率值传入 roc_curve 方法, 从概率中选择若干个值作为阈值  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 同时根据阈值判定正负例, 返回 fpr, tpr 和 阈值 thresholds  </span>
</span></span><span style="display:flex;"><span>fpr, tpr, thresholds <span style="color:#f92672">=</span> roc_curve(y_true<span style="color:#f92672">=</span>y_test,  
</span></span><span style="display:flex;"><span>                       y_score<span style="color:#f92672">=</span>probo[:, <span style="color:#ae81ff">1</span>], pos_label<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 阈值中的第一个值是第二个值 +1 得到, 为了让让曲线过 0 点  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;阈值:&#39;</span>, thresholds)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 计算 AUC   </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;用auc计算:&#39;</span>, auc(fpr, tpr))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;用roc_auc_score计算:&#39;</span>, roc_auc_score(y_true<span style="color:#f92672">=</span>y_test,  
</span></span><span style="display:flex;"><span>                                    y_score<span style="color:#f92672">=</span>probo[:, <span style="color:#ae81ff">1</span>]))  
</span></span></code></pre></div><pre><code>类别 2 的概率: [0.4663913  0.28570842 0.60050037 0.3758227  0.48450719]
阈值: [1.69092453 0.69092453 0.60050037 0.54308778 0.50384451 0.49358343
 0.48450719 0.47242245 0.4663913  0.42043757 0.39590375 0.39413886
 0.3843811  0.24698327]
用auc计算: 0.8819444444444444
用roc_auc_score计算: 0.8819444444444444
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 绘制 ROC 曲线  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">2</span>))  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(fpr, tpr, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;o&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;ROC曲线&#34;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>,<span style="color:#ae81ff">1</span>], lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;--&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;随机猜测&#34;</span>)   
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, ls<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;-.&#34;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;完美预测&#34;</span>)   
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlim(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.01</span>, <span style="color:#ae81ff">1.02</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylim(<span style="color:#f92672">-</span><span style="color:#ae81ff">0.01</span>, <span style="color:#ae81ff">1.02</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.1</span>, <span style="color:#ae81ff">0.2</span>))  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>yticks(np<span style="color:#f92672">.</span>arange(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1.1</span>, <span style="color:#ae81ff">0.2</span>))  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#34;FPR&#34;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#34;TPR&#34;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>grid()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#34;ROC曲线, AUC值为:</span><span style="color:#e6db74">{</span>auc(fpr, tpr)<span style="color:#e6db74">:</span><span style="color:#e6db74">.2f</span><span style="color:#e6db74">}</span><span style="color:#e6db74">&#34;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_6_0.png" alt="png"></p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>KNN 算法</h1>

<h2 id="1-关于-knn">1, 关于 KNN</h2>
<p>KNN (K-Nearest Neighbor), 即 K 近邻算法, K 个最近的邻居. 当需要预测一个未知样本的时候, 就由与该样本最近的 K
个邻居来决定</p>
<p>KNN 既可以用于分类, 也可用于回归. 用来分类时, 使用 K 个邻居中, 类别数量最多(或加权最多)者, 作为预测结果; 当用来回归分析时, 使用 K
个邻居的均值(或加权均值), 作为预测结果</p>
<p>KNN 算法的原理是: 样本映射到多维空间时, 相似度较高的样本, 距离也会较接近, “近朱者赤近墨者黑”</p>
<h2 id="2-k-值">2, K 值</h2>
<p>KNN 算法的 K 值是一个模型训练前就要人为指定的参数 <strong>超参数</strong> , 不同于模型内部通过训练数据计算得到的参数. KNN 的超参数, 需要通常通过
<strong>交叉验证</strong> 的方式来选择最合适的参数组合</p>
<p>K 值的选择非常重要, K 值较小时, 模型预测依赖附近的邻居, 敏感性高, 稳定性低, 容易导致过拟合; 反之, K 值较大, 敏感性低, 稳定性高,
容易欠拟合</p>
<p>K 值在数据量小时, 可以通过遍历所有样本(穷举)的方式找出最近的 K 个邻居, 当数据量庞大时, 穷举耗费大量时间, 此时可以采用 <strong>KD树</strong> 来找
K 个邻居</p>
<h2 id="3-交叉验证">3, 交叉验证</h2>
<p>KNN 的网格搜索交叉验证: 取不同的 K, 选择不同的距离或权重计算方式等, 将数据分为多个组, 一个组作为测试集, 其他部分作为训练集,
不断循环训练和测试, 对模型进行循环验证, 找出最佳参数组合</p>
<h2 id="4-距离的度量方式">4, 距离的度量方式</h2>
<p><strong>闵可夫斯基距离:</strong></p>
<p>设 n 维空间中两个点位 X 和 Y:</p>

<span class="math align-center">$X=\left(x_{1}, x_{2}, \ldots \ldots, x_{n}\right)$</span>

<span class="math align-center">$Y=\left(y_{1}, y_{2}, \ldots \ldots, y_{n}\right)$</span>
<p>则阁可夫斯基距离为:</p>

<span class="math align-center">$D(X, Y)=\left(\sum_{i=1}^{n}\left|x_{i}-y_{i}\right|^{p}\right)^{1 / p}$</span>
<p>当 p 为 1 时, 又称 <strong>曼哈顿距离</strong> ; 当 p 为 2 时, 称 <strong>欧几里得距离</strong></p>
<h2 id="5-权重">5, 权重</h2>
<p><strong>统一权重:</strong> K 个邻居权重相同, 不管近远都是 1/K</p>
<p><strong>距离加权权重:</strong> K 个邻居的权重, 与他们各自和待测样本的距离成反比, 同时要保证权重之和为 1. 例如 3 个邻居 a, b, c
距离待测样本的距离分别为 a, b 和 c, 则 a 的权重为:</p>

<span class="math align-center">$$\frac{\frac{1}{a}}{\frac{1}{a}+\frac{1}{b}+\frac{1}{c}}=\frac{b c}{b c+a c+a
b}$$</span><p>b 和 c 同理</p>
<h2 id="6-数据标准化">6, 数据标准化</h2>
<p>样本中的特征通常非常多，由于各特征的性质不同，通常具有不同的量纲(数量级).
当各特征间的量纲相差很大时，如果直接用原始特征值进行分析，就会突出数值较高的特征在综合分析中的作用，相对削弱数值较低特征的作用, 因此需要通过数据标准化,
将量纲统一, 才能客观地描述各个特征对模型的影响程度</p>
<p>线性回归和逻辑回归, 都是通过每个特征与其权重的乘积相加来进行计算, 不进行数据标准化(不考虑正则化), 对每个特征的权重影响较大, 但对结果不会造成影响,
而 KNN 是基于距离计算的, 如果特征的量纲不同, 量纲较大的特征会占据主导地位, 导致忽略量纲较小的特征, 从而对模型性能造成较大影响</p>
<h2 id="7-算法实现步骤">7, 算法实现步骤</h2>
<p>a, 确定超参数<br>
确定 K<br>
确定距离度量方式<br>
确定权重计算方式<br>
其他超参数</p>
<p>b, 从训练集中选择距离待测样本最近的 K 个样本</p>
<p>c, 根据 K 个样本对待测样本进行预测, 如果遇到多个样本距离相同的情况, 默认选取训练集中靠前的</p>
<h2 id="8-流水线-pipline">8, 流水线 Pipline</h2>
<p>流水线可以将每个评估器视为一个步骤, 然后将多个步骤作为整体依次执行. 例如数据处理工作较多时, 可能涉及更多步骤, 例如多项式扩展, One-Hot
编码, 特征选择, 数据标准化, 交叉验证等, 分别执行过于繁琐, 我们可以将数据处理与模型训练各个步骤作为一个整体来执行</p>
<p>流水线具有最后一个评估器的所有方法:</p>
<p>a, 当流水线对象调用 fit 方法时, 会从第一个评估器依次调用 fit_transform 方法, 然后到最后一个评估器调用 fit 方法</p>
<p>b, 当流水线对象调用 <em>其他</em> 方法时, 会从第一个评估器依次调用 transform 方法, 然后到最后一个评估器调用 <em>其他</em> 方法</p>
<h2 id="9-以鸢尾花为例-对逻辑回归和-knn-进行比较">9, 以鸢尾花为例, 对逻辑回归和 KNN 进行比较:</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neighbors <span style="color:#f92672">import</span> KNeighborsClassifier  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> classification_report  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_iris  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.preprocessing <span style="color:#f92672">import</span> StandardScaler  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LogisticRegression  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib <span style="color:#66d9ef">as</span> mpl  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> warnings  
</span></span><span style="display:flex;"><span>warnings<span style="color:#f92672">.</span>filterwarnings(<span style="color:#e6db74">&#34;ignore&#34;</span>)  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mpl<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;font.family&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;SimHei&#34;</span>  
</span></span><span style="display:flex;"><span>mpl<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#34;axes.unicode_minus&#34;</span>] <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>iris <span style="color:#f92672">=</span> load_iris()  
</span></span><span style="display:flex;"><span>X <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>data[:, :<span style="color:#ae81ff">2</span>]  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> iris<span style="color:#f92672">.</span>target  
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y,   
</span></span><span style="display:flex;"><span>                        test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 数据标准化: StandardScaler 均值标准差标准化, MinMaxScaler 最大最小值标准化  </span>
</span></span><span style="display:flex;"><span>ss <span style="color:#f92672">=</span> StandardScaler()  
</span></span><span style="display:flex;"><span>X_train <span style="color:#f92672">=</span> ss<span style="color:#f92672">.</span>fit_transform(X_train)  
</span></span><span style="display:flex;"><span>X_test <span style="color:#f92672">=</span> ss<span style="color:#f92672">.</span>transform(X_test)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 逻辑回归训练  </span>
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LogisticRegression()  
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(X_train,y_train)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># KNN 训练  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># n_neighbors: 邻居的数量  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># weights：权重计算方式, 可选值为 uniform 统一权重, 与 distance 加权权重  </span>
</span></span><span style="display:flex;"><span>knn <span style="color:#f92672">=</span> KNeighborsClassifier(n_neighbors<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;uniform&#34;</span>)  
</span></span><span style="display:flex;"><span>knn<span style="color:#f92672">.</span>fit(X_train, y_train)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 比较 AUC  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics <span style="color:#f92672">import</span> roc_curve, auc,roc_auc_score  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>lr_fpr, lr_tpr, lr_thresholds <span style="color:#f92672">=</span> roc_curve(y_test,  
</span></span><span style="display:flex;"><span>                lr<span style="color:#f92672">.</span>predict_proba(X_test)[:,<span style="color:#ae81ff">1</span>], pos_label<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>lr_auc <span style="color:#f92672">=</span> auc(lr_fpr, lr_tpr)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Logistic 算法: AUC = </span><span style="color:#e6db74">%.3f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> lr_auc)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>knn_fpr, knn_tpr, knn_thresholds <span style="color:#f92672">=</span> roc_curve(y_test,  
</span></span><span style="display:flex;"><span>                knn<span style="color:#f92672">.</span>predict_proba(X_test)[:,<span style="color:#ae81ff">1</span>], pos_label<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>knn_auc <span style="color:#f92672">=</span> auc(knn_fpr, knn_tpr)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;KNN 算法: AUC = </span><span style="color:#e6db74">%.3f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> knn_auc) 
</span></span></code></pre></div><pre><code>Logistic 算法: AUC = 0.835
KNN 算法: AUC = 0.794
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 将 KNN 算法参数进行调优再来比较  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> GridSearchCV  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># K 值取 1~10, 并定义需要的参数组合  </span>
</span></span><span style="display:flex;"><span>knn <span style="color:#f92672">=</span> KNeighborsClassifier()  
</span></span><span style="display:flex;"><span>grid <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;n_neighbors&#39;</span>: range(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">11</span>,<span style="color:#ae81ff">1</span>), <span style="color:#e6db74">&#39;weights&#39;</span>: [<span style="color:#e6db74">&#39;uniform&#39;</span>,<span style="color:#e6db74">&#39;distance&#39;</span>]}  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 网格搜索交叉验证  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># param_grid：需要检验的超参数组合  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># scoring：模型评估标准, accuracy 正确率  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># n_jobs：并发数量  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># cv：交叉验证折数  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># verbose：输出冗余信息  </span>
</span></span><span style="display:flex;"><span>gs <span style="color:#f92672">=</span> GridSearchCV(estimator<span style="color:#f92672">=</span>knn, param_grid<span style="color:#f92672">=</span>grid, scoring<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;accuracy&#39;</span>,  
</span></span><span style="display:flex;"><span>                  n_jobs<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, cv<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>gs<span style="color:#f92672">.</span>fit(X_train, y_train)  
</span></span><span style="display:flex;"><span>gs_fpr, gs_tpr, gs_thresholds <span style="color:#f92672">=</span> roc_curve(y_test,  
</span></span><span style="display:flex;"><span>                gs<span style="color:#f92672">.</span>predict_proba(X_test)[:,<span style="color:#ae81ff">1</span>], pos_label<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>gs_auc <span style="color:#f92672">=</span> auc(gs_fpr, gs_tpr)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;KNN 算法: AUC = </span><span style="color:#e6db74">%.3f</span><span style="color:#e6db74">&#39;</span> <span style="color:#f92672">%</span> gs_auc) 
</span></span></code></pre></div><pre><code>KNN 算法: AUC = 0.855
</code></pre>
<h2 id="10-以波士顿房价为例-对线性回归和-knn-进行比较">10, 以波士顿房价为例, 对线性回归和 KNN 进行比较:</h2>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_boston  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.neighbors <span style="color:#f92672">import</span> KNeighborsRegressor   
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.linear_model <span style="color:#f92672">import</span> LinearRegression  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X, y <span style="color:#f92672">=</span> load_boston(return_X_y<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)  
</span></span><span style="display:flex;"><span>X_train, X_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(X, y,  
</span></span><span style="display:flex;"><span>                        test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>knn <span style="color:#f92672">=</span> KNeighborsRegressor(n_neighbors<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;distance&#34;</span>)   
</span></span><span style="display:flex;"><span>knn<span style="color:#f92672">.</span>fit(X_train, y_train)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;KNN 算法 R²：&#34;</span>, knn<span style="color:#f92672">.</span>score(X_test, y_test))  
</span></span><span style="display:flex;"><span>lr <span style="color:#f92672">=</span> LinearRegression()  
</span></span><span style="display:flex;"><span>lr<span style="color:#f92672">.</span>fit(X_train, y_train)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;线性回归算法 R²：&#34;</span>, lr<span style="color:#f92672">.</span>score(X_test, y_test))  
</span></span></code></pre></div><pre><code>KNN 算法 R²： 0.5158073940789912
线性回归算法 R²： 0.6354638433202129
</code></pre>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 对 KNN 数据标准化和参数调优之后再来比较  </span>
</span></span><span style="display:flex;"><span>knn <span style="color:#f92672">=</span> KNeighborsRegressor()  
</span></span><span style="display:flex;"><span>grid <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;n_neighbors&#39;</span>: range(<span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">11</span>,<span style="color:#ae81ff">1</span>), <span style="color:#e6db74">&#39;weights&#39;</span>: [<span style="color:#e6db74">&#39;uniform&#39;</span>,<span style="color:#e6db74">&#39;distance&#39;</span>]}  
</span></span><span style="display:flex;"><span>gs <span style="color:#f92672">=</span> GridSearchCV(estimator<span style="color:#f92672">=</span>knn, param_grid<span style="color:#f92672">=</span>grid, scoring<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r2&#39;</span>,  
</span></span><span style="display:flex;"><span>                  n_jobs<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>, cv<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, verbose<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)   
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 利用流水线处理  </span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.pipeline <span style="color:#f92672">import</span> Pipeline  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义流水线的步骤: 类型为一个列表, 列表中的每个元素是元组类型  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 格式为：[(步骤名1，评估器1), (步骤名2， 评估器2), ……, (步骤名n, 评估器n)  </span>
</span></span><span style="display:flex;"><span>knn_steps <span style="color:#f92672">=</span> [(<span style="color:#e6db74">&#34;scaler&#34;</span>, StandardScaler()), (<span style="color:#e6db74">&#34;knn&#34;</span>, gs)]  
</span></span><span style="display:flex;"><span>knn_p <span style="color:#f92672">=</span> Pipeline(knn_steps)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 可以设置流水线的参数. 所有可用的参数可以通过 get_params 获取  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置格式如下: (步骤名__参数)  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># p.set_params(knn__n_neighbors=3, knn__weights=&#34;uniform&#34;)  </span>
</span></span><span style="display:flex;"><span>knn_p<span style="color:#f92672">.</span>fit(X_train, y_train)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;KNN 算法 R²：&#34;</span>, knn_p<span style="color:#f92672">.</span>score(X_test, y_test))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 线性回归数据标准化  </span>
</span></span><span style="display:flex;"><span>lr_steps <span style="color:#f92672">=</span> [(<span style="color:#e6db74">&#34;scaler&#34;</span>, StandardScaler()), (<span style="color:#e6db74">&#34;lr&#34;</span>, LinearRegression())]  
</span></span><span style="display:flex;"><span>lr_p <span style="color:#f92672">=</span> Pipeline(lr_steps)  
</span></span><span style="display:flex;"><span>lr_p<span style="color:#f92672">.</span>fit(X_train, y_train)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#34;线性回归算法 R²：&#34;</span>, lr_p<span style="color:#f92672">.</span>score(X_test, y_test))  
</span></span></code></pre></div><pre><code>KNN 算法 R²： 0.6441485149216897
线性回归算法 R²： 0.6354638433202131
</code></pre>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>朴素贝叶斯</h1>

<h2 id="1-概率基础">1, 概率基础</h2>
<p><strong>样本空间</strong> :</p>
<p>在 <strong>随机试验</strong> E 中, 实验的所有可能结果组成的集合, 称为 <strong>样本空间</strong> S, 样本空间的每个元素, 即 E 的每个结果, 称 <strong>样本点</strong></p>
<p><strong>随机事件</strong> :</p>
<p>进行随机试验时, 满足某种条件的样本点组成的集合, S 的子集, 称作 <strong>随机事件</strong> , 只有一个样本点时, 称作 <strong>基本事件</strong></p>
<p><strong>概率</strong> :</p>
<p>对于随机事件 A, 概率为:</p>

<span class="math align-center">$P(A)=\frac{A \text { 中基本事件数 }}{S \text { 中基本事件数 }}$</span>
<p><strong>条件概率</strong> :</p>
<p>定义事件 A 发生的前提下, 事件 B 发生的概率 P(B | A) 为条件概率:</p>

<span class="math align-center">$$P(B \mid A)=\frac{P(A B)}{P(A)}$$</span><p>由条件概率的定义可得, 事件 A 和 B 同时发生的概率 P(AB) 满足如下 <strong>乘法定理</strong> :</p>

<span class="math align-center">$$P(A B)=P(B \mid A) P(A)$$</span><p><strong>独立性:</strong></p>
<p>定义 A 和 B 两个事件, 如果满足:</p>

<span class="math align-center">$$P(A B)=P(A) P(B)$$</span><p>则称事件 A, B 相互独立. 再结合乘法定理, 则有:</p>

<span class="math align-center">$$P(B \mid A) = P(B)$$</span><p><strong>全概率公式:</strong></p>
<p>设随机试验 E 的样本空间为 S, 若事件 
<span class="math align-center">$B_{1}$</span>，
<span class="math align-center">$B_{2}$</span>，…, 
<span class="math align-center">$B_{n}$</span> 构成一个完备事件组(即它们两两相互独立，事件并集为 S),
且都有正概率，则对任意一个 E 的事件 A，有如下公式成立：</p>

<span class="math align-center">$$P(A)=P\left(A \mid B_{1}\right) P\left(B_{1}\right)+P\left(A \mid
B_{2}\right) P\left(B_{2}\right)+\ldots \ldots+P\left(A \mid B_{n}\right)
P\left(B_{n}\right)$$</span><p>此公式即为全概率公式. 特别地，对于任意两随机事件 A 和 B，有如下成立：</p>

<span class="math align-center">$$P(B)=P(B \mid A) P(A)+P(B \mid \bar{A}) P(\bar{A})$$</span><p><strong>贝叶斯公式:</strong></p>
<p>设随机试验 E 的样本空间为 S, 若事件 
<span class="math align-center">$B_{1}$</span>，
<span class="math align-center">$B_{2}$</span>，…, 
<span class="math align-center">$B_{n}$</span> 构成一个完备事件组(即它们两两相互独立，事件并集为 S),
且都有正概率，则对任意一个 E 的正概率事件 A，有如下公式成立( i 为 1~n 的正整数)：</p>

<span class="math align-center">$$P\left(B_{i} \mid A\right)=\frac{P\left(A B_{i}\right)}{P(A)}=\frac{P\left(A
\mid B_{i}\right) P\left(B_{i}\right)}{P(A)} \  
=\frac{P\left(A \mid B_{i}\right) P\left(B_{i}\right)}{\sum_{j=1}^{n} P\left(A
\mid B_{j}\right) P\left(B_{j}\right)}$$</span><p>贝叶斯公式将求解 P(B | A) 的概率转换成求 P(A | B) 的概率, 在求解某个事件概率非常困难时, 转换一下更方便求解</p>
<p>例: 从以往数据得出, 机器调整良好时生产的产品合格的概率是 98%, 机器故障时合格的概率是 55%, 每天开机时机器调整良好的概率为 95%.
求某日开机生产的第一件产品是合格品时, 机器调整良好的概率?</p>
<p>解: 设事件 A 为产品合格, B 为机器调整良好, 则 
<span class="math align-center">$\bar{B}$</span> 为机器故障</p>

<span class="math align-center">$$P(B \mid A)=\frac{P(A \mid B) P(B)}{P(A \mid B) P(B)+P(A \mid \bar{B})
P(\bar{B})} \  
=\frac{0.98 \times 0.95}{0.98 \times 0.95+0.55 \times 0.05} \  
=0.97$$</span><p><strong>先验概率和后验概率:</strong></p>
<p>由以往的数据得出的概率称为 <strong>先验概率</strong> , 如上例中的已知概率</p>
<p>得到某些信息后, 在先验概率的基础上进行修正而得到的概率, 称为 <strong>后验概率</strong> , 如上例中求解的概率</p>
<h2 id="2-朴素贝叶斯算法原理">2, 朴素贝叶斯算法原理</h2>
<p>朴素贝叶斯是基于概率的分类算法, 前提假设各个特征(自变量)之间是相互独立的, 设类别(因变量)为 Y, Y 包含 m 个类别(
<span class="math align-center">$y_{1},\ldots, y_{m}$</span>), 特征为 X, X 包含含有 n 个特征 (
<span class="math align-center">$x_{1}, \ldots, x_{n}$</span>), 然后通过计算比较, 在特征 X
确定的前提下, 类别 Y 中每个类别的概率大小, 概率最大者即为预测结果</p>
<p>设 Y 中任意一个类别为 y, 则:</p>

<span class="math align-center">$$P(y \mid X) = P\left(y \mid x_{1}, \ldots, x_{n}\right) \  
=\frac{P(y) P\left(x_{1}, \ldots, x_{n} \mid y\right)}{P\left(x_{1}, \ldots,
x_{n}\right)} \  
=\frac{P(y) P\left(x_{1} \mid y\right) P\left(x_{2} \mid y\right) \ldots
P\left(x_{n} \mid y\right)}{P\left(x_{1}, \ldots, x_{n}\right)} \  
=\frac{P(y) \prod_{i=1}^{n} P\left(x_{i} \mid y\right)}{P\left(x_{1}, \ldots,
x_{n}\right)}$$</span><p>上式分母为定值, 则:</p>

<span class="math align-center">$$P\left(y \mid X \right) \propto P(y) \prod_{i=1}^{n} P\left(x_{i} \mid
y\right)$$</span><p>所以最终预测类别 
<span class="math align-center">$\hat{y}$</span> 为分子部分值最大对应的类别:</p>

<span class="math align-center">$$\hat{y}=\arg \max_{y} P(y) \prod_{i=1}^{n} P\left(x_{i} \mid y\right)$$</span><p>不同的朴素贝叶斯算法, 主要是对 
<span class="math align-center">$P\left(x_{i} \mid y\right)$</span> 的分布假设不同, 进而采取不同的参数估计方式.
最终主要就是通过计算 
<span class="math align-center">$P\left(x_{i} \mid y\right)$</span> 的概率来计算结果</p>
<p>例: 预测第 11 条记录, 学生是否上课</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>天气</th>
<th>上课距离</th>
<th>成绩</th>
<th>课程</th>
<th>上课情况</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>晴</td>
<td>远</td>
<td>差</td>
<td>选修</td>
<td>逃课</td>
</tr>
<tr>
<td>2</td>
<td>晴</td>
<td>近</td>
<td>差</td>
<td>必修</td>
<td>上课</td>
</tr>
<tr>
<td>3</td>
<td>晴</td>
<td>近</td>
<td>好</td>
<td>必修</td>
<td>上课</td>
</tr>
<tr>
<td>4</td>
<td>阴</td>
<td>远</td>
<td>差</td>
<td>选修</td>
<td>逃课</td>
</tr>
<tr>
<td>5</td>
<td>阴</td>
<td>近</td>
<td>好</td>
<td>选修</td>
<td>上课</td>
</tr>
<tr>
<td>6</td>
<td>阴</td>
<td>近</td>
<td>好</td>
<td>必修</td>
<td>上课</td>
</tr>
<tr>
<td>7</td>
<td>雨</td>
<td>远</td>
<td>差</td>
<td>选修</td>
<td>逃课</td>
</tr>
<tr>
<td>8</td>
<td>雨</td>
<td>近</td>
<td>好</td>
<td>必修</td>
<td>上课</td>
</tr>
<tr>
<td>9</td>
<td>雨</td>
<td>近</td>
<td>差</td>
<td>必修</td>
<td>逃课</td>
</tr>
<tr>
<td>10</td>
<td>雨</td>
<td>远</td>
<td>好</td>
<td>选修</td>
<td>逃课</td>
</tr>
<tr>
<td>11</td>
<td>阴</td>
<td>近</td>
<td>差</td>
<td>选修</td>
<td>?</td>
</tr>
<tr>
<td>12</td>
<td>晴</td>
<td>远</td>
<td>好</td>
<td>选修</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>分别计算上课和逃课情况下, 各自的概率:</p>

<span class="math align-center">$$P(y=\text { 上课 }) \prod_{i=1}^{n} P\left(x_{i} \mid y=\text { 上课 }\right) \  
=P(y=\text { 上课 }) P\left(x_{1}=\text { 阴 } \mid y=\text { 上课 }\right)
P\left(x_{2}=\text { 近 } \mid y=\text { 上课 }\right) \  
P\left(x_{3}=\text {差 } \mid y=\text { 上课 }\right) P\left(x_{4}=\text { 选修 }
\mid y=\text { 上课 }\right) \  
=0.5 \times 0.4 \times 1 \times 0.2 \times 0.2 \  
=0.008$$</span>
<span class="math align-center">$$P(y=\text { 逃课 }) \prod_{i=1}^{n} P\left(x_{i} \mid y=\text { 逃课 }\right) \  
=P(y=\text { 逃课 }) P\left(x_{1}=\text { 阴 } \mid y=\text { 逃课 }\right)
P\left(x_{2}=\text { 近 } \mid y=\text { 逃课 }\right) \  
P\left(x_{3}=\text { 差 } \mid y=\text { 逃课 }\right) P\left(x_{4}=\text { 选修 }
\mid y=\text { 逃课 }\right) \  
=0.5 \times 0.2 \times 0.2 \times 0.8 \times 0.8 \  
=0.0128$$</span><p>可得预测结果为: 逃课</p>
<h2 id="3-平滑改进">3, 平滑改进</h2>
<p>当我们预测上例中, 第 12 条记录所属的类别时, 因为样本不是总体, 会出现上课的前提下, 距离远的概率为 0, 造成计算结果也为 0, 影响了预测结果,
因此需要平滑改进:</p>

<span class="math align-center">$$ P\left(x_{i} \mid y\right)=\frac{\text { 类别 } y \text { 中 } x_{i} \text {
取某个值出现的次数 }+ \alpha}{\text { 类别别 } y \text { 的总数 }+k * \alpha} $$</span><p>其中, k 为特征 
<span class="math align-center">$x_{i}$</span> 可能的取值数, α (α ≥ 0) 称为平滑系数, 当 α = 1 时, 称拉普拉斯平滑( Laplace
smoothing)</p>
<h2 id="4-算法优点">4, 算法优点</h2>
<p>即使训练集数据较少, 也能实现不错的预测; 算法训练速度非常快</p>
<p>因此算法假设特征之间是独立的, 可以单独考虑. 如果训练集有 N 个特征, 每个特征需要 M 个样本来训练, 则只需要训练 N*M 的样本数量,
而不是笛卡儿积的形式指数级增加</p>
<p>常用的朴素贝叶斯有: 高斯朴素贝叶斯, 伯努利朴素贝叶斯, 多项式朴素贝叶斯</p>
<h2 id="5-高斯朴素贝叶斯">5, 高斯朴素贝叶斯</h2>
<p>适用于连续变量, 其假定各个特征 x 在各个类别 y 下服从正态分布:</p>

<span class="math align-center">$$x_{i} \sim N\left(\mu_{y}, \sigma_{y}^{2}\right)$$</span><p>算法使用概率密度函数来计算 
<span class="math align-center">$P\left(x_{i} \mid y\right)$</span> 的概率:</p>

<span class="math align-center">$$P\left(x_{i} \mid y\right)=\frac{1}{\sqrt{2 \pi \sigma_{y}^{2}}} \exp
\left(-\frac{\left(x_{i}-\mu_{y}\right)^{2}}{2 \sigma_{y}^{2}}\right) $$</span><p>
<span class="math align-center">$\mu_{y}$</span>: 在类别为 y 的样本中, 特征 
<span class="math align-center">$x_{i}$</span> 的均值<br>

<span class="math align-center">$\sigma_{y}$</span>: 在类别为 y 的样本中, 特征 
<span class="math align-center">$x_{i}$</span> 的标件差</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> GaussianNB  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">10</span>, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>))  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>])  
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(np<span style="color:#f92672">.</span>concatenate([x, y<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  
</span></span><span style="display:flex;"><span>                   columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;x1&#39;</span>, <span style="color:#e6db74">&#39;x2&#39;</span>, <span style="color:#e6db74">&#39;x3&#39;</span>, <span style="color:#e6db74">&#39;y&#39;</span>])  
</span></span><span style="display:flex;"><span>display(data[:<span style="color:#ae81ff">3</span>])  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>gnb <span style="color:#f92672">=</span> GaussianNB()  
</span></span><span style="display:flex;"><span>gnb<span style="color:#f92672">.</span>fit(x, y)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;类别标签:&#39;</span>, gnb<span style="color:#f92672">.</span>classes_)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个类别的先验概率:&#39;</span>, gnb<span style="color:#f92672">.</span>class_prior_)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;样本数量:&#39;</span>, gnb<span style="color:#f92672">.</span>class_count_)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个类别下特征的均值:&#39;</span>, gnb<span style="color:#f92672">.</span>theta_)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个类别下特征的方差:&#39;</span>, gnb<span style="color:#f92672">.</span>sigma_)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试集  </span>
</span></span><span style="display:flex;"><span>x_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">7</span>, <span style="color:#ae81ff">2</span>]])  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;预测结果:&#39;</span>, gnb<span style="color:#f92672">.</span>predict(x_test))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;预测结果概率:&#39;</span>, gnb<span style="color:#f92672">.</span>predict_proba(x_test))  
</span></span></code></pre></div><pre><code>  x1	x2	x3	y
0	5	0	3	0
1	3	7	9	1
2	3	5	2	0


类别标签: [0 1]
每个类别的先验概率: [0.5 0.5]
样本数量: [4. 4.]
每个类别下特征的均值: [[5.   5.   3.  ]
 [6.5  5.75 7.5 ]]
每个类别下特征的方差: [[3.50000001 9.50000001 3.50000001]
 [5.25000001 7.68750001 2.75000001]]
预测结果: [0]
预测结果概率: [[0.99567424 0.00432576]]
</code></pre>
<h2 id="6-伯努利朴素贝叶斯">6, 伯努利朴素贝叶斯</h2>
<p>设实验 E 只有两个可能的结果, A 与 
<span class="math align-center">$\bar{A}$</span>, 则称 E 为伯努利试验</p>
<p>伯努利朴素贝叶斯, 适用于离散变量, 其假设各个特征 x 在各个类别 y 下服从 n 重伯努利分布(二项分布), 因伯努利试验仅有两个结果,
算法会首先对特征值进行二值化处理(假设二值化结果为 1 和 0 )</p>
<p>
<span class="math align-center">$P\left(x_{i} \mid y\right)$</span> 的概率为:</p>

<span class="math align-center">$$P\left(x_{i} \mid y\right)=P\left(x_{i}=1 \mid y\right)
x_{i}+\left(1-P\left(x_{i}=1 \mid y\right)\right)\left(1-x_{i}\right)$$</span><p>在训练集中, 会进行如下评估:</p>

<span class="math align-center">$$ P\left(x_{i}=1 \mid y\right)=\frac{N_{y i}+\alpha}{N_{y}+2 * \alpha} \  
P\left(x_{i}=0 \mid y\right)=1-P\left(x_{i}=1 \mid y\right) $$</span><p>
<span class="math align-center">$N_{y i}$</span>: 第 i 特征中, 属于类别 y, 数值为 1 的样本个数<br>

<span class="math align-center">$N_{y}$</span>: 属于类別 y 的所有样本个数<br>

<span class="math align-center">$\alpha$</span>: 平滑系数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> BernoulliNB  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">5</span>, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>))  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>])  
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(np<span style="color:#f92672">.</span>concatenate([x, y<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  
</span></span><span style="display:flex;"><span>                   columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;x1&#39;</span>, <span style="color:#e6db74">&#39;x2&#39;</span>, <span style="color:#e6db74">&#39;x3&#39;</span>, <span style="color:#e6db74">&#39;y&#39;</span>])  
</span></span><span style="display:flex;"><span>display(data[:<span style="color:#ae81ff">3</span>])  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>bnb <span style="color:#f92672">=</span> BernoulliNB()  
</span></span><span style="display:flex;"><span>bnb<span style="color:#f92672">.</span>fit(x, y)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 统计每个类别下, 特征中二值化后, 每个特征下值 1 出现的次数  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;值 1 出现的次数:&#39;</span>, bnb<span style="color:#f92672">.</span>feature_count_)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 每个类别的先验概率, 算法得到的该概率值是取对数后的结果,  </span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 需要取指数还原  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个类别的先验概率:&#39;</span>, np<span style="color:#f92672">.</span>exp(bnb<span style="color:#f92672">.</span>class_log_prior_))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 每个类别下, 每个特征的概率(也需要取指数还原)  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个特征的概率:&#39;</span>, np<span style="color:#f92672">.</span>exp(bnb<span style="color:#f92672">.</span>feature_log_prob_))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试集  </span>
</span></span><span style="display:flex;"><span>x_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#f92672">-</span><span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span>]])  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;预测结果:&#39;</span>, bnb<span style="color:#f92672">.</span>predict(x_test))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;预测结果概率:&#39;</span>, bnb<span style="color:#f92672">.</span>predict_proba(x_test))  
</span></span></code></pre></div><pre><code>  x1	x2	x3	y
0	0	-5	-2	0
1	-2	2	4	1
2	-2	0	-3	0


值 1 出现的次数: [[1. 2. 1.]
 [3. 3. 3.]]
每个类别的先验概率: [0.5 0.5]
每个特征的概率: [[0.33333333 0.5        0.33333333]
 [0.66666667 0.66666667 0.66666667]]
预测结果: [0]
预测结果概率: [[0.6 0.4]]
</code></pre>
<h2 id="7-多项式朴素贝叶斯">7, 多项式朴素贝叶斯</h2>
<p>多项式朴素贝叶斯, 适用于离散变量, 其假设各个特征 x 在各个类别 y 下服从多项式分布(每个特征下的值之和, 就是该特征发生(出现)的次数),
因此每个特征值不能是负数</p>
<p>
<span class="math align-center">$P\left(x_{i} \mid y\right)$</span> 的概率为:</p>

<span class="math align-center">$$P\left(x_{i} \mid y\right)=\frac{N_{y i}+\alpha}{N_{y}+\alpha n} $$</span><p>
<span class="math align-center">$N_{y i}$</span>: 特征 i 在类别 y 的样本中发生(出现)的次数<br>

<span class="math align-center">$N_{y}$</span>: 类别 y 的样本中, 所有特征发生(出现)的次数<br>

<span class="math align-center">$n$</span>: 特征数量<br>

<span class="math align-center">$\alpha$</span>: 平滑系数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.naive_bayes <span style="color:#f92672">import</span> MultinomialNB  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>seed(<span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>random<span style="color:#f92672">.</span>randint(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">5</span>, size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">3</span>))  
</span></span><span style="display:flex;"><span>y <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>])  
</span></span><span style="display:flex;"><span>data <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>DataFrame(np<span style="color:#f92672">.</span>concatenate([x, y<span style="color:#f92672">.</span>reshape(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>)], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),  
</span></span><span style="display:flex;"><span>                   columns<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;x1&#39;</span>, <span style="color:#e6db74">&#39;x2&#39;</span>, <span style="color:#e6db74">&#39;x3&#39;</span>, <span style="color:#e6db74">&#39;y&#39;</span>])  
</span></span><span style="display:flex;"><span>display(data[:<span style="color:#ae81ff">3</span>])  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mnb <span style="color:#f92672">=</span> MultinomialNB()  
</span></span><span style="display:flex;"><span>mnb<span style="color:#f92672">.</span>fit(x, y)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 每个类别的样本数量  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个类别的样本数:&#39;</span>, mnb<span style="color:#f92672">.</span>class_count_)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 每个特征在每个类别下发生(出现)的次数  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个特征发生(出现)次数:&#39;</span>, mnb<span style="color:#f92672">.</span>feature_count_)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 每个类别下, 每个特征的概率(需要取指数还原)  </span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;每个类别下特征的概率:&#39;</span>, np<span style="color:#f92672">.</span>exp(mnb<span style="color:#f92672">.</span>feature_log_prob_))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试集  </span>
</span></span><span style="display:flex;"><span>x_test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">4</span>]])  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;预测结果:&#39;</span>, mnb<span style="color:#f92672">.</span>predict(x_test))  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;预测结果概率:&#39;</span>, mnb<span style="color:#f92672">.</span>predict_proba(x_test))  
</span></span></code></pre></div><pre><code>  x1	x2	x3	y
0	1	4	2	0
1	1	4	4	1
2	4	4	2	0


每个类别的样本数: [4. 4.]
每个特征发生(出现)次数: [[10. 14. 10.]
 [ 9. 11. 11.]]
每个类别下特征的概率: [[0.2972973  0.40540541 0.2972973 ]
 [0.29411765 0.35294118 0.35294118]]
预测结果: [1]
预测结果概率: [[0.36890061 0.63109939]]
</code></pre>
<p><strong>利用鸢尾花数据集比较上述 3 个贝叶斯算法:</strong></p>
<p>对不同的数据集, 根据其分布情况选择适合的算法, 能得到更好的结果</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_iris  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> load_iris(return_X_y<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)  
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(x, y,  
</span></span><span style="display:flex;"><span>                        test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>models <span style="color:#f92672">=</span> [(<span style="color:#e6db74">&#39;高斯朴素贝叶斯分值:&#39;</span>, GaussianNB()),  
</span></span><span style="display:flex;"><span>          (<span style="color:#e6db74">&#39;伯努利朴素贝叶斯分值:&#39;</span>, BernoulliNB()),  
</span></span><span style="display:flex;"><span>          (<span style="color:#e6db74">&#39;多项式朴素贝叶斯分值:&#39;</span>, MultinomialNB())]  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> name, m <span style="color:#f92672">in</span> models:  
</span></span><span style="display:flex;"><span>    m<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>    print(name, m<span style="color:#f92672">.</span>score(x_test, y_test))  
</span></span></code></pre></div><pre><code>高斯朴素贝叶斯分值: 1.0
伯努利朴素贝叶斯分值: 0.23684210526315788
多项式朴素贝叶斯分值: 0.5789473684210527
</code></pre>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>决策树</h1>

<h2 id="1-概念理解">1, 概念理解</h2>
<p>决策树: 通过数据特征的差别, 用已知数据训练将不同数据划分到不同分支(子树)中, 层层划分, 最终得到一个树型结构, 用来对未知数据进行预测,
实现分类或回归</p>
<p>例如, 有如下数据集, 预测第 11 条数据能否偿还债务:</p>
<table>
<thead>
<tr>
<th>序号</th>
<th>有无房产</th>
<th>婚姻状况</th>
<th>年收入</th>
<th>能否偿还债务</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>是</td>
<td>单身</td>
<td>125</td>
<td>能</td>
</tr>
<tr>
<td>2</td>
<td>否</td>
<td>已婚</td>
<td>100</td>
<td>能</td>
</tr>
<tr>
<td>3</td>
<td>否</td>
<td>单身</td>
<td>100</td>
<td>能</td>
</tr>
<tr>
<td>4</td>
<td>是</td>
<td>已婚</td>
<td>110</td>
<td>能</td>
</tr>
<tr>
<td>5</td>
<td>是</td>
<td>离婚</td>
<td>60</td>
<td>能</td>
</tr>
<tr>
<td>6</td>
<td>否</td>
<td>离婚</td>
<td>95</td>
<td>不能</td>
</tr>
<tr>
<td>7</td>
<td>否</td>
<td>单身</td>
<td>85</td>
<td>不能</td>
</tr>
<tr>
<td>8</td>
<td>否</td>
<td>已婚</td>
<td>75</td>
<td>能</td>
</tr>
<tr>
<td>9</td>
<td>否</td>
<td>单身</td>
<td>90</td>
<td>不能</td>
</tr>
<tr>
<td>10</td>
<td>是</td>
<td>离婚</td>
<td>220</td>
<td>能</td>
</tr>
<tr>
<td>11</td>
<td>否</td>
<td>已婚</td>
<td>94</td>
<td>?</td>
</tr>
</tbody>
</table>
<p>我们可以将已知样本作如下划分(训练), 构建一颗决策树, 然后将第 11 条数据代入(测试), 落在哪一个叶子中, 它就是对应叶子的类别: 预测结果是
<strong>能</strong></p>
<p><img src="23-36-33.jpg" alt=""></p>
<p>上例中, 层级已经不可再分, 但如果只划分到婚姻状况就不再划分如何实现预测?</p>
<p>决策树实现预测:<br>
对于分类树, 叶子节点中, 哪个类别样本数量最多, 就将其作为未知样本的类别<br>
对于回归树, 使用叶子节点中, 所有样本的均值, 作为未知样本的结果</p>
<p>对于上例, 如果只划分到婚姻状况, 那对于婚姻状况这个叶子中, 不能偿还的最多, 预测结果就是 <strong>不能</strong></p>
<h2 id="2-分类决策树">2, 分类决策树</h2>
<p>对上例出现的情况, 我们会有如下问题:<br>
我们为什么以年收入开始划分, 依据是什么? 划分顺序怎么定?<br>
年收入为什么选 97.5 为划分阈值?<br>
要划分多少层才好, 是否越多越好?<br>
等等…</p>
<p>下面一步步来作讨论:</p>
<h3 id="201-信息熵">2.01, 信息熵</h3>
<p><strong>信息熵</strong> : 用来描述信源的不确定度, 不确定性越大, 信息熵越大. 例如, 明天海南要下雪, 不确定性非常小, 信息熵很小, 明天海南要下雨,
不确定性大, 信息熵就大</p>
<p>设随机变量 X 具有 m 个特征值, 各个值出现的概率为 
<span class="math align-center">$p_{1}$</span>, …, 
<span class="math align-center">$p_{m}$</span>, 且</p>

<span class="math align-center">$$p_{1}+p_{2}+\cdots+p_{m} = 1$$</span><p>则变量 X 的信息熵(信息期望值)为:</p>

<span class="math align-center">$$H(X)=-p_{1} \log_{2} p_{1} -p_{2} \log_{2} p_{2}-\cdots -p_{m} \log_{2}
p_{m}$$</span>
<span class="math align-center">$$ =-\sum_{i=1}^{m}p_{i}\log_{2}p_{i}$$</span><h3 id="202-概率分布与信息商的关系">2.02, 概率分布与信息商的关系</h3>
<p>假设明天下雨的概率从 0.01 ~ 0.99 递增, 那么不下雨的概率就从 0.99 ~ 0.01 递减, 看看概率分布和信息熵的关系:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.family&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;SimHei&#39;</span>  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.size&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">14</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 下雨的概率  </span>
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0.01</span>, <span style="color:#ae81ff">0.99</span>, <span style="color:#ae81ff">100</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 信息熵  </span>
</span></span><span style="display:flex;"><span>h <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>p <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log2(p) <span style="color:#f92672">-</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log2(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制关系图  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(p, h)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;概率分布&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;信息熵&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;概率分布和信息熵关系图&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_1_0.png" alt="png"></p>
<p>可见, 概率分布越均衡, 不确定性越大, 信息熵越大, 在所有概率都相等(p下雨=p不下雨)时, 信息熵最大</p>
<p>如果把概率分布转换到决策树的数据集上, 信息熵体现的就是数据的 <strong>不纯度</strong> , 即样本类别的均衡程度. 因为数据集是未分类的, 要把它分类,
样本类别越均衡, 各个类别的占比(概率)分布越均衡, 不纯度越高, 信息熵越大</p>
<h3 id="203-信息增益">2.03, 信息增益</h3>
<p>信息增益的定义如下:</p>

<span class="math align-center">$$I G\left(D_{p}, f\right)=I\left(D_{p}\right)-\sum_{j=1}^{n}
\frac{N_{j}}{N_{p}} I\left(D_{j}\right) $$</span><p>
<span class="math align-center">$f$</span>: 划分的特征<br>

<span class="math align-center">$D_{p}$</span>: 父节点, 即使用特征 f 分割之前的节点<br>

<span class="math align-center">$I G\left(D_{p}, f\right)$</span>: 父节点 
<span class="math align-center">$D_{p}$</span> 使用特征 f 划分下, 获得的信息增益<br>

<span class="math align-center">$I\left(D_{p}\right)$</span>：父节点不纯度, 信息熵是度量标准之一<br>

<span class="math align-center">$D_{j}$</span>: 父节点 
<span class="math align-center">$D_{p}$</span> 经过分割之后, 会产生 n 个子节点, 
<span class="math align-center">$D_{j}$</span> 为第 j 个子节点<br>

<span class="math align-center">$I\left(D_{j}\right)$</span>：子节点不纯度<br>

<span class="math align-center">$N_{p}$</span>: 父节点 
<span class="math align-center">$D_{p}$</span> 包含样本的数量<br>

<span class="math align-center">$N_{j}$</span>: 第 j 个子节点 
<span class="math align-center">$D_{j}$</span> 包含样本的数量</p>
<p>如果是二叉树, 即父节点最多分为左右两个子节点, 此时, 信息增益为:</p>

<span class="math align-center">$$I G\left(D_{p}, f\right)=I\left(D_{p}\right)-\frac{N_{l e f t}}{N_{p}}
I\left(D_{l e f t}\right)-\frac{N_{r i g h t}}{N_{p}} I\left(D_{r i g h
t}\right)$$</span><p>可见, 信息增益就是父节点的不纯度减去所有子节点的(加权)不纯度</p>
<p>父节点的不纯度是不变的, 在选择特征进行类别划分时, 应该让子节点的不纯度尽可能低, 这样训练可以更快完成, 信息增益也最大. 这正是训练决策树时,
选择特征顺序的依据</p>
<p>以开头的例子为例, 不纯度使用信息熵度量, 则根节点的信息熵为:</p>

<span class="math align-center">$$I\left(D_{p}\right)=-0.7 * \log _{2} 0.7-0.3 * \log _{2} 0.3=0.88$$</span><p>如果以”有无房产”划分, 则可计算得子节点信息熵:</p>

<span class="math align-center">$$\begin{array}{l}  
I\left(D_{\text {有房产 }}\right)=0 \  
I\left(D_{\text {无房产 }}\right)=1  
\end{array}$$</span><p>从而可得根节点信息增益为:</p>

<span class="math align-center">$$I G(\text { 有无房产 })=0.88-0.4 * 0-0.6 * 1=0.28 $$</span><p>同理,</p>

<span class="math align-center">$$I G(\text { 婚姻状况 })=0.205 $$</span><p>而对于年收入, 将年收入排序后, 取不同类别的分界点年收入(75 与 85, 95 与 100)的均值进行划分, 比较哪一个信息增益大:</p>

<span class="math align-center">$$\begin{array}{l}  
I\left(D_{\text {年收入 }< 80}\right)=0 \  
I\left(D_{\text { 年收入 } >=80}\right)=0.954 \  
I G(\text { 年收入 }=80)=0.88-0.2 * 0-0.8 * 0.954=0.117  
\end{array}$$</span><p>同理,</p>

<span class="math align-center">$$I G(\text { 年收入 }=97.5)=0.395$$</span><p>可见, 以 年收入=97.5 划分时, 信息增益最大, 故首先选它进行划分</p>
<p>根节点划分结束, 第二层的父节点以同样的方式计算之后再次划分, 一直到划分停止</p>
<h3 id="204-过拟合与欠拟合">2.04, 过拟合与欠拟合</h3>
<p>如果不设置条件, 是不是划分深度越大越好呢?</p>
<p>下面以鸢尾花数据集为例, 看看划分深度对模型效果的影响:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_iris  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeClassifier  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> load_iris(return_X_y<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)  
</span></span><span style="display:flex;"><span>x <span style="color:#f92672">=</span> x[:, :<span style="color:#ae81ff">2</span>]  
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(  
</span></span><span style="display:flex;"><span>    x, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">参数介绍:  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">criterion: 不纯度度量标准, 默认基尼系数 gini, 信息熵为 entropy  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">splitter: 选择划分节点的方式, 默认最好 best, 随机 random  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">max_depth: 划分深度, 默认 None 不设限  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">min_samples_split: 划分节点的最小样本数, 默认 2  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">min_samples_leaf: 划分节点后, 叶子节点的最少样本数, 默认 1  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">max_features: 划分节点时, 考虑的最大特征数, 默认 None 考虑所有, 设置数量后会随机选择  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">random_state: 随机种子, 控制模型的随机行为  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>  
</span></span><span style="display:flex;"><span>tree <span style="color:#f92672">=</span> DecisionTreeClassifier()  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义列表, 用来储存不同深度下, 模型的分值  </span>
</span></span><span style="display:flex;"><span>train_score <span style="color:#f92672">=</span> []  
</span></span><span style="display:flex;"><span>test_score <span style="color:#f92672">=</span> []  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置深度 1~12 开始训练  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> depth <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">13</span>):  
</span></span><span style="display:flex;"><span>    tree <span style="color:#f92672">=</span> DecisionTreeClassifier(criterion<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;entropy&#39;</span>,  
</span></span><span style="display:flex;"><span>                    max_depth<span style="color:#f92672">=</span>depth, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>    tree<span style="color:#f92672">.</span>fit(x_train, y_train) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    train_score<span style="color:#f92672">.</span>append(tree<span style="color:#f92672">.</span>score(x_train, y_train))  
</span></span><span style="display:flex;"><span>    test_score<span style="color:#f92672">.</span>append(tree<span style="color:#f92672">.</span>score(x_test, y_test))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(train_score, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;训练集分值&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(test_score, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;测试集分值&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;划分深度&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;分值&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show() 
</span></span></code></pre></div><p><img src="output_6_0.png" alt="png"></p>
<p>可见, 划分深度小, 训练集和测试集的分值都小, 容易欠拟合<br>
随着划分深度的增加, 分值都在增加, 模型预测效果也在变好<br>
当深度增加到一定程度, 深度再增加, 训练集分值随着增加, 但造成了模型过分依赖训练集数据特征, 从而测试集分值减小, 容易过拟合</p>
<h2 id="3-不纯度度量标准">3, 不纯度度量标准</h2>
<p>不纯度度量标准有:</p>
<p><strong>信息熵</strong></p>

<span class="math align-center">$$I_{H}(D)=-\sum_{i=1}^{m} p(i \mid D) \log _{2} p(i \mid D) $$</span><p>m: 节点 D 中含有样本的类别数量<br>

<span class="math align-center">$p(i \mid D)$</span>: 节点 D 中, 属于类别 i 的样本占节点 D 中样本总数的比例(概率)</p>
<p><strong>基尼系数</strong></p>

<span class="math align-center">$$I_{G}(D)=1-\sum_{i=1}^{m} p(i \mid D)^{2}$$</span><p><strong>错误率</strong></p>

<span class="math align-center">$$I_{E}(D)=1-\max {p(i \mid D)}$$</span><p>看看各个度量标准与概率分布的关系:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">entropy</span>(p):  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>p <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log2(p) <span style="color:#f92672">-</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p) <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log2(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">gini</span>(p):  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> p<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span> <span style="color:#f92672">-</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p)<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">error</span>(p):  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> np<span style="color:#f92672">.</span>max([p, <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p], axis<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>p <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>linspace(<span style="color:#ae81ff">0.0001</span>, <span style="color:#ae81ff">0.9999</span>, <span style="color:#ae81ff">200</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>en <span style="color:#f92672">=</span> entropy(p)  
</span></span><span style="display:flex;"><span>er <span style="color:#f92672">=</span> error(p)  
</span></span><span style="display:flex;"><span>g <span style="color:#f92672">=</span> gini(p)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, lab, ls <span style="color:#f92672">in</span> zip([en, g, er],  
</span></span><span style="display:flex;"><span>                      [<span style="color:#e6db74">&#39;信息熵&#39;</span>, <span style="color:#e6db74">&#39;基尼系数&#39;</span>, <span style="color:#e6db74">&#39;错误率&#39;</span>],  
</span></span><span style="display:flex;"><span>                      [<span style="color:#e6db74">&#39;-&#39;</span>, <span style="color:#e6db74">&#39;:&#39;</span>, <span style="color:#e6db74">&#39;--&#39;</span>]):  
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>plot(p, i, label<span style="color:#f92672">=</span>lab, linestyle<span style="color:#f92672">=</span>ls, lw<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xlabel(<span style="color:#e6db74">&#39;概率分布&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>ylabel(<span style="color:#e6db74">&#39;不纯度&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_9_0.png" alt="png"></p>
<p>可见, 无论选哪一种度量标准, 样本属于同一类, 不纯度都是 0; 样本中不同类别占比相同时, 不纯度最大</p>
<h2 id="4-决策树常用算法介绍">4, 决策树常用算法介绍</h2>
<p><strong>ID3</strong></p>
<p>ID3 (Iterative Dichotomiser3), 迭代二分法特点:<br>
-使用多叉树结构<br>
-使用信息熵作为不纯度度量, 选择信息增益最大的特征划分<br>
-经典算法, 简单, 训练快<br>
局限:<br>
-不支持连续特征<br>
-不支持缺失值处理<br>
-不支持回归<br>
-倾向选择特征取值多的特征来划分, 例如按身份证号划分, 一个号码就是一个特征</p>
<p><strong>C4.5</strong></p>
<p>ID3算法改进而来, 特点:<br>
-使用多叉树结构<br>
-不支持回归<br>
优化:<br>
-支持缺失值处理<br>
-连续值进行离散化处理<br>
-信息熵作为不纯度度量, 但选择 <strong>信息增益率</strong> 最大的特征划分</p>
<p>信息增益率:</p>

<span class="math align-center">$$I G_{\text {Ratio}}\left(D_{p}, f\right)=\frac{I G_{H}\left(D_{p},
f\right)}{I_{H}(f)} $$</span><p>
<span class="math align-center">$I_{H}(f)$</span>: 在特征 
<span class="math align-center">$f$</span> 下, 取各个特征值计算得到的信息熵之和, 其实就是特征 
<span class="math align-center">$f$</span> 的不纯度, 特征值越多, 特征不纯度越大</p>
<p>选择信息增益最大的特征来划分, 父节点的信息熵不变, 就要求信息增益的第二项 $\sum_{j=1}^{n} \frac{N_{j}}{N_{p}}
I\left(D_{j}\right)$ 最小, 从而会倾向选择特征取值多的特征</p>
<p>因为, 特征取值多, 通常划分之后子节点的不纯度(信息熵)就更低, 例如极端情况, 选身份证划分, 划分之后不管什么类别, 子节点都只有一种类别,
不纯度都是 0, 第二项就是 0, 信息增益就最大</p>
<p>因此, 采用信息增益率, 将 信息增益/特征不纯度, 就避免了 特征不纯度大 造成 信息增益大 而选择类别多的特征来划分的情况</p>
<p>看看类别数量与信息熵的关系:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>en <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> p: np<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span>p <span style="color:#f92672">*</span> np<span style="color:#f92672">.</span>log2(p))  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>a1 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.7</span>])  
</span></span><span style="display:flex;"><span>a2 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.3</span>, <span style="color:#ae81ff">0.4</span>])  
</span></span><span style="display:flex;"><span>a3 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.25</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>)  
</span></span><span style="display:flex;"><span>a4 <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#ae81ff">0.1</span>] <span style="color:#f92672">*</span> <span style="color:#ae81ff">10</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(en(a1), en(a2), en(a3), en(a4), sep<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;</span><span style="color:#ae81ff">\n</span><span style="color:#e6db74">&#39;</span>)  
</span></span></code></pre></div><pre><code>0.8812908992306927
1.5709505944546684
2.0
3.321928094887362
</code></pre>
<p><strong>CART</strong></p>
<p>CART (Classification And Regression Tree), 分类回归树, 特点如下:<br>
-使用二叉树结构<br>
-支持连续值与缺失值处理<br>
-分类时, 使用基尼系数作为不纯度度量, 选择基尼增益最大的特征划分<br>
-回归时, 使用 MSE 或 MAE 最小的特征划分</p>
<h2 id="5-回归决策树">5, 回归决策树</h2>
<p>回归决策树因变量 y 是连续的, 使用叶子节点的均值来预测未知样本, 使用 MSE 或 MAE 作为特征划分的评估指标</p>
<p>在 scikit-learn 中, 使用的是优化的 CART 算法来实现决策树</p>
<p>以波士顿房价为例来实现(参数参考分类决策树):</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> load_boston  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.tree <span style="color:#f92672">import</span> DecisionTreeRegressor  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.model_selection <span style="color:#f92672">import</span> train_test_split  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> load_boston(return_X_y<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)  
</span></span><span style="display:flex;"><span>x_train, x_test, y_train, y_test <span style="color:#f92672">=</span> train_test_split(  
</span></span><span style="display:flex;"><span>            x, y, test_size<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>tree <span style="color:#f92672">=</span> DecisionTreeRegressor(max_depth<span style="color:#f92672">=</span><span style="color:#ae81ff">5</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>tree<span style="color:#f92672">.</span>fit(x_train, y_train)  
</span></span><span style="display:flex;"><span>print(tree<span style="color:#f92672">.</span>score(x_train, y_train))  
</span></span><span style="display:flex;"><span>print(tree<span style="color:#f92672">.</span>score(x_test, y_test))  
</span></span></code></pre></div><pre><code>0.9204825770764915
0.8763987309111113
</code></pre>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1>K-Means 算法</h1>

<h2 id="1-聚类">1, 聚类</h2>
<p>前面接触的算法, 都是 <strong>监督学习</strong> , 即训练数据中自变量(特征)和因变量(结果)都是已知的, 用含有结果的训练集建立模型,
然后对未知结果的数据进行预测</p>
<p>聚类属于 <strong>无监督学习</strong> , 训练数据中没有”已知结果的监督”. 聚类的目的, 就是通过已知样本数据的特征, 将数据划分为若干个类别,
每个类别成一个类簇, 使得同一个簇内的数据相似度越大, “物以类聚”, 不同簇之间的数据相似度越小, 聚类效果越好</p>
<p>聚类的样本相似度根据距离来度量</p>
<h2 id="2-k-means">2, K-Means</h2>
<p>即 K 均值算法, 是常见的聚类算法, 该算法将数据集分为 K 个簇, 每个簇使用簇内所有样本的均值来表示, 该均值称为”质心”</p>
<p>K-Means 算法的目标, 就是选择适合的质心, 使得每个簇内, 样本点距质心的距离尽可能的小, 从而保证簇内样本有较高相似度</p>
<p>算法实现步骤:</p>
<p>a, 从样本中选择 K 个点作为初始质心<br>
b, 计算每个样本点到各个质心的距离, 将样本点划分到距离最近的质心所对应的簇中<br>
c, 计算每个簇内所有样本的均值, 使用该均值作为新的质心<br>
d, 重复 b 和 c, 重复一定次数质心一般会趋于稳定, 如果达到以下条件, 重复结束:<br>
– 质心位置变化小于指定的阈值<br>
– 达到最迭代环次数</p>
<p>对于算法的实现步骤, 我们有几个重要的疑问:</p>
<p>– 1.怎么评价质心是否达到了最佳位置?<br>
– 2.初始质心随机选, 还是选在哪里?<br>
– 3. K 值怎么定?</p>
<h2 id="3-算法优化目标">3, 算法优化目标</h2>
<p>样本的相似度是根据距离来度量的, 一般使用簇内 <strong>误差平方和</strong> (within-cluster SSE 簇惯性) 来作为优化算法的目标函数,
距离常用欧氏距离, 优化目标就是使 SSE 最小化:</p>

<span class="math align-center">$$S S E=\sum_{i=1}^{k}
\sum_{j=1}^{m_{i}}\left(\left|x_{j}-\mu_{i}\right|^{2}\right)$$</span><p>k: 族的数量</p>
<p>
<span class="math align-center">$m_{i}$</span>: 第 i 个簇含有的样本数量</p>
<p>
<span class="math align-center">${\mu}_{i}$</span>: 第 i 个族的质心</p>
<p>
<span class="math align-center">$\left|x_{j}-\mu_{i}\right|$</span>: 第 i 个族中，每个样本 
<span class="math align-center">$x_{j}$</span> 与质心 
<span class="math align-center">$\mu_{i}$</span> 的距离</p>
<p>同一个数据集, 相同的簇数, SSE 越小, 通常质心位置更佳, 算法模型更好</p>
<h2 id="4-初始质心的影响">4, 初始质心的影响</h2>
<p>初始质心可以随机选择, 但由于算法是通过迭代初始质心一步步实现, 初始质心的位置受随机性影响, 算法训练的最终结果也会受到影响</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> KMeans  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.datasets <span style="color:#f92672">import</span> make_blobs  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> matplotlib.pyplot <span style="color:#66d9ef">as</span> plt  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.family&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#e6db74">&#39;Microsoft YaHei&#39;</span>   
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;font.size&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#ae81ff">12</span>   
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>rcParams[<span style="color:#e6db74">&#39;axes.unicode_minus&#39;</span>] <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>   
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">生成数据:  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">n_samples: 样本数量  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">n_features: 特征数  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">centers: 聚类中心  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">cluster_std: 簇的标准差, 可以统一指定, 也分别指定  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">&#39;&#39;&#39;</span>  
</span></span><span style="display:flex;"><span>centers <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">5</span>, <span style="color:#ae81ff">2</span>], [<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>]]  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> make_blobs(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">90</span>,  
</span></span><span style="display:flex;"><span>                  n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>,  
</span></span><span style="display:flex;"><span>                  centers<span style="color:#f92672">=</span>centers,  
</span></span><span style="display:flex;"><span>                  cluster_std<span style="color:#f92672">=</span>[<span style="color:#ae81ff">2.2</span>, <span style="color:#ae81ff">2.5</span>, <span style="color:#ae81ff">2</span>],  
</span></span><span style="display:flex;"><span>                  random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># x 是特征, y 是类别标签  </span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制原始数据  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>,<span style="color:#ae81ff">8</span>))  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">221</span>)  
</span></span><span style="display:flex;"><span>colors <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([<span style="color:#e6db74">&#39;Coral&#39;</span>, <span style="color:#e6db74">&#39;SeaGreen&#39;</span>, <span style="color:#e6db74">&#39;RoyalBlue&#39;</span>])  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x[:, <span style="color:#ae81ff">0</span>], x[:, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span>colors[y], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;原始数据&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">&#39;原始数据&#39;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义绘制聚类结果的函数  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">plot_cluster</span>(model, train, test<span style="color:#f92672">=</span><span style="color:#66d9ef">None</span>):  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">global</span> colors  <span style="color:#75715e"># 使用上面的颜色  </span>
</span></span><span style="display:flex;"><span>    cc <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>cluster_centers_ <span style="color:#75715e"># 获取质心  </span>
</span></span><span style="display:flex;"><span>    label <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>labels_ <span style="color:#75715e"># 获取聚类结果的标签  </span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 绘制质心  </span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(cc[:, <span style="color:#ae81ff">0</span>], <span style="color:#75715e"># 质心的 x 坐标  </span>
</span></span><span style="display:flex;"><span>                cc[:, <span style="color:#ae81ff">1</span>], <span style="color:#75715e"># 质心的 y 坐标  </span>
</span></span><span style="display:flex;"><span>                marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;*&#39;</span>,  
</span></span><span style="display:flex;"><span>                s<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>,  
</span></span><span style="display:flex;"><span>                c<span style="color:#f92672">=</span>colors)  
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 绘制训练集  </span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(train[:, <span style="color:#ae81ff">0</span>], train[:, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, c<span style="color:#f92672">=</span>colors[label])  
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 绘制测试集  </span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> test <span style="color:#f92672">is</span> <span style="color:#f92672">not</span> <span style="color:#66d9ef">None</span>:  
</span></span><span style="display:flex;"><span>        y_hat <span style="color:#f92672">=</span> model<span style="color:#f92672">.</span>predict(test)  
</span></span><span style="display:flex;"><span>        plt<span style="color:#f92672">.</span>scatter(test[:, <span style="color:#ae81ff">0</span>], test[:, <span style="color:#ae81ff">1</span>], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;+&#39;</span>,  
</span></span><span style="display:flex;"><span>                    s<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>, c<span style="color:#f92672">=</span>colors[y_hat])          
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 标题  </span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>title(<span style="color:#e6db74">f</span><span style="color:#e6db74">&#39;SSE:</span><span style="color:#e6db74">{</span>model<span style="color:#f92672">.</span>inertia_<span style="color:#e6db74">:</span><span style="color:#e6db74">.1f</span><span style="color:#e6db74">}</span><span style="color:#e6db74"> 迭代次数:</span><span style="color:#e6db74">{</span>model<span style="color:#f92672">.</span>n_iter_<span style="color:#e6db74">}</span><span style="color:#e6db74">&#39;</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 测试集  </span>
</span></span><span style="display:flex;"><span>test <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>array([[<span style="color:#ae81ff">6</span>, <span style="color:#ae81ff">5</span>]])      
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制不同初始质心的聚类结果  </span>
</span></span><span style="display:flex;"><span>seed <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">10</span>, <span style="color:#ae81ff">100</span>]  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">5</span>):  
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">2</span>, <span style="color:#ae81ff">2</span>, i)  
</span></span><span style="display:flex;"><span>    kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, <span style="color:#75715e"># 簇数  </span>
</span></span><span style="display:flex;"><span>                    init<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;random&#39;</span>, <span style="color:#75715e"># 初始化方式  </span>
</span></span><span style="display:flex;"><span>                    n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, <span style="color:#75715e"># 初始化质心组数  </span>
</span></span><span style="display:flex;"><span>                    random_state<span style="color:#f92672">=</span>seed[i<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>])  
</span></span><span style="display:flex;"><span>    kmeans<span style="color:#f92672">.</span>fit(x)  
</span></span><span style="display:flex;"><span>    plot_cluster(kmeans, x)  
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 测试结果  </span>
</span></span><span style="display:flex;"><span>    plot_cluster(kmeans, x, test)  
</span></span></code></pre></div><p><img src="output_1_0.png" alt="png"></p>
<p>从上图可以看出受初始化质心的影响, 聚类效果(SSE) 与 收敛速度(迭代次数) 会不同, 也即是可能会收敛到局部最小, 而不是整体最优; 同时,
也可以看出 SSE 越小, 整体结果越优, 越接近原始数据</p>
<h2 id="5-k-means-优化">5, K-Means++ 优化</h2>
<p>针对上述初始化质心造成的问题, 设置初始化多组质心可以得到缓解, 但通常限于聚类簇数较少的情况, 如果簇数较多, 可能就不会有效</p>
<p>于是有了 K-Means++, 选择初始化质心时, 不在随机选, 而是按下述步骤进行选择:</p>
<p>– 1, 从训练数据中随机选择一个样本点, 作为初始质心<br>
– 2, 对任意一个非质心样本点 
<span class="math align-center">$x^{(i)}$</span>, 计算 
<span class="math align-center">$x^{(i)}$</span> 与现有最近质心的距离 
<span class="math align-center">$D\left(x^{(i)}\right)$</span><br>
– 3, 根据概率 
<span class="math align-center">$\frac{D\left(x^{(i)}\right)^{2}}{\sum_{j=1}^{m}D\left(x^{(j)}\right)^{2}}$</span> 最大, 来选择最远的一个样本点 
<span class="math align-center">$x^{(i)}$</span> 作为质心, m 为非质心样本点数量<br>
– 4, 重复 2 和 3, 直到选择了 K 个质心为止</p>
<p>做了优化之后, 保证了初始质心不会集中, 而是分散在数据集中</p>
<p>下面试试 K-Means++ 的聚类效果:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, init<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;k-means++&#39;</span>, n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)  
</span></span><span style="display:flex;"><span>kmeans<span style="color:#f92672">.</span>fit(x)  
</span></span><span style="display:flex;"><span>plot_cluster(kmeans, x)  
</span></span></code></pre></div><p><img src="output_3_0.png" alt="png"></p>
<h2 id="6-确定-k-值">6, 确定 K 值</h2>
<p>K 是超参数, 需要预先人为指定</p>
<p>有时需要按照建模的需求和目的来选择聚类的个数, 但是 K 值选择不当, 聚类效果可能不佳. 例如实际 3 类, K 选了 10, 或者 K 无限制,
取值和样本点个数一样, 最后每个点一个类, SEE 为 0, 但是聚类已经毫无意义</p>
<p>如果不是硬性要求 K 的取值, 怎么确定最佳的 K 值呢? 一个比较好的方法就是 <strong>肘部法则</strong> :</p>
<p>SEE 需要越小越好, K 又不能取太大, 我们可以看看他们之间的关系:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># 设置列表储存 SSE  </span>
</span></span><span style="display:flex;"><span>sse <span style="color:#f92672">=</span> []  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># K 值从 1~9 变化  </span>
</span></span><span style="display:flex;"><span>scope <span style="color:#f92672">=</span> range(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">10</span>)  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> k <span style="color:#f92672">in</span> scope:  
</span></span><span style="display:flex;"><span>    kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span>k)  
</span></span><span style="display:flex;"><span>    kmeans<span style="color:#f92672">.</span>fit(x)  
</span></span><span style="display:flex;"><span>    sse<span style="color:#f92672">.</span>append(kmeans<span style="color:#f92672">.</span>inertia_)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>xticks(scope)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>plot(scope, sse, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;o&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>show()  
</span></span></code></pre></div><p><img src="output_5_0.png" alt="png"></p>
<p>从上图可以看出, K 增加, SSE 减小, 但当 K &gt; 3 时, K 再增加, SSE 减小变得缓慢, 所以 K 选择 3, 实际情况也是 3</p>
<h2 id="6-mini-batch-k-means">6, Mini Batch K-Means</h2>
<p>K-Means 每次迭代都会使用所有数据参与运算, 当数据集较大时, 会比较耗时. Mini Batch K-Means (小批量 K-Means)
算法每次迭代使用小批量样本训练, 逐批次累计的方式进行计算, 从而大大减少计算时间. 效果上, 通常只是略差于 K-Means</p>
<p>Mini Batch K-Means 算法实现步骤:</p>
<p>a, 从数据集中随机选择部分数据, 使用 K-Means 算法在这部分数据上聚类, 获取质心<br>
b, 再从数据集中随机选择部分数据, 分别分配给最近的质心<br>
c, 每个簇根据现有的数据集更新质心<br>
d, 重复 b 和 c, 直到质心变化小于指定阈值或达到最大迭代次数</p>
<p>下面比较一下两个算法:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> time  
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> pandas <span style="color:#66d9ef">as</span> pd  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.cluster <span style="color:#f92672">import</span> MiniBatchKMeans  
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> sklearn.metrics.pairwise <span style="color:#f92672">import</span> pairwise_distances_argmin  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> warnings
</span></span><span style="display:flex;"><span>warnings<span style="color:#f92672">.</span>filterwarnings(<span style="color:#e6db74">&#39;ignore&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 生成数据  </span>
</span></span><span style="display:flex;"><span>centers <span style="color:#f92672">=</span> [[<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">1</span>], [<span style="color:#ae81ff">400</span>, <span style="color:#ae81ff">100</span>], [<span style="color:#ae81ff">100</span>, <span style="color:#ae81ff">400</span>]]  
</span></span><span style="display:flex;"><span>x, y <span style="color:#f92672">=</span> make_blobs(n_samples<span style="color:#f92672">=</span><span style="color:#ae81ff">8000</span>, n_features<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, centers<span style="color:#f92672">=</span>centers,  
</span></span><span style="display:flex;"><span>                  cluster_std<span style="color:#f92672">=</span><span style="color:#ae81ff">120</span>, random_state<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义函数, 用于计算模型训练时间  </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">elapsed_time</span>(model, data):  
</span></span><span style="display:flex;"><span>    start <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()  
</span></span><span style="display:flex;"><span>    model<span style="color:#f92672">.</span>fit(data)  
</span></span><span style="display:flex;"><span>    end <span style="color:#f92672">=</span> time<span style="color:#f92672">.</span>time()  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">return</span> end <span style="color:#f92672">-</span> start  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_clusters <span style="color:#f92672">=</span> len(centers)  
</span></span><span style="display:flex;"><span>kmeans <span style="color:#f92672">=</span> KMeans(n_clusters<span style="color:#f92672">=</span>n_clusters)  
</span></span><span style="display:flex;"><span>mbk <span style="color:#f92672">=</span> MiniBatchKMeans(n_clusters<span style="color:#f92672">=</span>n_clusters,  
</span></span><span style="display:flex;"><span>                      batch_size<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>, <span style="color:#75715e"># 小批量的大小  </span>
</span></span><span style="display:flex;"><span>                     n_init<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span> <span style="color:#75715e"># 和 KMeans 统一为 10  </span>
</span></span><span style="display:flex;"><span>                     )  
</span></span><span style="display:flex;"><span>kmeans_time <span style="color:#f92672">=</span> elapsed_time(kmeans, x)  
</span></span><span style="display:flex;"><span>mbk_time <span style="color:#f92672">=</span> elapsed_time(mbk, x)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;K-Means耗时:&#39;</span>, kmeans_time)  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;Mini Batch K-Means耗时:&#39;</span>, mbk_time)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 绘制聚类效果  </span>
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">5</span>))  
</span></span><span style="display:flex;"><span>model <span style="color:#f92672">=</span> [kmeans, mbk]  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, m <span style="color:#f92672">in</span> enumerate(model, start<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):  
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, i)  
</span></span><span style="display:flex;"><span>    plot_cluster(m, x)  
</span></span></code></pre></div><pre><code>K-Means耗时: 0.051812171936035156
Mini Batch K-Means耗时: 0.04886937141418457
</code></pre>
<p><img src="output_7_1.png" alt="png"></p>
<p>可见, 聚类耗时 K-Means 更多, 如果数据量很大, 耗时会更明显, 而聚类效果基本一样. 但发现颜色对不上, 这是因为质心的随机性,
聚类之后质心虽然最终落在相同的位置, 但是顺序不一致, 从而聚类的结果标签不一致, 即使是同一个算法, 运行几次, 标签结果也会不一致</p>
<p>我们将相同簇用相同的颜色绘制:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>figure(figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">12</span>, <span style="color:#ae81ff">5</span>))  
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 定义列表, 用来保存两个模型预测结果  </span>
</span></span><span style="display:flex;"><span>y_hat_list <span style="color:#f92672">=</span> []  
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> i, m <span style="color:#f92672">in</span> enumerate(model, start<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>):  
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>subplot(<span style="color:#ae81ff">1</span>, <span style="color:#ae81ff">2</span>, i)  
</span></span><span style="display:flex;"><span>    y_hat <span style="color:#f92672">=</span> m<span style="color:#f92672">.</span>predict(x)  
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> m <span style="color:#f92672">==</span> mbk:  
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#39;&#39;&#39;  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        因为输出的质心顺序就是训练结果标签的顺序  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        故可以按 mbk 训练的质心, 去找 kmeans 训练的相同簇的质心  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        pairwise_distances_argmin(x, y) 解释:  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        依次取出数组 X 中的元素 x,   
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        计算找到数组 Y 中与 x 距离最近的元素 y 的索引,   
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        返回索引构成的数组  
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        &#39;&#39;&#39;</span>  
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 将两者相同簇的质心一一对应并按 mbk 质心的顺序封装成字典  </span>
</span></span><span style="display:flex;"><span>        ar <span style="color:#f92672">=</span> pairwise_distances_argmin(  
</span></span><span style="display:flex;"><span>        mbk<span style="color:#f92672">.</span>cluster_centers_, kmeans<span style="color:#f92672">.</span>cluster_centers_)  
</span></span><span style="display:flex;"><span>        dict_ <span style="color:#f92672">=</span> dict(enumerate(ar))  
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># 用 mbk 的训练结果标签 y_hat 就可以寻找到对应的 kmeans 的质心  </span>
</span></span><span style="display:flex;"><span>        y_hat <span style="color:#f92672">=</span> pd<span style="color:#f92672">.</span>Series(y_hat)<span style="color:#f92672">.</span>map(dict_)<span style="color:#f92672">.</span>values  
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 将预测结果加入到列表中  </span>
</span></span><span style="display:flex;"><span>    y_hat_list<span style="color:#f92672">.</span>append(y_hat)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    plt<span style="color:#f92672">.</span>scatter(x[:, <span style="color:#ae81ff">0</span>], x[:, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span>colors[y_hat], marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>)  
</span></span></code></pre></div><p><img src="output_9_0.png" alt="png"></p>
<p>比较两个算法聚类结果的差异:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>same <span style="color:#f92672">=</span> y_hat_list[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">==</span> y_hat_list[<span style="color:#ae81ff">1</span>]  
</span></span><span style="display:flex;"><span>diff <span style="color:#f92672">=</span> y_hat_list[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">!=</span> y_hat_list[<span style="color:#ae81ff">1</span>]  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x[same, <span style="color:#ae81ff">0</span>], x[same, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;g&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;预测相同&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>scatter(x[diff, <span style="color:#ae81ff">0</span>], x[diff, <span style="color:#ae81ff">1</span>], c<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;r&#39;</span>, marker<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;.&#39;</span>, label<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;预测不同&#39;</span>)  
</span></span><span style="display:flex;"><span>plt<span style="color:#f92672">.</span>legend()  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;相同数量:&#39;</span>, x[same]<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])  
</span></span><span style="display:flex;"><span>print(<span style="color:#e6db74">&#39;不同数量:&#39;</span>, x[diff]<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>])  
</span></span></code></pre></div><pre><code>相同数量: 7967
不同数量: 33
</code></pre>
<p><img src="output_11_1.png" alt="png"></p>
<p>两个算法聚类结果只有 33 个样本点不同</p>

            <footer class="footline">
            </footer>
          </article>

          </section>        </div>
      </main>
    </div>
    <aside id="sidebar" class="default-animation showVisitedLinks">
      <div id="header-wrapper" class="default-animation">
        <div id="header" class="default-animation">
          <a id="logo" href="/">
            <br><font face="serif" size="6" color="#17202A"><b>🏠Jupyter.fun</b></font>
          </a>
        </div>
        <div class="searchbox default-animation">
          <label for="search-by"><i class="fas fa-search"></i></label>
          <input data-search-input id="search-by" type="search" placeholder="Search...">
          <span data-search-clear=""><i class="fas fa-times"></i></span>
        </div>
        <script>
          var contentLangs=['en'];
        </script>
        <script src="/js/auto-complete.js?1661054444" defer></script>
        <script src="/js/lunr.min.js?1661054444" defer></script>
        <script src="/js/lunr.stemmer.support.min.js?1661054444" defer></script>
        <script src="/js/lunr.multi.min.js?1661054444" defer></script>
        <script src="/js/lunr.en.min.js?1661054444" defer></script>
        <script src="/js/search.js?1661054444" defer></script>
      </div>
      <div id="content-wrapper" class="highlightable">
        <ul class="topics">
          <li data-nav-id="/python/" title="Python 语法基础知识详解和查询手册" class="dd-item"><input type="checkbox" id="section-c8861a2651b27ee3930a98ce392020d1" class="toggle"/><label for="section-c8861a2651b27ee3930a98ce392020d1" ></label><a href="/python/">Python<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/01_numeric/" title="数字类型概述" class="dd-item"><input type="checkbox" id="section-f26ae7da78c4758569d4b453b66b4231" class="toggle"/><label for="section-f26ae7da78c4758569d4b453b66b4231" ></label><a href="/python/01_numeric/">数字类型<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/01_numeric/02_int/" title="整数及其位运算" class="dd-item"><a href="/python/01_numeric/02_int/">整数及其位运算<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/01_numeric/03_bool/" title="布尔值及布尔运算" class="dd-item"><a href="/python/01_numeric/03_bool/">布尔值及布尔运算<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/01_numeric/04_float/" title="浮点数" class="dd-item"><a href="/python/01_numeric/04_float/">浮点数<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/01_numeric/05_complex/" title="复数" class="dd-item"><a href="/python/01_numeric/05_complex/">复数<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/01_numeric/06_number_operations/" title="数字运算" class="dd-item"><a href="/python/01_numeric/06_number_operations/">数字运算<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/02_string/" title="字符串概述" class="dd-item"><input type="checkbox" id="section-0a8894164d573ed7362e26196efabc7a" class="toggle"/><label for="section-0a8894164d573ed7362e26196efabc7a" ></label><a href="/python/02_string/">字符串<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/02_string/02_concatenated_string/" title="字符串拼接" class="dd-item"><a href="/python/02_string/02_concatenated_string/">字符串拼接<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/03_escape_character/" title="转义字符" class="dd-item"><a href="/python/02_string/03_escape_character/">转义字符<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/04_str.count/" title="str.count 统计" class="dd-item"><a href="/python/02_string/04_str.count/">str.count<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/05_str.replace/" title="str.replace 替换" class="dd-item"><a href="/python/02_string/05_str.replace/">str.replace<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/06_str.expandtabs/" title="str.expandtabs 替换制表符" class="dd-item"><a href="/python/02_string/06_str.expandtabs/">str.expandtabs<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/07_str.split/" title="str.split 拆分" class="dd-item"><a href="/python/02_string/07_str.split/">str.split<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/08_str.rsplit/" title="str.rsplit 拆分" class="dd-item"><a href="/python/02_string/08_str.rsplit/">str.rsplit<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/09_str.partition/" title="str.partition 拆分" class="dd-item"><a href="/python/02_string/09_str.partition/">str.partition<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/10_str.rpartition/" title="str.rpartition 拆分" class="dd-item"><a href="/python/02_string/10_str.rpartition/">str.rpartition<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/11_str.splitlines/" title="str.splitlines 按行拆分" class="dd-item"><a href="/python/02_string/11_str.splitlines/">str.splitlines<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/12_str.strip/" title="str.strip 移除两边字符" class="dd-item"><a href="/python/02_string/12_str.strip/">str.strip<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/13_str.lstrip/" title="str.lstrip 移除左边字符" class="dd-item"><a href="/python/02_string/13_str.lstrip/">str.lstrip<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/14_str.rstrip/" title="str.rstrip 移除右边字符" class="dd-item"><a href="/python/02_string/14_str.rstrip/">str.rstrip<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/15_str.find/" title="str.find 查找最小索引" class="dd-item"><a href="/python/02_string/15_str.find/">str.find<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/16_str.rfind/" title="str.rfind 查找最大索引" class="dd-item"><a href="/python/02_string/16_str.rfind/">str.rfind<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/17_str.index/" title="str.index 查找最小索引" class="dd-item"><a href="/python/02_string/17_str.index/">str.index<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/18_str.rindex/" title="str.rindex 查找最大索引" class="dd-item"><a href="/python/02_string/18_str.rindex/">str.rindex<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/19_str.join/" title="str.join 拼接字符串" class="dd-item"><a href="/python/02_string/19_str.join/">str.join<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/20_str.startswith/" title="str.startswith 指定字符串开头？" class="dd-item"><a href="/python/02_string/20_str.startswith/">str.startswith<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/21_str.endswith/" title="str.endswith 指定字符串结尾？" class="dd-item"><a href="/python/02_string/21_str.endswith/">str.endswith<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/22_str.ljust/" title="str.ljust 左对齐" class="dd-item"><a href="/python/02_string/22_str.ljust/">str.ljust<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/23_str.center/" title="str.center 居中" class="dd-item"><a href="/python/02_string/23_str.center/">str.center<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/24_str.rjust/" title="str.rjust 右对齐" class="dd-item"><a href="/python/02_string/24_str.rjust/">str.rjust<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/25_str.format/" title="str.format 格式化" class="dd-item"><a href="/python/02_string/25_str.format/">str.format<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/26_str.format_map/" title="str.format_map 格式化" class="dd-item"><a href="/python/02_string/26_str.format_map/">str.format_map<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/27_f-string/" title="f-string 格式化字符串" class="dd-item"><a href="/python/02_string/27_f-string/">f-string 格式化<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/28_string_operators/" title="字符串操作符" class="dd-item"><a href="/python/02_string/28_string_operators/">字符串操作符<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/29_str.encode/" title="str.encode 编码为字节串" class="dd-item"><a href="/python/02_string/29_str.encode/">str.encode<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/30_str.capitalize/" title="str.capitalize 首字符大写" class="dd-item"><a href="/python/02_string/30_str.capitalize/">str.capitalize<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/31_str.casefold/" title="str.casefold 消除大小写" class="dd-item"><a href="/python/02_string/31_str.casefold/">str.casefold<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/32_str.lower/" title="str.lower 转小写" class="dd-item"><a href="/python/02_string/32_str.lower/">str.lower<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/33_str.title/" title="str.title 单词首字母大写" class="dd-item"><a href="/python/02_string/33_str.title/">str.title<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/34_str.upper/" title="str.upper 转大写" class="dd-item"><a href="/python/02_string/34_str.upper/">str.upper<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/35_str.swapcase/" title="str.swapcase 大小写互转" class="dd-item"><a href="/python/02_string/35_str.swapcase/">str.swapcase<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/36_str.zfill/" title="str.zfill 填充 0" class="dd-item"><a href="/python/02_string/36_str.zfill/">str.zfill<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/37_str.translate/" title="str.translate 按表转换" class="dd-item"><a href="/python/02_string/37_str.translate/">str.translate<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/38_str.maketrans/" title="str.maketrans 生成转换表" class="dd-item"><a href="/python/02_string/38_str.maketrans/">str.maketrans<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/39_str.isalnum/" title="str.isalnum 是字母或数字？" class="dd-item"><a href="/python/02_string/39_str.isalnum/">str.isalnum<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/40_str.isalpha/" title="str.isalpha 是字母（包括汉字等）？" class="dd-item"><a href="/python/02_string/40_str.isalpha/">str.isalpha<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/41_str.isdecimal/" title="str.isdecimal 是十进制字符？" class="dd-item"><a href="/python/02_string/41_str.isdecimal/">str.isdecimal<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/42_str.isdigit/" title="str.isdigit 是数字？" class="dd-item"><a href="/python/02_string/42_str.isdigit/">str.isdigit<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/43_str.isnumeric/" title="str.isnumeric 是数值字符？" class="dd-item"><a href="/python/02_string/43_str.isnumeric/">str.isnumeric<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/44_str.islower/" title="str.islower 是小写？" class="dd-item"><a href="/python/02_string/44_str.islower/">str.islower<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/45_str.isupper/" title="str.isupper 是大写？" class="dd-item"><a href="/python/02_string/45_str.isupper/">str.isupper<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/46_str.istitle/" title="str.istitle 是标题字符串？" class="dd-item"><a href="/python/02_string/46_str.istitle/">str.istitle<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/47_str.isascii/" title="str.isascii 是 ASCII 字符？" class="dd-item"><a href="/python/02_string/47_str.isascii/">str.isascii<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/48_str.isidentifier/" title="str.isidentifier 是有效标识符？" class="dd-item"><a href="/python/02_string/48_str.isidentifier/">str.isidentifier<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/49_str.isprintable/" title="str.isprintable 是可打印字符？" class="dd-item"><a href="/python/02_string/49_str.isprintable/">str.isprintable<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/50_str.isspace/" title="str.isspace 是空白字符？" class="dd-item"><a href="/python/02_string/50_str.isspace/">str.isspace<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/51_str.removeprefix/" title="str.removeprefix  移除前缀" class="dd-item"><a href="/python/02_string/51_str.removeprefix/">str.removeprefix<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/02_string/52_str.removesuffix/" title="str.removesuffix 移除后缀" class="dd-item"><a href="/python/02_string/52_str.removesuffix/">str.removesuffix<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/03_list/" title="列表" class="dd-item"><input type="checkbox" id="section-9dce683949d5d69d823966a12cca5136" class="toggle"/><label for="section-9dce683949d5d69d823966a12cca5136" ></label><a href="/python/03_list/">列表概述<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/03_list/02_list_comprehension/" title="列表推导式" class="dd-item"><a href="/python/03_list/02_list_comprehension/">列表推导式<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/03_list_indexes_and_slices/" title="列表的索引和切片" class="dd-item"><a href="/python/03_list/03_list_indexes_and_slices/">列表的索引和切片<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/15_list.clear/" title="list.clear 删除所有元素" class="dd-item"><a href="/python/03_list/15_list.clear/">list.clear<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/04_list_operator/" title="列表操作符" class="dd-item"><a href="/python/03_list/04_list_operator/">列表操作符<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/05_list.append/" title="list.append 添加一个元素" class="dd-item"><a href="/python/03_list/05_list.append/">list.append<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/06_list.extend/" title="list.extend 加入可迭代对象中元素" class="dd-item"><a href="/python/03_list/06_list.extend/">list.extend<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/07_list.insert/" title="list.insert 插入一个元素" class="dd-item"><a href="/python/03_list/07_list.insert/">list.insert<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/08_list.sort/" title="list.sort 对列表原地排序" class="dd-item"><a href="/python/03_list/08_list.sort/">list.sort<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/09_list.reverse/" title="list.reverse 反转列表中元素" class="dd-item"><a href="/python/03_list/09_list.reverse/">list.reverse<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/10_list.pop/" title="list.pop 删除元素并返回" class="dd-item"><a href="/python/03_list/10_list.pop/">list.pop<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/11_list.remove/" title="list.remove 移除一个元素" class="dd-item"><a href="/python/03_list/11_list.remove/">list.remove<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/12_list.count/" title="list.count 统计元素出现次数" class="dd-item"><a href="/python/03_list/12_list.count/">list.count<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/13_list.index/" title="list.index 查找最小索引" class="dd-item"><a href="/python/03_list/13_list.index/">list.index<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/03_list/14_list.copy/" title="list.copy 列表的一个浅拷贝" class="dd-item"><a href="/python/03_list/14_list.copy/">list.copy<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/04_tuple/" title="元组概述" class="dd-item"><input type="checkbox" id="section-dc7ea79191f25dfd309c92b5e4905cab" class="toggle"/><label for="section-dc7ea79191f25dfd309c92b5e4905cab" ></label><a href="/python/04_tuple/">元组<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/04_tuple/02_tuple.count/" title="tuple.count 统计元素出现次数" class="dd-item"><a href="/python/04_tuple/02_tuple.count/">tuple.count<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/04_tuple/03_tuple.index/" title="tuple.index 查找元素索引" class="dd-item"><a href="/python/04_tuple/03_tuple.index/">tuple.index<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/04_tuple/04_operator/" title="元组操作符" class="dd-item"><a href="/python/04_tuple/04_operator/">元组操作符<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/05_sequence/" title="序列类型概述" class="dd-item"><input type="checkbox" id="section-b631793e0b67cf3997ee6b6ccb5a1018" class="toggle"/><label for="section-b631793e0b67cf3997ee6b6ccb5a1018" ></label><a href="/python/05_sequence/">序列<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/05_sequence/02_range/" title="range 对象" class="dd-item"><a href="/python/05_sequence/02_range/">range<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/05_sequence/03_index_slice/" title="序列索引和切片" class="dd-item"><a href="/python/05_sequence/03_index_slice/">序列索引和切片<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/05_sequence/04_common_operations/" title="序列通用操作" class="dd-item"><a href="/python/05_sequence/04_common_operations/">序列通用操作<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/06_set/" title="集合概述" class="dd-item"><input type="checkbox" id="section-c643fef3f2d56a8ad628665722912338" class="toggle"/><label for="section-c643fef3f2d56a8ad628665722912338" ></label><a href="/python/06_set/">集合<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/06_set/02_set.isdisjoint/" title="set.isdisjoint 交集为空吗？" class="dd-item"><a href="/python/06_set/02_set.isdisjoint/">set.isdisjoint<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/03_set.issubset/" title="set.issubset 是子集吗？" class="dd-item"><a href="/python/06_set/03_set.issubset/">set.issubset<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/04_set.issuperset/" title="set.issuperset 是超集吗？" class="dd-item"><a href="/python/06_set/04_set.issuperset/">set.issuperset<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/05_set.union/" title="set.union 并集" class="dd-item"><a href="/python/06_set/05_set.union/">set.union<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/06_set.intersection/" title="set.intersection 交集" class="dd-item"><a href="/python/06_set/06_set.intersection/">set.intersection<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/07_set.difference/" title="set.difference 差集" class="dd-item"><a href="/python/06_set/07_set.difference/">set.difference<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/08_set.symmetric_difference/" title="set.symmetric_difference 对称差" class="dd-item"><a href="/python/06_set/08_set.symmetric_difference/">set.symmetric_difference<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/09_set.copy/" title="set.copy 浅拷贝" class="dd-item"><a href="/python/06_set/09_set.copy/">set.copy<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/10_set.update/" title="set.update 合并更新" class="dd-item"><a href="/python/06_set/10_set.update/">set.update<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/11_set.intersection_update/" title="set.intersection_update 交集更新" class="dd-item"><a href="/python/06_set/11_set.intersection_update/">set.intersection_update<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/12_set.difference_update/" title="set.difference_update 差集更新" class="dd-item"><a href="/python/06_set/12_set.difference_update/">set.difference_update<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/13_set.symmetric_difference_update/" title="set ^= other 对称差集更新" class="dd-item"><a href="/python/06_set/13_set.symmetric_difference_update/">set ^= other<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/14_set.add/" title="set.add 添加元素" class="dd-item"><a href="/python/06_set/14_set.add/">set.add<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/15_set.remove/" title="set.remove 删除元素" class="dd-item"><a href="/python/06_set/15_set.remove/">set.remove<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/16_set.discard/" title="set.discard 删除元素" class="dd-item"><a href="/python/06_set/16_set.discard/">set.discard<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/17_set.pop/" title="set.pop 删除元素并返回" class="dd-item"><a href="/python/06_set/17_set.pop/">set.pop<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/06_set/18_set.clear/" title="set.clear 清空集合元素" class="dd-item"><a href="/python/06_set/18_set.clear/">set.clear<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/07_dictionary/" title="字典概述" class="dd-item"><input type="checkbox" id="section-6edd912ea63088ea7d421ba4a9a6b0c9" class="toggle"/><label for="section-6edd912ea63088ea7d421ba4a9a6b0c9" ></label><a href="/python/07_dictionary/">字典<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/07_dictionary/02_dict.keys/" title="dict.keys 键视图" class="dd-item"><a href="/python/07_dictionary/02_dict.keys/">dict.keys<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/03_dict.values/" title="dict.values 值视图" class="dd-item"><a href="/python/07_dictionary/03_dict.values/">dict.values<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/04_dict.items/" title="dict.items 键值对视图" class="dd-item"><a href="/python/07_dictionary/04_dict.items/">dict.items<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/05_dict.get/" title="dict.get 获取某个键的值" class="dd-item"><a href="/python/07_dictionary/05_dict.get/">dict.get<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/06_dict.copy/" title="dict.copy 浅拷贝" class="dd-item"><a href="/python/07_dictionary/06_dict.copy/">dict.copy<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/07_dict.pop/" title="dict.pop 删除元素并返回值" class="dd-item"><a href="/python/07_dictionary/07_dict.pop/">dict.pop<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/08_dict.popitem/" title="dict.popitem 删除元素并返回键值对" class="dd-item"><a href="/python/07_dictionary/08_dict.popitem/">dict.popitem<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/09_dict.fromkeys/" title="dict.fromkeys 创建字典" class="dd-item"><a href="/python/07_dictionary/09_dict.fromkeys/">dict.fromkeys<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/10_dict.clear/" title="dict.clear 清空字典元素" class="dd-item"><a href="/python/07_dictionary/10_dict.clear/">dict.clear<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/11_dict.setdefault/" title="dict.setdefault 获取或插入元素" class="dd-item"><a href="/python/07_dictionary/11_dict.setdefault/">dict.setdefault<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/12_dict.update/" title="dict.update 更新字典" class="dd-item"><a href="/python/07_dictionary/12_dict.update/">dict.update<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/13_loop_through/" title="字典遍历" class="dd-item"><a href="/python/07_dictionary/13_loop_through/">字典遍历<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/07_dictionary/14_operator/" title="字典操作符" class="dd-item"><a href="/python/07_dictionary/14_operator/">字典操作符<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/08_other_types/" title="生成器表达式和推导式" class="dd-item"><input type="checkbox" id="section-6dcb28ff486050e64e8c2098986ff9d6" class="toggle"/><label for="section-6dcb28ff486050e64e8c2098986ff9d6" ></label><a href="/python/08_other_types/">其他类型<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/08_other_types/01_none/" title="空值 None" class="dd-item"><a href="/python/08_other_types/01_none/">None<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/08_other_types/03_other_types/" title="其他内置类型" class="dd-item"><a href="/python/08_other_types/03_other_types/">其他内置类型<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/09_operator/" title="操作符概述" class="dd-item"><input type="checkbox" id="section-bf0c286771bede167ff18be220caecff" class="toggle"/><label for="section-bf0c286771bede167ff18be220caecff" ></label><a href="/python/09_operator/">操作符<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/09_operator/02_compare/" title="比较运算符" class="dd-item"><a href="/python/09_operator/02_compare/">比较运算符<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/09_operator/03_precedence/" title="操作符优先级" class="dd-item"><a href="/python/09_operator/03_precedence/">操作符优先级<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/09_operator/04_special/" title="操作符特殊用法" class="dd-item"><a href="/python/09_operator/04_special/">操作符特殊用法<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/10_statement/" title="表达式语句" class="dd-item"><input type="checkbox" id="section-64a5f0f51786f2cfc0f1a295e3568242" class="toggle"/><label for="section-64a5f0f51786f2cfc0f1a295e3568242" ></label><a href="/python/10_statement/">语句<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/10_statement/02_assignment/" title="赋值语句" class="dd-item"><a href="/python/10_statement/02_assignment/">赋值语句<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/03_if/" title="if" class="dd-item"><a href="/python/10_statement/03_if/">if<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/04_for/" title="for" class="dd-item"><a href="/python/10_statement/04_for/">for<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/05_while/" title="while" class="dd-item"><a href="/python/10_statement/05_while/">while<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/06_break/" title="break" class="dd-item"><a href="/python/10_statement/06_break/">break<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/07_continue/" title="continue" class="dd-item"><a href="/python/10_statement/07_continue/">continue<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/08_del/" title="del" class="dd-item"><a href="/python/10_statement/08_del/">del<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/09_pass/" title="pass" class="dd-item"><a href="/python/10_statement/09_pass/">pass<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/10_def/" title="def" class="dd-item"><a href="/python/10_statement/10_def/">def<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/11_return/" title="return" class="dd-item"><a href="/python/10_statement/11_return/">return<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/12_yield/" title="yield" class="dd-item"><a href="/python/10_statement/12_yield/">yield<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/13_class/" title="class 语句" class="dd-item"><a href="/python/10_statement/13_class/">class 语句<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/14_try/" title="try" class="dd-item"><a href="/python/10_statement/14_try/">try<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/15_raise/" title="raise" class="dd-item"><a href="/python/10_statement/15_raise/">raise<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/16_with/" title="with" class="dd-item"><a href="/python/10_statement/16_with/">with<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/17_assert/" title="assert" class="dd-item"><a href="/python/10_statement/17_assert/">assert<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/18_import/" title="import" class="dd-item"><a href="/python/10_statement/18_import/">import<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/19_global/" title="global" class="dd-item"><a href="/python/10_statement/19_global/">global<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/10_statement/20_nonlocal/" title="nonlocal" class="dd-item"><a href="/python/10_statement/20_nonlocal/">nonlocal<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/11_built-in_function/" title="abs() 数字取绝对值" class="dd-item"><input type="checkbox" id="section-c688daa3d748f201265692938123fc94" class="toggle"/><label for="section-c688daa3d748f201265692938123fc94" ></label><a href="/python/11_built-in_function/">内置函数<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/11_built-in_function/02_all/" title="all() 所有元素布尔值为真？" class="dd-item"><a href="/python/11_built-in_function/02_all/">all()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/03_any/" title="any() 有一个元素布尔值为真？" class="dd-item"><a href="/python/11_built-in_function/03_any/">any()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/04_ascii/" title="ascii() 返回对象的可打印字符串" class="dd-item"><a href="/python/11_built-in_function/04_ascii/">ascii()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/05_bin/" title="bin() 整数的二进制形式" class="dd-item"><a href="/python/11_built-in_function/05_bin/">bin()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/06_bool/" title="bool 返回对象的布尔值" class="dd-item"><a href="/python/11_built-in_function/06_bool/">bool<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/07_bytes/" title="bytes 创建 bytes 对象" class="dd-item"><a href="/python/11_built-in_function/07_bytes/">bytes<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/08_callable/" title="callable() 是可调用对象？" class="dd-item"><a href="/python/11_built-in_function/08_callable/">callable()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/09_chr/" title="chr() 返回 Unicode 码位值对应字符" class="dd-item"><a href="/python/11_built-in_function/09_chr/">chr()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/10_classmethod/" title="classmethod 封装函数为类方法" class="dd-item"><a href="/python/11_built-in_function/10_classmethod/">classmethod<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/11_compile/" title="compile() 创建代码对象" class="dd-item"><a href="/python/11_built-in_function/11_compile/">compile()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/12_complex/" title="complex 创建复数" class="dd-item"><a href="/python/11_built-in_function/12_complex/">complex<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/13_delattr/" title="delattr() 删除对象属性" class="dd-item"><a href="/python/11_built-in_function/13_delattr/">delattr()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/14_dict/" title="dict 创建字典" class="dd-item"><a href="/python/11_built-in_function/14_dict/">dict<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/15_dir/" title="dir() 返回对象属性列表" class="dd-item"><a href="/python/11_built-in_function/15_dir/">dir()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/16_divmod/" title="divmod() 求两个数的商和余" class="dd-item"><a href="/python/11_built-in_function/16_divmod/">divmod()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/17_enumerate/" title="enumerate 枚举" class="dd-item"><a href="/python/11_built-in_function/17_enumerate/">enumerate<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/18_eval/" title="eval() 解析字符串或代码并求值" class="dd-item"><a href="/python/11_built-in_function/18_eval/">eval()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/19_exec/" title="exec() 解析字符串或代码并求值" class="dd-item"><a href="/python/11_built-in_function/19_exec/">exec()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/20_filter/" title="filter 真值元素筛选" class="dd-item"><a href="/python/11_built-in_function/20_filter/">filter<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/21_float/" title="float 创建浮点数" class="dd-item"><a href="/python/11_built-in_function/21_float/">float<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/22_format/" title="format() 格式化" class="dd-item"><a href="/python/11_built-in_function/22_format/">format()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/23_frozenset/" title="frozenset 创建不可变集合" class="dd-item"><a href="/python/11_built-in_function/23_frozenset/">frozenset<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/24_getattr/" title="getattr() 获取对象的属性" class="dd-item"><a href="/python/11_built-in_function/24_getattr/">getattr()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/25_globals/" title="globals() 返回全局变量字典" class="dd-item"><a href="/python/11_built-in_function/25_globals/">globals()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/26_hasattr/" title="hasattr() 是对象的属性吗？" class="dd-item"><a href="/python/11_built-in_function/26_hasattr/">hasattr()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/27_hash/" title="hash() 返回对象的哈希值" class="dd-item"><a href="/python/11_built-in_function/27_hash/">hash()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/28_help/" title="help 启动帮助系统" class="dd-item"><a href="/python/11_built-in_function/28_help/">help<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/29_hex/" title="hex() 整数的十六进制形式" class="dd-item"><a href="/python/11_built-in_function/29_hex/">hex()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/30_id/" title="id() 返回对象的唯一标识" class="dd-item"><a href="/python/11_built-in_function/30_id/">id()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/31_input/" title="input() 接受输入返回字符串" class="dd-item"><a href="/python/11_built-in_function/31_input/">input()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/32_int/" title="int 创建整数" class="dd-item"><a href="/python/11_built-in_function/32_int/">int<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/33_isinstance/" title="isinstance() 是给定类的实例？" class="dd-item"><a href="/python/11_built-in_function/33_isinstance/">isinstance()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/34_issubclass/" title="issubclass() 是给定类的子类吗？" class="dd-item"><a href="/python/11_built-in_function/34_issubclass/">issubclass()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/35_iter/" title="iter() 转迭代器" class="dd-item"><a href="/python/11_built-in_function/35_iter/">iter()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/36_len/" title="len() 返回元素个数" class="dd-item"><a href="/python/11_built-in_function/36_len/">len()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/37_list/" title="list 创建列表" class="dd-item"><a href="/python/11_built-in_function/37_list/">list<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/38_locals/" title="locals() 返回局部变量的字典" class="dd-item"><a href="/python/11_built-in_function/38_locals/">locals()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/39_map/" title="map 以给定函数转换元素" class="dd-item"><a href="/python/11_built-in_function/39_map/">map<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/40_max/" title="max() 求最大项" class="dd-item"><a href="/python/11_built-in_function/40_max/">max()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/41_min/" title="min() 求最小项" class="dd-item"><a href="/python/11_built-in_function/41_min/">min()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/42_next/" title="next() 返回迭代器下一个元素" class="dd-item"><a href="/python/11_built-in_function/42_next/">next()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/43_object/" title="object 所有类的基类" class="dd-item"><a href="/python/11_built-in_function/43_object/">object<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/44_oct/" title="oct() 整数的八进制形式" class="dd-item"><a href="/python/11_built-in_function/44_oct/">oct()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/45_open/" title="open() 打开文件" class="dd-item"><a href="/python/11_built-in_function/45_open/">open()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/46_ord/" title="ord() 返回单个字符 Unicode 码位值" class="dd-item"><a href="/python/11_built-in_function/46_ord/">ord()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/47_pow/" title="pow() 幂运算并取余" class="dd-item"><a href="/python/11_built-in_function/47_pow/">pow()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/48_print/" title="print()" class="dd-item"><a href="/python/11_built-in_function/48_print/">print()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/49_property/" title="property 返回 property 属性" class="dd-item"><a href="/python/11_built-in_function/49_property/">property<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/50_range/" title="range 创建 range 序列" class="dd-item"><a href="/python/11_built-in_function/50_range/">range<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/51_repr/" title="repr() 返回对象的可打印字符串" class="dd-item"><a href="/python/11_built-in_function/51_repr/">repr()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/52_reversed/" title="reversed 序列逆置" class="dd-item"><a href="/python/11_built-in_function/52_reversed/">reversed<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/53_round/" title="round() 数字舍入" class="dd-item"><a href="/python/11_built-in_function/53_round/">round()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/54_set/" title="set 创建集合" class="dd-item"><a href="/python/11_built-in_function/54_set/">set<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/55_setattr/" title="setattr() 设置或新增属性" class="dd-item"><a href="/python/11_built-in_function/55_setattr/">setattr()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/56_slice/" title="slice 创建切片对象" class="dd-item"><a href="/python/11_built-in_function/56_slice/">slice<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/57_sorted/" title="sorted() 返回排序列表" class="dd-item"><a href="/python/11_built-in_function/57_sorted/">sorted()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/58_staticmethod/" title="staticmethod 封装函数为静态方法" class="dd-item"><a href="/python/11_built-in_function/58_staticmethod/">staticmethod<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/59_str/" title="str 创建字符串" class="dd-item"><a href="/python/11_built-in_function/59_str/">str<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/60_sum/" title="sum() 数字求和或序列拼接" class="dd-item"><a href="/python/11_built-in_function/60_sum/">sum()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/61_super/" title="super 调用委托给父类或兄弟类" class="dd-item"><a href="/python/11_built-in_function/61_super/">super<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/62_tuple/" title="tuple 创建元组" class="dd-item"><a href="/python/11_built-in_function/62_tuple/">tuple<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/63_type/" title="type 判断类型或创建类" class="dd-item"><a href="/python/11_built-in_function/63_type/">type<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/64_vars/" title="vars() 返回对象的变量字典" class="dd-item"><a href="/python/11_built-in_function/64_vars/">vars()<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/11_built-in_function/65_zip/" title="zip 重组可迭代对象" class="dd-item"><a href="/python/11_built-in_function/65_zip/">zip<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/12_function/" title="函数概述" class="dd-item"><input type="checkbox" id="section-e6b48a8a67c6f7ff797a323f6568f234" class="toggle"/><label for="section-e6b48a8a67c6f7ff797a323f6568f234" ></label><a href="/python/12_function/">函数<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/12_function/02_function_definition/" title="函数定义" class="dd-item"><a href="/python/12_function/02_function_definition/">函数定义<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/12_function/03_formal_parameter/" title="函数形参" class="dd-item"><a href="/python/12_function/03_formal_parameter/">函数形参<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/12_function/04_return_value/" title="函数返回值" class="dd-item"><a href="/python/12_function/04_return_value/">函数返回值<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/12_function/05_recursion/" title="递归函数" class="dd-item"><a href="/python/12_function/05_recursion/">递归函数<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/12_function/06_doc/" title="函数文档" class="dd-item"><a href="/python/12_function/06_doc/">函数文档<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/12_function/07_function_call/" title="函数调用" class="dd-item"><a href="/python/12_function/07_function_call/">函数调用<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/12_function/08_lambda/" title="lambda 函数" class="dd-item"><a href="/python/12_function/08_lambda/">lambda 函数<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/12_function/09_generator/" title="生成器函数" class="dd-item"><a href="/python/12_function/09_generator/">生成器函数<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/13_exception/" title="错误和异常" class="dd-item"><input type="checkbox" id="section-bc4f4d42c44eaf0579ed86ae2d0d51db" class="toggle"/><label for="section-bc4f4d42c44eaf0579ed86ae2d0d51db" ></label><a href="/python/13_exception/">错误和异常<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/13_exception/02_handling/" title="处理异常" class="dd-item"><a href="/python/13_exception/02_handling/">处理异常<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/13_exception/03_raising/" title="抛出异常" class="dd-item"><a href="/python/13_exception/03_raising/">抛出异常<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/13_exception/04_user_defined/" title="自定义异常" class="dd-item"><a href="/python/13_exception/04_user_defined/">自定义异常<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/13_exception/05_finally/" title="finally 清理操作" class="dd-item"><a href="/python/13_exception/05_finally/">finally<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/14_file/" title="文件对象概述" class="dd-item"><input type="checkbox" id="section-e195820c7e4bf396e327bfb0a46a50f9" class="toggle"/><label for="section-e195820c7e4bf396e327bfb0a46a50f9" ></label><a href="/python/14_file/">文件<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/14_file/02_read/" title="读取文件内容" class="dd-item"><a href="/python/14_file/02_read/">读取文件内容<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/14_file/03_write/" title="文件写入内容" class="dd-item"><a href="/python/14_file/03_write/">文件写入内容<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/15_module/" title="模块概述" class="dd-item"><input type="checkbox" id="section-4385ec01f3a88bacfccd61dd3520ad8b" class="toggle"/><label for="section-4385ec01f3a88bacfccd61dd3520ad8b" ></label><a href="/python/15_module/">模块<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/15_module/02_create_module/" title="创建模块" class="dd-item"><a href="/python/15_module/02_create_module/">创建模块<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/15_module/04_executable_file/" title="可执行文件" class="dd-item"><a href="/python/15_module/04_executable_file/">可执行文件<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/15_module/05_import_code/" title="导入操作" class="dd-item"><a href="/python/15_module/05_import_code/">导入操作<i class="fas fa-check read-icon"></i></a></li></ul></li>
          <li data-nav-id="/python/16_class/" title="面向对象概述" class="dd-item"><input type="checkbox" id="section-1462fcc8311b5f31727642b5a438fc6d" class="toggle"/><label for="section-1462fcc8311b5f31727642b5a438fc6d" ></label><a href="/python/16_class/">面向对象<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/python/16_class/02_class_definition/" title="类定义" class="dd-item"><a href="/python/16_class/02_class_definition/">类定义<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/03_instance/" title="实例" class="dd-item"><a href="/python/16_class/03_instance/">实例<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/04_object/" title="对象" class="dd-item"><a href="/python/16_class/04_object/">对象<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/05_name/" title="名称" class="dd-item"><a href="/python/16_class/05_name/">名称<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/06_variable/" title="变量" class="dd-item"><a href="/python/16_class/06_variable/">变量<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/07_attribute/" title="属性" class="dd-item"><a href="/python/16_class/07_attribute/">属性<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/08_method/" title="方法" class="dd-item"><a href="/python/16_class/08_method/">方法<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/09_namespace/" title="命名空间" class="dd-item"><a href="/python/16_class/09_namespace/">命名空间<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/10_scope/" title="作用域" class="dd-item"><a href="/python/16_class/10_scope/">作用域<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/11_create_instance/" title="创建实例" class="dd-item"><a href="/python/16_class/11_create_instance/">创建实例<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/12_attribute_operation/" title="属性操作" class="dd-item"><a href="/python/16_class/12_attribute_operation/">属性操作<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/13_method_operation/" title="方法操作" class="dd-item"><a href="/python/16_class/13_method_operation/">方法操作<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/python/16_class/14_inheritance/" title="类继承" class="dd-item"><a href="/python/16_class/14_inheritance/">类继承<i class="fas fa-check read-icon"></i></a></li></ul></li></ul></li>
          <li data-nav-id="/regex/" title="通俗易懂 Python 正则表达式" class="dd-item"><a href="/regex/">正则表达式<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/numpy/" title="Numpy 基础快速了解和查询" class="dd-item"><a href="/numpy/">Numpy<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/pandas/" title="Pandas 基础快速了解和查询" class="dd-item"><a href="/pandas/">Pandas<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/matplotlib/" title="Matpotlib 快速入门" class="dd-item"><a href="/matplotlib/">Matpotlib<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/" title="数理统计、机器学习入门" class="dd-item active parent"><input type="checkbox" id="section-7311b56f8e68ee915dd2e657bac269ed" class="toggle" checked/><label for="section-7311b56f8e68ee915dd2e657bac269ed" ></label><a href="/statistics/">机器学习入门<i class="fas fa-check read-icon"></i></a><ul>
          <li data-nav-id="/statistics/statistics1/" title="描述统计" class="dd-item"><a href="/statistics/statistics1/">描述统计<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics2/" title="推断统计" class="dd-item"><a href="/statistics/statistics2/">推断统计<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics3/" title="线性回归" class="dd-item"><a href="/statistics/statistics3/">线性回归<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics4/" title="逻辑回归" class="dd-item"><a href="/statistics/statistics4/">逻辑回归<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics5/" title="分类模型评估" class="dd-item"><a href="/statistics/statistics5/">分类模型评估<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics6/" title="KNN 算法" class="dd-item"><a href="/statistics/statistics6/">KNN 算法<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics7/" title="朴素贝叶斯" class="dd-item"><a href="/statistics/statistics7/">朴素贝叶斯<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics8/" title="决策树" class="dd-item"><a href="/statistics/statistics8/">决策树<i class="fas fa-check read-icon"></i></a></li>
          <li data-nav-id="/statistics/statistics9/" title="K-Means 算法" class="dd-item"><a href="/statistics/statistics9/">K-Means 算法<i class="fas fa-check read-icon"></i></a></li></ul></li>
        </ul>
        <div id="shortcuts">
          <div class="nav-title">More</div>
          <ul>
            <li><a class="padding" href="/https:/billwuhao.com">@billwuhao.com</a></li>
            <li><a class="padding" href="/https:/github.com/jupyterfun"><i class='fab fa-fw fa-github'></i> Jupyterfun</a></li>
            <li><a class="padding" href="/about">About</a></li>
          </ul>
        </div>
        <div class="footermargin footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVisitedLinks showFooter"></div>
        <hr class="default-animation footerLangSwitch footerVariantSwitch footerVisitedLinks footerFooter showVisitedLinks showFooter"/>
        <div id="prefooter" class="footerLangSwitch footerVariantSwitch footerVisitedLinks showVisitedLinks">
          <ul>
            <li id="select-language-container" class="footerLangSwitch">
              <a class="padding select-container">
                <i class="fas fa-language fa-fw"></i>
                <span>&nbsp;</span>
                <div class="select-style">
                  <select id="select-language" onchange="location = baseUri + this.value;">
                  </select>
                </div>
                <div class="select-clear"></div>
              </a>
            </li>
            <li id="select-variant-container" class="footerVariantSwitch">
              <a class="padding select-container">
                <i class="fas fa-paint-brush fa-fw"></i>
                <span>&nbsp;</span>
                <div class="select-style">
                  <select id="select-variant" onchange="window.variants && variants.changeVariant( this.value );">
                    <option id="blue" value="blue" selected>Blue</option>
                  </select>
                </div>
                <div class="select-clear"></div>
              </a>
              <script>window.variants && variants.markSelectedVariant();</script>
            </li>
            <li class="footerVisitedLinks showVisitedLinks"><a class="padding" onclick="clearHistory();"><i class="fas fa-history fa-fw"></i> Clear History</a></li>
          </ul>
        </div>
        <div id="footer" class="footerFooter showFooter">
      <p>©2022 吴明文</p>
        </div>
      </div>
    </aside>
    <script src="/js/clipboard.min.js?1661054444" defer></script>
    <script src="/js/perfect-scrollbar.min.js?1661054444" defer></script>
    <script src="/js/featherlight.min.js?1661054444" defer></script>
    <script>
      function useMathJax( config ){
        if( !Object.assign ){
          
          return;
        }
        window.MathJax = Object.assign( window.MathJax || {}, {
          loader: {
            load: ['[tex]/mhchem']
          },
          startup: {
            elements: [
              '.math'
            ]
          },
          tex: {
            inlineMath: [
              ['$', '$'], 
              ['\\(', '\\)']
            ]
          },
          options: {
            enableMenu: false 
          }
        }, config );
      }
      useMathJax( JSON.parse("{}") );
    </script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="/js/jquery.svg.pan.zoom.js?1661054444" defer></script>
    <script src="https://unpkg.com/mermaid/dist/mermaid.min.js" defer></script>
    <script>
      window.themeUseMermaid = JSON.parse("{ \"theme\": \"default\" }");
    </script>
    <script src="https://unpkg.com/rapidoc/dist/rapidoc-min.js" defer></script>
    <script>
      window.themeUseSwagger = JSON.parse("{ \"theme\": \"light\" }");
    </script>
    <script src="/js/theme.js?1661054444" defer></script>
  </body>
</html>
